#   Version 6.3.0
##################################################################
# Pseudo-BNF Definitions for Search Language
#
# FORMATTING
# - adjacent tokens implicitly allow no whitespace.
# - all literals are case-insensitive.
# - aside from reserved characters ("<>()|?*+") and <tokens>, everything else is taken literally.
#   those characters need to be quoted.  use \"  to represent a quote.
# - examples are now broken into a separate values -- example1,
#    example2, example3.  if there's a text comment that goes with
#    exampleN then it should be in the commentN attribute.  for example:
#    "example2 = 5d" "comment2 = 5 days"
# - The only reserved characters are "<>()|?*+",
# * whitespace (including newlines) matches \s+
# * regex-like grouping
#      (): grouping
#      (<term>)? : <term> is optional
#      (<term>)* : <term> is repeated 0 or more times
#      (<term>)+ : <term> is repeated 1 or more times
#
#
#########################
# RECENT CHANGES: 24NOV08
#
# - <terms> can be named for readability with a colon and a default value "move <field:fromfield=localhost> to <field:tofield=localhost>"
# - all public commands should have a shortdesc value
# - to simplify the bnf for the end user, an optional "simplesyntax" can be specified,
#   - to remove things like (m|min|mins|minute|minutes) so that the bnf becomes understandable
#   - potentially can be used to remove obscure/rarely used features
#     of search commands, but that should be done sparingly and with
#     some thought.
#
# - for a command's DESCRIPTION, when automatically converted to html:
#   - multiple whitespace are removed
#   - for convenience, \p\ will cause a paragraph break and \i\ a newline and indent (<br>&nbsp;&nbsp;&nbsp;&nbsp;)
#   - <terms> are italicized, UPPERCASETERMS and quoted terms are put into <code/>
#
#########################
#
# conventions in attributes:
#
# internal commands are distinguished by the attribute:
# usage = internal
#
# the general principle of operation is described in:
# description = text
#
# special notes of operation are described in:
# note = text
#
# to show which commands are related to the current command
# related = top, rare
#
# fields that begin with "_" are considered Splunk internal fields
# and are not used in many of the commands that by default operate
# on all the fields
#
# any stanza that ends with "-command" is considered a command.
#
#
#########################
# common term definitions
#########################
#
# field is any field, non-wildcarded
# wc-field represents wildcarded fields
# wc-string is any wildcarded string
#
#########################
#
# <field-list> ::= <field> | <field-list> <ws> <field>
# <field> ::= <string>
# <wc-field-list> ::= <wc-field> | <wc-field-list> <ws> <wc-field>
# <wc-field> ::= <wc-string>
# <wc-string> ::= <string>
#
# <field-and-value> ::= ? a field and value separated by a :: eg host::localhost ?
# <field-and-value-list> ::= <field-and-value> | <field-and-value-list> <ws> <field-and-value>
#
# <tag> ::= <string>
# <tag-list> ::= <tag> | <tag-list>
#
# <bool> ::= <true> | <false>
# <true> ::= T | TRUE
# <false> ::= F | FALSE
#
# <string> ::= ? any string enclosed by double quotes, or any unbroken sequence of alphanumeric characters plus underscore ?
# <int> ::= ? any integer ?
#
# we do not support 0 and 100th percentiles
# with the perc aggregator, to achieve, use the min() and max() aggregators.
# <percentile> ::= ? any integer between 1 and 99 ?#
# <num> ::= ? any real number ?
#
# <filename> ::= ? a valid path to a file on the server ?
#
# [filename]
# syntax = <string>
# description = A path on the filesystem of the server
#
# [term]
# syntax = [a-zA-Z0-9_-]+
#
# [command-pipeline]
# syntax = <generating-command> (| <command>)*
#
###########################################################################
# COMMAND LIST:
#
##################
# NOTE:
# pre* commands are not included in this list as all pre* commands are the
# map portion of the original command with exactly the same syntax and cannot be
# access externally (via CLI or UI)
# (e.g. prestats / stats, prediscretize / discretize, prededup / dedup)
##################
#
# !!PLEASE DEFINE THESE UNDEFINED STANZAS:
#
# Ignoring undefined stanza: search-pipeline
# Ignoring undefined stanza: search-directive
# Ignoring undefined stanza: quoted-str


##################
# abstract/excerpt
##################
[abstract-command]
syntax = abstract (maxterms=<int>)? (maxlines=<int>)?
alias = excerpt
shortdesc = Shortens the text of results to a brief summary representation
description = Produce an abstract -- a summary or brief representation -- of the text of search results.  The original text is replaced by the summary, which is produced by a scoring mechanism.  If the event is larger than the selected maxlines, those with more terms and more terms on adjacent lines are preferred over those with fewer terms.  If a line has a search term, its neighboring lines also partially match, and may be returned to provide context. When there are gaps between the selected lines, lines are prefixed with "...". \p\\
	    If the text of a result has fewer lines or an equal number of lines to maxlines, no change will occur.\i\\
	    * <maxlines> accepts values from 1 - 500. \i\\
	    * <maxterms> accepts values from 1 - 1000.
commentcheat = Show a summary of up to 5 lines for each search result.
examplecheat = ... |abstract maxlines=5
category = formatting
maintainer = david
appears-in = 3.0
usage = public
related = highlight
tags = condense summarize summary outline pare prune shorten skim snip sum trim

##################
# addinfo
##################
[addinfo-command]
syntax = addinfo
shortdesc = Add fields that contain common information about the current search.
description = Adds global information about the search to each event.  addinfo is primarily an internal component of summary indexing. \i\\
              Currently the following fields are added: \i\\
              "info_min_time"    - the earliest time bound for the search \i\\
              "info_max_time"    - the latest time bound for the search \i\\
              "info_search_id"   - query id of the search that generated the event \i\\
              "info_search_time" - time when the search was executed.
maintainer = ledion
comment = Add information about the search to each event.
example = ... |addinfo
appears-in = 3.2
usage = public
tags = search info
category = fields::add
related = search

##################
# addtotals
##################

[addtotals-command]
syntax = addtotals (row=<bool>)? (col=<bool>)? (labelfield=<field>)? (label=<string>)? (fieldname=<field>)? <field-list>
alias = addcoltotals
shortdesc = Computes the sum of all numeric fields for each result.
description = If "row=t" (default if invoked as 'addtotals') for each result, computes the arithmetic sum of all\
	      numeric fields that match <field-list> (wildcarded field list).\
	      If list is empty all fields are considered.\
	      The sum is placed in the specified field or "Total" if none was specified.\
	      If "col=t" (default if invoked as 'addcoltotals'), adds a new result at the end that represents the sum of each field.\
	      LABELFIELD, if specified, is a field that will be added to this summary \
	      event with the value set by the 'label' option.
comment1 = Compute the sums of the numeric fields of each results.
example1 = ... | addtotals
comment2 = Compute the sums of the numeric fields that match the given list, and save the sums in the field "sum".
example2 = ... | addtotals fieldname=sum foobar* *baz*
comment3 = Compute the sums of all the fields, and put the sums in a summary event called "change_name".
example3 = ... | addtotals col=t labelfield=change_name label=ALL
comment4 = Compute the sums of all the fields, and put the sums in a summary event called "change_name".
example4 = ... | addcoltotals labelfield=change_name label=ALL
commentcheat = Calculate the sums of the numeric fields of each result, and put the sums in the field "sum".
examplecheat = ... | addtotals fieldname=sum
category = reporting
appears-in = 3.0
maintainer = steveyz
usage = public
related = stats
tags = total add calculate sum

##################
# anomalies
##################

[anomalies-command]
syntax = anomalies (threshold=<num>)? (labelonly=<bool>)? (normalize=<bool>)? (maxvalues=<int>)? (field=<field>)? (blacklist=<filename>)? (blacklistthreshold=<num>)? (<by-clause>)?
shortdesc = Computes an "unexpectedness" score for an event.
description = Determines the degree of "unexpectedness" of an event's field \
    value, based on the previous MAXVALUE events.  By default it \
    removes events that are well-expected (unexpectedness > \
    THRESHOLD). The default THRESHOLD is 0.01.  If LABELONLY is true, \
    no events are removed, and the "unexpectedness" attribute is set \
    on all events.  The FIELD analyzed by default is "_raw".  \
    By default, NORMALIZE is true, which normalizes numerics.  For cases \
    where FIELD contains numeric data that should not be normalized, but \
    treated as categories, set NORMALIZE=false. The \
    BLACKLIST is a name of a csv file of events in \
    $SPLUNK_HOME/var/run/splunk/<BLACKLIST>.csv, such that any incoming \
    events that are similar to the blacklisted events are treated as \
    not anomalous (i.e., uninteresting) and given an unexpectedness \
    score of 0.0.  Events that match blacklisted events with a \
    similarity score above BLACKLISTTHRESHOLD (defaulting to 0.05) are \
    marked as unexpected.  The inclusion of a 'by' clause, allows the \
    specification of a list of fields to segregate results for anomaly \
    detection.  For each combination of values for the specified \
    field(s), events with those values are treated entirely separately. \
    Therefore, 'anomalies by source' will look for anomalies in each \
    source separately -- a pattern in one source will not affect that \
    it is anomalous in another source.
maintainer = david
appears-in = 4.0
comment1 = Return only anomalous events.
example1 = ... | anomalies
comment2 = Show most interesting events first, ignoring any in the blacklist 'boringevents'.
example2 = ... | anomalies blacklist=boringevents | sort -unexpectedness
comment3 = Use with transactions to find regions of time that look unusual.
example3 = ... | transaction maxpause=2s | anomalies
usage = public
related = anomalousvalue, cluster, kmeans, outlier
tags = anomaly unusual odd irregular dangerous unexpected outlier
category = results::filter

##################
# anomalousvalue
##################

[anomalousvalue-command]
syntax = anomalousvalue <av-option>* <anovalue-action-option>? <anovalue-pthresh-option>? <field-list>?
shortdesc = Finds and summarizes irregular, or uncommon, search results.
description = Identifies or summarizes the values in the data that are anomalous either by frequency of occurrence \
              or number of standard deviations from the mean.  If a field-list is given, only those fields are \
	      considered.  Otherwise all non internal fields are considered. \p\\
	      For fields that are considered anomalous, a new field is added with the following scheme.  \
	      If the field is numeric, e.g. \"size\",  the new field will be  \"Anomaly_Score_Num(size)\". \
	      If the field is non-numeric, e.g. \"name\", the nnew field will be \"Anomaly_Score_Cat(name)\".
maintainer = steveyz
appears-in = 3.0
comment1 = Return only uncommon values.
example1 = ... | anomalousvalue
commentcheat = Return events with uncommon values.
examplecheat = ... | anomalousvalue action=filter pthresh=0.02
category = reporting
usage = public
related = af, anomalies, cluster, kmeans, outlier
tags = anomaly unusual odd irregular dangerous unexpected

[av-option]
syntax=minsupcount=<int>|maxanofreq=<float>|minsupfreq=<float>|minnormfreq=<float>
description = Parameters to the anomalousvalue command.  \
	      minsupcount is the minimum number of rows that must contain a field in order to consider the field at all. \
	      maxanofreq is the maximum frequency (as a decimal) for a value to be considered anomalous. \
	      minsupfreq is the minimum support frequency.  A field must be in at least this fraction of overall events to be considered. \
	      minnormfreq is the minimum normal frequency.  A field's values must be considered normal at least this fraction of times \
	         or else the field is not considered for determining if the event is anomalous


[anovalue-action-option]
syntax = action=(annotate|filter|summary)
description = If action is ANNOTATE, new fields will be added to the event containing anomalous values that \
              indicate the anomaly scores of the values. \
	      If action is FILTER, events with anomalous value(s) are retained while non-anomalous values are dropped. \
              If action is SUMMARY, a table summarizing the anomaly statistics for each field is generated.
default = "action=filter"

[anovalue-pthresh-option]
syntax = pthresh=<num>
description = Probability threshold (as a decimal) that has to be met for a value to be deemed anomalous
default = "pthresh=0.01"

##################
## anomalydetection
###################

[anomalydetection-command]
syntax = anomalydetection <anoma-method-option>? <anoma-action-option>? <anoma-pthresh-option>? <anoma-cutoff-option>? <field-list>?
shortdesc = Find anomalous events in a set of search results.
description = Identify anomalous events by computing a probability for each event and then detecting unusually small probablities. \
              The probability is defined as the product of the frequencies of each individual field value in the event. \
              For categorical fields, the frequency of a value X is the number of times X occurs divided by the total number of events. \
              For numerical fields, we first build a histogram for all the values, then compute the frequency of a value X \
              as the size of the bin that contains X divided by the number of events. \
              Missing values are treated by adding a special value and updating its count just like a normal value. \
              Histograms are built using the standard Scott's rule to determine the bin width.
maintainer = nnguyen
appears-in = Ember
comment = The way probabilities are computed is called the Naive Bayes method, which means the individual fields are considered independent. \
          This is a simplication to make the command reasonably fast.
example1 = ... | anomalydetection
comment1 = Return only anomalous events.
example2 = ... | anomalydetection action=summary
comment2 = Return a short summary of how many anomalous events are there and some other statistics such as the threshold value used to detect them.
category = streaming, reporting
usage = public
related = anomalies, anomalousvalue, outlier, cluster, kmeans
tags = anomaly unusual odd irregular dangerous unexpected Bayes

[anoma-method-option]
syntax = method=(histogram|zscore|iqr)
description = There are three methods instead of one because we've combined two older commands, anomalousvalue and outlier, with the new method. \
              The new method is invoked by choosing the 'histogram' option. The anomalousvalue command is invoked by choosing 'zscore', \
              and the outlier command is invoked by choosing 'iqr'. \
              Below we will describe other options associated with the histogram method. For the other two methods, the associated options are \
              exactly the same as before. That is, the queries '...| anomalousvalue ...' and '...| anomalydetection method=zscore ...' where the '...' are \
              exactly the same in the two queries will produce exactly the same outputs. The same scheme applies to outlier.
example1 = ... | anomalydetection method=zscore action=filter pthresh=0.05
comment1 = This query returns the same output as that returned by the query '... | anomalousvalue action=filter pthresh=0.05'.
example2 = ... | anomalydetection method=iqr action=tf param=4 uselower=true mark=true
comment2 = This query returns the same output as that returned by the query '...| outlier action=tf param=4 uselower=true mark=true'.
default = "method=histogram"

[anoma-action-option]
syntax = action=(filter|annotate|summary) if the method is histogram or zscore \
         action=(transform|tf|remove|rm) if the method is iqr
description = If the method is zscore or iqr, then the actions have the same meaning as in the anomalousvalue and outlier commands. \
              If the method is histogram, then the meanings are:\
              If action is FILTER, anomalous events are retained while others are dropped. \
              If action is ANNOTATE, new fields will be added to anomalous events that indicate the probability of the event as well as which field \
              may be the cause of the anomaly. \
              If the action is SUMMARY, a table summarizing the anomaly statistics for the search results is generated.
default = "action=filter"
comment = This is the default action when method=histogram. If method is zscore or iqr, then the default action is the default ones for those commands, \
          i.e., 'filter' for zscore and 'transform' for iqr

[anoma-pthresh-option]
syntax = pthresh=<num>
description = First, this option only applies when the method is either histogram or zscore. An invalid argument error will be returned if the method is iqr. \
              In the histogram case, it means the probability (as a decimal) that has to be met for an event to be deemed anomalous. \
              In the zscore case, it means the same as in the annomalousvalue command.
default = If the method is zscore, then the default is 0.01 (as in the anomalousvalue command). If the method is histogram, the default is not any fixed value \
          but is instead calculated for each data set during the analysis.

[anoma-cutoff-option]
syntax = cutoff=(true|false)
description = This option applies to histogram method only. If the cutoff is false, the algorithm uses the formula threshold = 1st-quartile - 1.5*iqr without \
                modification. If the cutoff is true, the algorithm modifies the above formula in order to come up with a smaller number of anomalies.
default = "cutoff=true"


##################
# append
##################
[append-command]
syntax = append (<subsearch-options>)* <subsearch>
shortdesc = Appends the results of a subsearch results to the current results.
description = Append the results of a subsearch to the current search as additional results at the end of the current results.
comment = Append the current results with the tabular results of errors.
example = ... | chart count by category1 | append [search error | chart count by category2]
maintainer = steveyz
appears-in = 4.0
usage = public
tags = append join combine unite combine
category = results::append
related = appendcols, join, set

[subsearch-options]
syntax = maxtime=<int> | maxout=<int> | timeout=<int>
description = maxtime provides the maximum number of seceonds to run the subsearch before finalizing.  maxout provides the maximum number of results to emit from the subsearch. timeout provides the maximum number of seconds to wait for the search to fully finish.

##################
# appendcols
##################
[appendcols-command]
syntax = appendcols (override=<bool> | <subsearch-options>)* <subsearch>
shortdesc = Appends the fields of the subsearch results to current results, first results to first result, second to second, etc.
description = Appends fields of the results of the subsearch into input search results by combining the external fields of the subsearch (fields that do not start with '_') into the current results.  The first subsearch result is merged with the first main result, the second with the second, and so on.  If option override is false (default), if a field is present in both a subsearch result and the main result, the main result is used.  If it is true, the subsearch result's value for that field is used.
comment = Search for "404" events and append the fields in each event to the previous search results.
example = ... | appendcols [search 404]
maintainer = steveyz
appears-in = 4.0
usage = public
tags = append join combine unite
category = fields::add
related = append, join, set

##################
# appendpipe
##################
[appendpipe-command]
syntax = appendpipe (run_in_preview=<bool>)? [<subpipeline>]
description = Appends the result of the subpipeline applied to the current result set to results
comment = Append subtotals for each action across all users
example = index=_audit | stats count by action user | appendpipe [stats sum(count) as count by action | eval user = "ALL USERS"] | sort action
maintainer = steveyz
appears-in = 4.2
usage = public
tags = append join combine unite combine
category = results::append
related = append appendcols join set


##################
# arules
##################

[arules-command]
syntax = arules (<arules-option> )* <fields>
shortdesc = Find association rules between field values
description = Finding association rules between values. This is the algorithm behind most online \
	    shopping websites. When a customer buys an item, these sites are able to recommend \
	    related items that other customers also buy when they buy the first one. Arules finds such relationships and not only for \
	    shopping items but any kinds of fields. Note that stricly speaking, arules does not find relationships between fields, but rather \
	    between the values of the fields.
maintainer = nnguyen
appears-in = 6.0
usage = public
example1 = ... | arules field1 field2 field3
comment1 = Running arules with default support (=3) and confidence (=.5) \
	 The minimum number of fields is 2. There is no maximum restriction.
example2 = ... | arules sup=3 conf=.6 field1 field2 field3
comment2 = The sup option must be a positive integer \
	 The conf option must be a float between 0 and 1 \
	 In general, the higher the support, the less noisy the output will be. However, setting the support too high may exclude too much \
	 useful data in some circumstances. The conf option should be at least 0.5, otherwise the associations will not be significant. The higher \
	 the conf, the more significant the associations will be, but at the expense of retaining less associations.

category = streaming, reporting
related = associate, correlate
tags = associate contingency correlate correspond dependence independence

[arules-option]
syntax = <sup> | <conf>
default = ("sup=3" | "conf=.5")
description = The sup option specifies a required level of support, or computed level of association between fields.  Support is expressed as the output Support and Implied Support fields.  The conf option specifies a measure of how certain the algorithm is about that association. Confidence is expressed as the output Strength field.  (For example a small number of datapoints that are entirely equivalent would have high support but low confidence.)  For either option, asssociations which are below the limits will not be included in output results.


##################
# associate
##################

[associate-command]
syntax = associate (<associate-option> )* <field-list>?
shortdesc = Identifies correlations between fields.
description = Searches for relationships between pairs of fields.  More specifically, this command tries to identify \
	      cases where the entropy of field1 decreases significantly based on the condition of field2=value2. \
	      field1 is known as the target key and field2 the reference key and value2 the reference value. \
	      If a list of fields is provided, analysis will be restrict to only those fields.  By default all fields \
	      are used.
usage = public
maintainer = steveyz
appears-in = 3.0
comment1 = Analyze all fields to find a relationship.
example1 = ... | associate
comment2 = Analyze all events from host "reports" and return results associated with each other.
example2 = host="reports" | associate supcnt=50 supfreq=0.2 improv=0.5
commentcheat = Return results associated with each other (that have at least 3 references to each other).
examplecheat = ... | associate supcnt=3
category = reporting
related = correlate, contingency
tags = associate contingency correlate connect link correspond dependence independence

[associate-option]
syntax = <associate-supcnt-option>|<associate-supfreq-option>|<associate-improv-option>
description = Associate command options

[associate-supcnt-option]
syntax = supcnt=<int>
description = Minimum number of times the reference key=reference value combination must be appear. \
	      Must be a non-negative integer.
default = "supcnt=100"

[associate-supfreq-option]
syntax = supfreq=<num>
description = Minimum frequency of reference key=reference value combination, as a fraction of the number of total events.
default = "supfreq=0.1"

[associate-improv-option]
syntax = improv=<num>
description = Minimum entropy improvement for target key.  That is, \
	      entropy(target key) - entropy(target key given reference key/value) \
	      must be greater than or equal to this.
default = "improv=0.5"

##################
# audit
##################
[audit-command]
syntax = audit
shortdesc = Returns audit trail information that is stored in the local audit index.
description = View audit trail information stored in the local "audit" index. Also validate signed audit events while checking for gaps and tampering.
commentcheat = View information in the "audit" index.
examplecheat = index=audit | audit
category = administrative
usage = public
appears-in = 3.2
maintainer = rdas
tags = audit trail security
related = metadata

##################
# autoregress
##################
[autoregress-command]
syntax = autoregress <field> (AS <field:newfield>)? (p=<int:p_start>("-"<int:p_end>)?)?
shortdesc = Prepares events or results for calculating the moving average.
description = Sets up data for auto-regression (e.g. moving average) by copying one or more of the previous values for <field> into each event.  If <newfield> is provided, one prior value will be copied into <newfield> from a count of 'p' events prior.  In this case, 'p' must be a single integer.  If <newfield> is not provided, the single or multiple values will be copied into fields named '<field>_p<p-val>'.  In this case 'p' may be a single integer, or a range <p_start>-<p_end>.  For a range, the values will be copied from 'p_start' events prior to 'p_end' events prior.  If 'p' option is unspecified, it defaults to 1 (i.e., copy only the previous one value of <field> into <field>_p1.  The first few events will lack previous values, since they do not exist.
comment1 = Calculate a moving average of event size; the first N average numbers are omitted by eval since summing null fields results in null.
example1 = ... | eval rawlen=len(_raw) | autoregress rawlen p=1-4 | eval moving_average = (rawlen + rawlen_p1 + rawlen_p2 + rawlen_p3 + rawlen_p4) / 5
comment2 = For each event, copy the 2nd, 3rd, 4th, and 5th previous values of the 'count' field into the respective fields 'count_p2', 'count_p3', 'count_p4', and 'count_p5'.
example2 = ... | autoregress count p=2-5
comment3 = For each event, copy the 3rd previous value of the 'foo' field into the field 'oldfoo'.
example3 = ... | autoregress foo AS oldfoo p=3
usage = public
appears-in = 4.0
maintainer = steveyz
tags = average mean
alias = ar
category = reporting
related = accum, delta, streamstats, trendline

##################
# analyzefields
##################
[af-command]
syntax = analyzefields classfield=<field>
shortdesc = Finds degree of correlation between a target discrete field and other numerical fields.
description = Using <field> as a discrete random variable, analyze all *numerical* fields to determine the ability for each of those fields to "predict" the value of the classfield.\
	      In other words, analyzefields determines the stability of the relationship bteween values in the target classfield and numeric values in other fields. \i\\
	      As a reporting command, analyzefields consumes all input results, and generates one output result per identified numeric field. \i\\
	      For best results, classfield should have 2 distinct values, although multi-class analysis is possible.
comment1 = Analyze the numerical fields to predict the value of "is_activated".
example1 = ... | analyzefields classfield=is_activated
usage = public beta
appears-in = 4.0
maintainer = steveyz
alias = af
tags = analyze predict
category = reporting
related = anomalousvalue

##################
# accum
##################
[accum-command]
syntax = accum <field> (AS <field:newfield>)?
shortdesc = Keeps a running total of a specified numeric field.
description = For each event where <field> is a number, keep a running total of the sum of this number and write it out to either the same field, or a newfield if specified.
comment1 = Save the running total of "count" in a field called "total_count".
example1 = ... | accum count AS total_count
usage = public
appears-in = 4.0
maintainer = steveyz
category = fields::add
tags = total sum accumulate
related = autoregress, delta, streamstats, trendline

##################
# delta
##################
[delta-command]
syntax = delta <field> (AS <field:newfield>)? (p=<int>)?
shortdesc = Computes the difference in field value between nearby results.
description = For each event where <field> is a number, compute the difference, in search order, between the current event's value of <field> and a previous event's value of <field> and write this difference into <field:newfield>.  If <newfield> if not specified, it defaults to "delta(<field>)"   If p is unspecified, the default = 1, meaning the the immediate previous value is used.  p=2 would mean that the value before the previous value is used, etc etc etc.
note = Historical search order is from new events to old events, so values ascending over time will show negative deltas, and vice versa.  Realtime search is in the incoming data order, so delta can produce odd values for data which arrives out-of-order from the original data order (eg. when files are acquired out-of-order on forwarders).
example1 = ... | delta count AS countdiff
comment1 = For each event where 'count' exists, compute the difference between count and its previous value and store the result in 'countdiff'.
example2 = ... | delta count p=3
comment2 = Compute the difference between current value of count and the 3rd previous value of count and store the result in 'delta(count)'
usage = public
appears-in = 4.0
maintainer = steveyz
tags = difference delta change distance
category = fields::add
related = accum, autoregress, streamstats, trendline

##################
# bucket
##################

[bucket-command]
syntax = bucket (<bucketing-option> )* <field> (as <field>)?
alias = bin, discretize
shortdesc = Puts continuous numerical values into discrete sets.
description = Puts continuous numerical values in fields into discrete sets, or buckets, by adjusting the value of 'field', so that all items in the set have the same value for 'field'.  Note: Bucket is called by chart and timechart automatically and is only needed for statistical operations that timechart and chart cannot process.
usage = public
commentcheat1 = Bucket search results into 10 bins, and return the count of raw events for each bucket.
examplecheat1 = ... | bucket size bins=10 | stats count(_raw) by size
commentcheat2 = Return the average "thruput" of each "host" for each 5 minute time span.
examplecheat2 = ... | bucket _time span=5m | stats avg(thruput) by _time host
category = reporting
appears-in = 3.0
maintainer = steveyz
related = chart, timechart
tags = bucket band bracket round chunk lump span

[bucketing-option]
syntax = (<bucket-bins> <bucket-minspan>?)|<bucket-span>|<bucket-start-end>
description = Discretization option.

[bucket-minspan]
syntax = minspan=(<span-length>)
description = Specifies the smallest span granularity to use automatically inferring span from the data time range.

[bucket-bins]
syntax = bins=<int>
description = Sets the maximum number of bins to discretize into. \
	      Given this upper-bound guidance, the bins will snap to \
	      human sensible bounds.
note = The actual number of bins will almost certainly be smaller than the given number.
example1 = bins=10

[bucket-span]
syntax = span=(<span-length>|<log-span>)
description = Sets the size of each bucket.
comment1 = set span to 10 seconds
example1 = span=10
comment2 = set span to 2 days
example2 = span=2d
comment3 = set span to 5 minutes
example3 = span=5m

[log-span]
syntax = (<num>)?log(<num>)?
description = Sets to log based span, first number if coefficient, second number is base \
	      coefficient, if supplied, must be real number >= 1.0 and < base \
	      base, if supplied, must be real number > 1.0 (strictly greater than 1)
example1 = log
comment1 = set log span of base 10, coeff 1.0, e.g. ...,0.1,1,10,100,...
example2 = 2log5
comment2 = set log span of base 5, coeff 2.0, e.g. ...,0.4,2,10,50,250,1250,...

[bucket-start-end]
syntax = (start=<num>|end=<num>)
description = Sets the minimum and maximum extents for numerical buckets.
note = Data outside of the [start, end] range is discarded.

[span-length]
syntax = <int:span>(<timescale>)?
description = Span of each bin. \
	      If using a timescale, this is used as a time range.\
	      If not, this is an absolute bucket "length."
comment1 = set span to 10 seconds
example1 = 10
comment2 = set span to 2 days
example2 = 2d
comment3 = set span to 5 minutes
example3 = 5m

[timescale]
syntax = <ts-sec>|<ts-min>|<ts-hr>|<ts-day>|<ts-month>|<ts-subseconds>
description = Time scale units.

[ts-subseconds]
syntax = us|ms|cs|ds
description = Time scale in microseconds("us"), milliseconds("ms"), \
	      centiseconds("cs"), or deciseconds("ds")

[ts-sec]
syntax = s|sec|secs|second|seconds
simplesyntax = seconds
description = Time scale in seconds.

[ts-min]
syntax = m|min|mins|minute|minutes
simplesyntax = minutes
description = Time scale in minutes.

[ts-hr]
syntax = h|hr|hrs|hour|hours
simplesyntax = hours
description = Time scale in hours.

[ts-day]
syntax = d|day|days
simplesyntax = days
description = Time scale in days.

[ts-month]
syntax = mon|month|months
simplesyntax = months
description = Time scale in months.

##################
# chart
##################

[chart-command]
simplesyntax = chart (agg=<stats-agg-term>)? ( <stats-agg-term> | ( "(" <eval-expression> ")" ) )+ ( BY <field> (<bucketing-option> )* (<split-by-clause>)? )? | ( over <field> (<bucketing-option>)* (by <split-by-clause>)? )?
syntax = chart (sep=<string>)? (format=<string>)? (cont=<bool>)? (limit=<int>)? (agg=<stats-agg-term>)? ( <stats-agg-term> | <sparkline-agg-term> | ( "(" <eval-expression> ")" ) )+ \
       	       ( by <field> (<bucketing-option> )* (<split-by-clause>)? )? | \
	       ( over <field> (<bucketing-option>)* (by <split-by-clause>)? )?
shortdesc = Returns results in a tabular output for charting.
description = Creates a table of statistics suitable for charting.  Whereas timechart generates a \
              chart with _time as the x-axis, chart lets you select an arbitrary field as the \
              x-axis with the "by" or "over" keyword. If necessary, the x-axis field is converted \
              to discrete numerical quantities.\p\\
              When chart includes a split-by-clause, the columns in the output table represents a \
              distinct value of the split-by-field. (With stats, each row represents a single \
              unique combination of values of the group-by-field. The table displays ten columns \
              by default, but you can specify a where clause to adjust the number of columns.\p\\
              When a where clause is not provided, you can use limit and agg options to specify \
              series filtering. If limit=0, there is no series filtering. \p\\
              When specifying multiple data series with a split-by-clause, you can use sep and \
              format options to construct output field names.
commentcheat1 = Return the average (mean) "size" for each distinct "host".
examplecheat1 = ... | chart avg(size) by host
commentcheat2 = Return the the maximum "delay" by "size", where "size" is broken down into a maximum of 10 equal sized buckets.
examplecheat2 = ... | chart max(delay) by size bins=10
commentcheat3 = Return the ratio of the average (mean) "size" to the maximum "delay" for each distinct "host" and "user" pair.
examplecheat3 = ... | chart eval(avg(size)/max(delay)) by host user
commentcheat4 = Return max(delay) for each value of foo split by the value of bar.
examplecheat4 = ... | chart max(delay) over foo by bar
commentcheat5 = Return max(delay) for each value of foo.
examplecheat5 = ... | chart max(delay) over foo
category = reporting
appears-in = 3.0
usage = public
maintainer = steveyz
supports-multivalue = true
related = timechart, bucket, sichart
tags = chart graph report sparkline count dc mean avg stdev var min max mode median


##################
# cofilter
##################

[cofilter-command]
syntax = cofilter field1 field2
shortdesc = Find how many times field1 and field2 values occurred together
description = For this command, we think of field1 values as "users" and field2 values as "items". \
	    The goal of the command is to compute, for each pair of item (i.e., field2 values), how many \
	    users (i.e., field1 values) used them both (i.e., occurred with each of them).
maintainer = nnguyen
appears-in = 6.0
usage = public
example1 = ... | cofilter field1 field2
comment1 = user field must be specified first and item field second
category = streaming, reporting
related = associate, correlate
tags = arules associate contingency correlate correspond dependence independence


##################
# collapse
##################
[collapse-command]
syntax = collapse (chunksize=<num>)? (force=<bool>)?
description = Purely internal operation that condenses multi-file results into as few files as chunksize option will allow.  (default chunksize=50000).  Operation automatically invoked by output* operators.  If force=true and the results are entirely in memory, re-divide the results into appropriated chunked files (this option is new for 5.0).
maintainer = steveyz
example1 = ... | collapse
usage = internal
appears-in = 3.2
optout-in = lite, lite_free


##################
# concurrency
##################

[concurrency-command]
syntax = concurrency duration=<field> (start=<field>)? (output=<field>)?
shortdesc = Given a duration field, finds the number of "concurrent" events for each event
description = If each event represents something that occurs over a span of time, where that \
	    span is specified in the duration field, calculate the number of concurrent events \
	    for each event start time.  An event X is concurrent with event Y if \
	    the X start time, X.start, lies between Y.start and (Y.start + Y.duration). \
	    In other words, the concurrent set of events is calculated for each event start time, \
	    and that number is attached to the event.  \
	    The units of start and duration are assumed to be the same.  If you have different \
	    units, you will need to convert them to corresponding units prior to using the concurrency \
	    command. \
	    Unless specified, the start field is assumed to be _time and the output field will \
	    be 'concurrency' \
	    Limits: If concurrency exceeds limits.conf [concurrency] max_count \
	    (Defaults to 10 million), results will not be accurate.
maintainer = steveyz
appears-in = 4.1
usage = public
comment1 = Calculate the number of concurrent events for each event start time and emit as field 'foo'
example1 = ... | concurrency duration=total_time output=foo
commentcheat = Calculate the number of concurrent events using the 'et' field as the start time \
	       and 'length' as the duration.
examplecheat = ... | concurrency duration=length start=et
comment2 = Calculate the number of ongoing http requests at the start time of each http request in a splunk access log
example2 = ... | eval spent_in_seconds = spent / 1000 | concurrency duration=spent_in_seconds
category = reporting
related = timechart
tags = concurrency

##################
# contingency
##################

[contingency-command]
syntax = contingency (<contingency-option> )* <field> <field>
alias = counttable, ctable
shortdesc = Builds a contingency table for two fields.
description = In statistics, contingency tables are used to record \
	      and analyze the relationship between two or more (usually categorical) variables.  Many metrics of \
	      association or independence can be calculated based on contingency tables, such as the phi \
              coefficient or the V of Cramer.
maintainer = steveyz
appears-in = 3.0
usage = public
comment1 = Build a contingency table for fields "host" and "sourcetype".
example1 = ... | contingency host sourcetype
commentcheat = Build a contingency table of "datafields" from all events.
examplecheat = ... | contingency datafield1 datafield2 maxrows=5 maxcols=5 usetotal=F
category = reporting
related = associate, correlate
tags = associate contingency correlate connect link correspond dependence independence

[contingency-option]
syntax = <contingency-maxopts>|<contingency-mincover>|<contingency-usetotal>|<contingency-totalstr>
description = Options for the contingency table

[contingency-maxopts]
syntax = (maxrows|maxcols)=<int>
description = Maximum number of rows or columns.  If the number of distinct values of the field exceeds this maximum, \
	      the least common values will be ignored.  There is a ceiling on the values permitted for maxrows and maxcols\
	      from limits.conf, [ctable] stanza maxvalues. This limit defaults to 1000.  Values for over this will be rejected \
	      and values of 0 for these settings mean to use this maxvalues setting.
default = ("maxrows=0" | "maxcols=0")

[contingency-mincover]
syntax = (mincolcover|minrowcover)=<num>
description = Cover only this percentage of values for the row or column field.  If the number of entries needed to \
              cover the required percentage of values exceeds maxrows or maxcols, maxrows or maxcols takes precedence.
default = ("mincolcover=1.0" | "minrowcover=1.0")

[contingency-usetotal]
syntax = usetotal=<bool>
description = Add row and column totals
default = "usetotal=true"

[contingency-totalstr]
syntax = totalstr=<field>
description = Field name for the totals row/column
default = "totalstr=TOTAL"


##################
# convert
##################

[convert-command]
simplesyntax = convert (timeformat=<string>)? ( (auto|dur2sec|mstime|memk|none|num|rmunit|rmcomma|ctime|mktime) "(" <field>? ")" (as <field>)?)+
syntax = convert (timeformat=<string>)? (<convert-function> (as <wc-field>)?)+
shortdesc = Converts field values into numerical values.
description = Converts the values of fields into numerical values. When renaming a field using "as", the original field is left intact. The timeformat option is used by ctime and mktime conversions.  Default = "%m/%d/%Y %H:%M:%S".
maintainer = steveyz
appears-in = 3.0
commentcheat1 = Convert every field value to a number value except for values in the field "foo" (use the "none" argument to specify fields to ignore).
examplecheat1 = ... | convert auto(*) none(foo)
commentcheat2 = Change all memory values in the "virt" field to Kilobytes.
examplecheat2 = ... | convert memk(virt)
commentcheat3 = Change the sendmail syslog duration format (D+HH:MM:SS) to seconds. For example, if "delay="00:10:15"", the resulting value will be "delay="615"".
examplecheat3 = ... | convert dur2sec(delay)
commentcheat4 = Convert values of the "duration" field into number value by removing string values in the field value. For example,  if "duration="212 sec"", the resulting value will be "duration="212"".
examplecheat4 = ... | convert rmunit(duration)
category = fields::convert
usage = public
tags = interchange transform translate convert ctime mktime dur2sec mstime memk
related = eval

[convert-function]
syntax = <convert-auto>|<convert-dur2sec>|<convert-mstime>|<convert-memk>|<convert-none>|<convert-num>|<convert-rmunit>|<convert-rmcomma>|<convert-ctime>|<convert-mktime>

[convert-auto]
syntax = auto("(" (<wc-field>)? ")")?
description = Automatically convert the field(s) to a number using the best conversion. \
	      Note that if not all values of a particular field can be converted using a known conversion type, \
	      the field is left untouched and no conversion at all in done for that field.
example1 = ... | convert auto(*)
example2 = ... | convert auto
example3 = ... | convert auto()
example4 = ... | convert auto(delay) auto(xdelay)
example5 = ... | convert auto(delay) as delay_secs
example6 = ... | convert auto(*delay) as *delay_secs
example7 = ... | convert auto(*) as  *_num

[convert-ctime]
syntax = ctime"("<wc-field>?")"
description = Convert an epoch time to an ascii human readable time.  Use timeformat option to specify exact format to convert to.
example1 = ... | convert timeformat="%H:%M:%S" ctime(_time) as timestr

[convert-mktime]
syntax = mktime"("<wc-field>?")"
description = Convert an human readable time string to an epoch time.  Use timeformat option to specify exact format to convert from.
example1 = ... | convert mktime(timestr)

[convert-dur2sec]
syntax = dur2sec"("<wc-field>?")"
description = Convert a duration format "[D+]HH:MM:SS" to seconds.
example1 = ... | convert dur2sec(xdelay)
example2 = ... | convert dur2sec(*delay)

[convert-mstime]
syntax = mstime"(" <wc-field>? ")"
description = Convert a MM:SS.SSS format to seconds.

[convert-memk]
syntax = memk"(" <wc-field>? ")"
description = Convert a {KB, MB, GB} denominated size quantity into a KB
example1 = ... | convert memk(VIRT)

[convert-none]
syntax = none"(" <wc-field>? ")"
description = In the presence of other wildcards, indicates that the matching fields should not be converted.
example1 = ... | convert auto(*) none(foo)

[convert-num]
syntax = num"("<wc-field>? ")"
description = Like auto(), except non-convertible values are removed.

[convert-rmcomma]
syntax = rmcomma"("<wc-field>? ")"
description = Removes all commas from value, e.g. '1,000,000.00' -> '1000000.00'

[convert-rmunit]
syntax = rmunit"(" <wc-field>? ")"
description = Looks for numbers at the beginning of the value and removes trailing text.
example1 = ... | convert rmunit(duration)

##################
# copyresults
##################

[copyresults-command]
syntax      = copyresults <copyresults-dest-option> <copyresults-sid-option>
description = Copies the results of a search to a specified location within the config directory structure. This command is primarily used to populate lookup tables.
maintainer = ledion
appears-in = 4.0
usage      = internal
example1   = ... | copyresults dest=etc/system/local/lookups/myLookupTable.csv

[copyresults-dest-option]
syntax      = dest=<string>
description = The destination file where to copy the results to. The string is interpreted as path relative \
              to SPLUNK_HOME and (1) should point to a .csv file and (2) the file should be located either \
              in etc/system/lookups/ or etc/apps/<app-name>/lookups/

[copyresults-sid-option]
syntax      = sid=<string>
description = The search id of the job whose results are to be copied. Note, the user who is running this \
              command should have permission to the job pointed by this id.

##################
# correlate
##################

[correlate-command]
syntax = correlate
shortdesc = Calculates the correlation between different fields.
description = Calculates a co-occurrence matrix, which contains the percentage of times that two \
	      fields exist in the same events.  The RowField field contains the name of the field considered \
	      for the row, while the other column names (fields) are the fields it is being compared against. \
	      Values are the ratio of occurrences when both fields appeared to occurrences when only one field appeared.
maintainer = steveyz
appears-in = 3.0
usage = public
example1 = ... | correlate
comment1 = Calculate the correlation between all fields.
commentcheat = Calculate the co-occurrence correlation between all fields.
examplecheat = ... | correlate
category = reporting
related = associate, contingency
tags = associate contingency correlate connect link correspond dependence independence

##################
# crawl
##################
[crawl-command]
syntax      = crawl (files|network)? (<crawl-option> )*
shortdesc = Crawls the filesystem for files of interest to Splunk, or network for ports of interest.
description = Crawls for the discovery of new sources to index.  Default crawl settings are found in crawl.conf and crawl operations are logged to $SPLUNK_HOME/var/log/splunk/crawl.log. Generally to be used in conjunction with the "input" command.  Specify crawl options to override settings in crawl.conf.  Note: If you add crawl to a search, Splunk only returns data it generates from crawl. Splunk doesn't return any data generated before | crawl.  "files" is the default crawl type, but "network" can also be specified.
comment1 = Crawl using default settings defined in crawl.conf.
example1 = | crawl
comment2 = Crawl bob's home directory.
example2 = | crawl root=/home/bob
comment3 = Add all sources found in bob's home directory to the 'preview' index.
example3 = | crawl root=/home/bob | input add index=preview
commentcheat = Crawl root and home directories and add all possible inputs found (adds configuration information to "inputs.conf").
examplecheat =  | crawl root="/;/Users/" | input add
category = administrative
generating = true
maintainer  = david
appears-in  = 3.2-Britney
usage       = public
tags = discover crawl explore probe
related = input

[crawl-option]
syntax = <string>=<string>
description = Override settings from crawl.conf.
example1 = root=/home/bob


##################
# createrss
##################
[createrss-command]
syntax      = createrss path=<string> name=<string> link=<string> descr=<string> count=<int> (graceful=0|1)?
shortdesc = Adds the RSS item into the specified RSS feed.
description = If the RSS feed does not exist, it creates one. The arguments are as follow \i\\
              PATH  - the path of the rss feed (no ../ allowed) can be accessed via http://splunk/rss/path    \i\\
              NAME  - the name/title of the rss item to add \i\\
              LINK  - link where the rss item points to \i\\
              DESCR - the description field of the rss item \i\\
              COUNT - maximum number of items in the rss feed when reached last items is dropped \i\\
              GRACEFUL - (optional) controls whether on error an exception is raised or simply logged - this is \i\\
                         useful when you don't want createrss to break the search pipeline
maintainer = ledion
appears-in = 3.0
usage = internal
related = sendemail
category = alerting

##################
# datamodel
##################

[datamodel-command]
syntax      = datamodel (<modelName>)? (<objectName>)? (search)?
shortdesc   = Allows user to examine data models and run the search for a datamodel object.
description = Must be the first command in a search.  When used with no arguments, returns the JSON for all datamodels available in the \
	      current context. When used with just a modelName, returns JSON for a single model.  When used with a modelName and objectName, \
	      returns the JSON for a single object.  When used with modelName, objectName and 'search', runs the search for the specified \
	      object.
example1 =  | datamodel
#example2 =  | datamodel myModel
#example3 =  | datamodel myModel myObject
#example4 =  | datamodel myModel myObject search
category = results::filter
appears-in = 6.0
maintainer = aneels
usage = public
related = pivot
tags = datamodel model pivot
optout-in = lite, lite_free

##################
# dedup
##################

[dedup-command]
syntax = dedup (<int:N>)? <field-list> <dedup-keepevents>? <dedup-keepempty>? <dedup-consecutive>? (sortby <sort-by-clause>)?
shortdesc = Removes events which contain an identical combination of values for selected fields.
description = Keep the first N (where N > 0) results for each combination of values for the specified field(s) \
	      The first argument, if a number, is interpreted as N.  If this number is absent, N is assumed to be 1. \
	      The optional sortby clause is equivalent to performing a sort command before the dedup command except that it is executed more efficiently.  The keepevants flag will keep all events, but for events with duplicate values, remove those fields values instead of the entire event. \p\\
	      Normally, events with a null value in any of the fields are dropped.  The keepempty \
	      flag will retain all events with a null value in any of the fields.
maintainer = steveyz
appears-in = 3.2
usage = public beta
example1 = ... | dedup 3 source
comment1 = For events that have the same 'source' value, keep the first 3 that occur and remove all subsequent events.
example2 = ... | dedup source sortby +_time
comment2 = Remove duplicates of results with the same source value and sort the events by the '_time' field in ascending order.
example3 = ... | dedup group sortby -_size
comment3 = Remove duplicates of results with the same source value and sort the events by the '_size' field in descending order.
commentcheat = Remove duplicates of results with the same host value.
examplecheat = ... | dedup host
category = results::filter
tags = duplicate redundant extra
related = uniq

[dedup-keepempty]
syntax = keepempty=<bool>
description = If an event contains a null value for one or more of the specified fields, the event is either \
	      retained (if keepempty=true) or discarded (default).
default = "keepempty=f"

[dedup-consecutive]
syntax = consecutive=<bool>
description = Only eliminate events that are consecutively duplicate
default = "consecutive=f"

[dedup-keepevents]
syntax = keepevents=<bool>
description = Keep all events, remove the fields from field-list in the duplication case instead
default = "keepevents=f"

##################
# diff
##################
[diff-command]
syntax = diff (position1=<int>)? (position2=<int>)? (attribute=<string>)? (diffheader=<bool>)? (context=<bool>)? (maxlen=<int>)?
shortdesc = Returns the difference between two search results.
description = Compares a field from two search results, returning the line-by-line 'diff' of the two. \
	      The two search results compared is specified by the two position values (position1 and position2), \
	      hich default to 1 and 2 (i.e., compare the first two results).  \p\\
	      By default, the text of the two search results (i.e., the "_raw" field) are compared, \
	      but other fields can be compared, using 'attribute'.  \p\\
	      If 'diffheader' is true, the traditional diff headers are created using the source keys \
	      of the two events as filenames. 'diffheader' defaults to false.  \p\\
	      If 'context' is true, the output is generated in context-diff format.  Otherwise, unified diff format is used.\
	      'context' defaults to false (unified). \p\\
	      If 'maxlen' is provided, it controls the maximum content in bytes diffed from the two events. \
	      It defaults to 100000, meaning 100KB, if maxlen=0, there is no limit.
default = diff position1=1 position2=2 attribute=_raw header=f context=f
comment1 = Compare the 9th search results to the 10th
example1 = ... | diff position1=9 position2=10
commentcheat = Compare the "ip" values of the first and third search results.
examplecheat = ... | diff pos1=1 pos2=3 attribute=ip
category = formatting
maintainer = david
usage = public
appears-in=3.0
tags = diff differentiate distinguish contrast
related = set

##################
# dispatch
##################

[dispatch-command]
syntax = dispatch (ttl=<num>)? (maxresults=<num>)? (maxtime=<num>)? (id=<string>)? (spawn_process=<bool>)? (label=<string>)? (start_time=<num>)? (end_time=<num>)? <server-list> [<search-pipeline>]
description = Encapsulates long running, streaming reports. \i\\
  "id" is the directory in which to place the results relative to $SPLUNK_HOME/var/run/splunk/dispatch.\i\\
  "maxresults" is the maximum number of final results to return from the search-pipeline\i\\
  "maxtime" is the maximum time (in seconds) to spend on the search before finalizing\i\\
  "ttl" represents the number of the seconds the results of the dispatched search-pipeline will live on\i\\
  disk before being cleaned up\i\\
  "spawn_process" controls if the search should run in a separate spawned process ( defaults to true ).\i\\
  "start_time" set the search's start/earliest time\i\\
  "end_time"   set the search's end/latest time\i\\
  "label"      set the search's label
appears-in = 3.2
usage = internal
maintainer = ssorkin
example1 = | dispatch [search | stats count]
example2 = | dispatch id=foo [search | top source]
example3 = | dispatch server1 server2 [search | top host]
optout-in = lite, lite_free

[server-list]
syntax = (<string> )*
description = A list of possibly wildcarded servers changes in the context of the differences. Try it see if it makes sense.  \
	    * - header=[true | false] : optionally you can show a header that tries to explain the diff output \
	    * - attribute=[attribute name] : you can choose to diff just a single attribute of the results.
appears-in = 3.x
default = "*"


##################
# editinfo
##################
[editinfo-command]
syntax = editinfo ((keyset|starttime|endtime|msg_error|msg_warn|msg_info|msg_debug)=<string>)*
description = Edit information in SearchResultsInfo
category = formatting
appears-in = 3.x
maintainer = steveyz
usage = internal
related = editinfo
tags = editinfo
optout-in = lite, lite_free


##################
# erex
##################

[erex-command]
syntax    = erex <field> examples=<erex-examples> (counterexamples=<erex-examples>)? (fromfield=<field>)? (maxtrainers=<int>)?
shortdesc = Automatically extracts field values similar to the example values.
description = Example-based regular expression \
   extraction. Automatically extracts field values from FROMFIELD \
   (defaults to _raw) that are similar to the EXAMPLES \
   (comma-separated list of example values) and puts them in FIELD. \
   An informational message is output with the resulting regular expression.  \
   That expression can then be used with the REX command for \
   more efficient extraction.  To learn the extraction rule for \
   pulling out example values, it learns from at most MAXTRAINERS \
   (defaults to 100, must be between 1-1000).
comment1 = Extracts out values like "7/01", putting them into the "monthday" attribute.
example1 = ... | erex monthday examples="7/01"
comment2 = Extracts out values like "7/01" and "7/02", but not patterns like "99/2", putting extractions into the "monthday" attribute.
example2 = ... | erex monthday examples="7/01, 07/02" counterexamples="99/2"
category = fields::add
appears-in = 4.0
maintainer = david
usage = public
related = extract, kvform, multikv, regex, rex, xmlkv
tags = regex regular expression extract

[erex-examples]
syntax = ""<string>(, <string> )*""
comment1 = examples are foo and bar
example1 = "foo, bar"


#################
# eval
#################

[eval-command]
syntax = eval <eval-field>=<eval-expression>
shortdesc = Calculates an expression and puts the resulting value into a field.
description = Performs an arbitrary expression evaluation, providing mathematical, string, and boolean operations. The results of eval are written to a specified destination field, which can be a new or existing field. If the destination field exists, the values of the field are replaced by the results of eval. The syntax of the expression is checked before running the search, and an exception will be thrown for an invalid expression. For example, the result of an eval statement is not allowed to be boolean. If search time evaluation of the expression is unsuccessful for a given event, eval erases the value in the result field.
commentcheat = Set velocity to distance / time.
examplecheat = ... | eval velocity=distance/time
comment1 = Set full_name to the concatenation of first_name, a space, and last_name.
example1 = ... | eval full_name = first_name." ".last_nameSearch
comment2 = Set sum_of_areas to be the sum of the areas of two circles
example2 = ... | eval sum_of_areas = pi() * pow(radius_a, 2) + pi() * pow(radius_b, 2)
comment3 = Set status to some simple http error codes.
example3 = ... | eval error_msg = case(error == 404, "Not found", error == 500, "Internal Server Error", error == 200, "OK")
comment4 = Set status to OK if error is 200; otherwise, Error.
example4 = ... | eval status = if(error == 200, "OK", "Error")
comment5 = Set lowuser to the lowercase version of username.
example5 = ... | eval lowuser =  lower(username)
category = fields::add
related = where
appears-in = Madonna
usage = public
maintainer = marquardt
tags = evaluate math string bool formula calculate compute abs case cidrmatch coalesce commands exact exp floor if ifnull isbool isint isnotnull isnull isnum isstr len like ln log lower match max md5 min mvappend mvcount mvindex mvfilter mvjoin mvsort mvdedup now null nullif pi pow random relative_time replace round searchmatch sigfig split sqrt strftime strptime substr time tostring trim ltrim rtrim typeof upper urldecode validate

[eval-field]
syntax = <field>
description = A field name for your evaluated value.
example = velocity
tags = eval evaluate calculate add subtract sum count measure multiply divide

[eval-expression]
syntax = <eval-math-exp> | <eval-concat-exp> | <eval-compare-exp> | <eval-bool-exp> | <eval-function-call>
description = A combination of literals, fields, operators, and functions that represent the value of your destination field.  The following are the basic operations you can perform with eval. For these evaluations to work, your values need to be valid for the type of operation. For example, with the exception of addition, arithmetic operations may not produce valid results if the values are not numerical. For addition, Splunk can concatenate the two operands if they are both strings. When concatenating values with '.', Splunk treats both values as strings regardless of their actual type.
tags = eval evaluate calculate add subtract sum count measure multiply divide where

[eval-math-exp]
syntax =  (<field>|<num>) ((+|-|*|/|%) <eval-expression>)*
example = pi() * pow(radius_a, 2) + pi() * pow(radius_b, 2)

[eval-concat-exp]
syntax = ((<field>|<string>|<num>) (. <eval-expression>)*)|((<field>|<string>) (+ <eval-expression>)*)
description = concatenate fields and strings
comment = create a new field by concatenating the field first_name, a space character, and the field last_name.
example = first_name." ".last_nameSearch

[eval-compare-exp]
syntax = (<field>|<string>|<num>) (<|>|<=|>=|!=|=|==|LIKE) <eval-expression>

[eval-bool-exp]
syntax = (NOT|!)? (<eval-compare-exp>|<eval-function-call>) ((AND|OR|XOR) <eval-expression>)*

[eval-function-call]
syntax = <eval-function> "(" <eval-expression> ("," <eval-expression>)* ")"

[eval-function]
syntax = abs|case|cidrmatch|coalesce|commands|exact|exp|floor|if|ifnull|isbool|isint|isnotnull|isnull|isnum|isstr|len|like|ln|log|lower|match|max|md5|min|mvappend|mvcount|mvdedup|mvindex|mvfilter|mvjoin|mvrange|mvsort|mvzip|now|null|nullif|pi|pow|random|relative_time|replace|round|searchmatch|sigfig|spath|split|sqrt|strftime|strptime|substr|time|tostring|trim|ltrim|rtrim|typeof|upper|urldecode|validate
description = Function used by eval.
example1 = abs(number)
comment1 = Takes a number and returns its absolute value.
example2 = case(error == 404, "Not found", error == 500, "Internal Server Error", error == 200, "OK")
comment2 = Takes an even number of arguments with arguments 1,3,5, etc. being boolean expressions. The function returns the argument following the first expression that evaluates to true, defaulting to NULL if none are true.
example3 = cidrmatch("123.132.32.0/25", ip)
comment3 = Takes two arguments, the first being the subnet to match and the second being an ip address. This boolean function returns true if the ip matches the valid subnet, or false otherwise.
example4 = coalesce(null(), "Returned value", null())
comment4 = Takes any number of arguments and returns the first value that is not null. The ifnull function does the exact same thing, so both names are acceptable.
example5 = exact(3.14 * num)
comment5 = Takes a number as its argument, and returns the exact value of the result without truncating for significant figures or precision. Uses double precision.
example6 = exp(3)
comment6 = Takes a number x and returns e^x.
example7 = floor(1.9)
comment7 = Takes a number x and returns the floor of x, which in this example is 1.
example8 = if(error == 200, "OK", "Error")
comment8 = Takes three arguments, the first being a boolean expression. The function returns the second argument if it evaluates to true, and the third otherwise.
example9 = isbool(field)
comment9 = Takes one argument, returning true iff the argument is boolean. There are corresponding functions for numbers (isnum), integers (isint), strings (isstr), and null (isnull).
example10 = isnotnull(field)
comment10 = Takes one argument, returning true iff the field is not null. A useful check for whether the field contains a value.
example11 = len(field)
comment11 = Takes one string argument, returning the length of the string. In this example it would return the length of the string in field.
example12 = like(field, "foo%")
comment12 = Takes two string arguments, returning true iff the first argument is like the SQLite pattern in the second argument. This example returns true if the field value starts with foo.
example13 = ln(bytes)
comment13 = Takes a number and returns its natural log.
example14 = log(number, 2)
comment14 = Takes either one or two numeric arguments, returning the log of the first argument using the base provided by the second argument, in this case log of number base 2. The default base is 10 if the second argument is omitted.
example15 = lower(username)
comment15 = Takes one string argument and returns the lowercase version, in this example lowercasing the value provided by the field username. The upper function also exists for returning the uppercase version.
example16 = match(field, "^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$")
comment16 = Takes two string arguments, and returns true iff the first argument matches the regex provided by the second argument. This example returns true iff field matches the basic pattern of an ip address. Note that the example used ^ and $ to perform a full match.
example17 = max(1, 3, 6, 7, "f"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$")oo", field)
comment17 = Takes an arbitrary number of number and string arguments and returns the max, with strings being greater than numbers. So this example will return either "foo" or field, depending on the value of field. The corresponding min function also exists.
example18 = md5(field)
comment18 = Takes one string argument and returns its md5 hash value as a string, in this case returning the hash of field's value.
example19 = mvcount(multifield)
comment19 = Takes a field argument, returning the number of values if the field is multivalued, one if the field is single valued and has a value, and null otherwise.
example20 = mvindex(multifield, 2)
comment20 = Takes two or three arguments, the first being a field and the second two being numbers, which returns a subset of the multivalued field using the indexes provided. For mvindex(mvfield, startindex, [endindex]) endindex is inclusive and optional. Both start and endindex can be negative, where -1 = last element, and if endindex is not specified, it returns just the value at startindex. If the indexes are out of range or invalid, the result is null. Since indexes start at zero, this example returns the third value in multifield if it exists.
example21 = mvfilter(match(email, "\.net$") OR match(email, "\.org$"))
comment21 = Takes one argument which is a boolean expression that references EXACTLY one field. It returns that multivalued field filtered by the given expression, so this example will return all values of the field email that end in .net or .org.
example22 = now()
comment22 = Takes no arguments and returns the time that the search was started, in unix time (seconds since epoch).
example23 = null()
comment23 = Takes no arguments and returns null, which is how the evaluation engine represents no value. Setting a field to null will clear its value.
example24 = nullif(fielda, fieldb)
comment24 = Takes two arguments, returning the first argument if the arguments are different, and null otherwise.
example25 = pi()
comment25 = Takes no arguments, returning the pi constant to 11 digits of precision.
example26 = pow(x, y)
comment26 = Takes two numeric arguments, returning x^y
example27 = random()
comment27 = Takes no arguments, returns a pseudo-random number.
example28 = replace(date, "^(\d{1,2})/(\d{1,2})/", "\2/\1/")
comment28 = Takes three arguments - replace(input_string, regex_pattern_to_replace, replacement_string) returns the input string with all instances of the regex pattern replaced with the replacement string. You can also include matched groups in the third argument by escaping the group number matched. The example returns date with the month and day numbers switched, so if the input was 1/12/2009 the return value would be 12/1/2009.
example29 = round(3.5)
comment29 = Takes one or two numeric arguments, returning the first argument rounded to the amount of decimal places specified by the second. The default is to round to an integer, so the example would return 4. round(2.555, 2) would return 2.56.
example30 = searchmatch("foo AND bar")
comment30 = Takes one argument, which is a search string. The function returns true iff the event matches the search string.
example31 = sqrt(9)
comment31 = Takes one numeric argument and returns its square root, in this example it would be 3.
example32 = substr("string", 1, 3) + substr("string", -3)
comment32 = Takes either two or three arguments - the first is a string and the last two are numeric. The function returns a substring of the first argument starting at the index specified by the second argument with the number of characters specified by the third. If a third argument is not given, it returns the rest of the string. The indexes follow SQLite semantics in that they start at 1 and negative indices can be used, which start from the end of the string. The example concatenates "str" and "ing" together, returning "string"
example33 = tostring(1==1) + " " + tostring(15, "hex") + " " + tostring(12345.6789, "commas")
comment33 = Takes one or two arguments, returning a string representation of the first argument with optional formatting for numbers. For numbers you can specify "hex" or "commas" as a second argument to format the number accordingly. The return value for this example is "True 0xF 12,345.68".
example34 = trim("   ZZZZabcZZ ", " Z")
comment34 = Takes one or two string arguments, and returns the first string with the characters in the second argument trimmed from both sides. If no second argument is specified, spaces and tabs are trimmed. This example returns "abc". There are also ltrim and rtrim functions for trimming only the left and right sides, respectively.
example35 = typeof(12) + typeof("string") + typeof(1==2) + typeof(badfield)
comment35 = Takes one argument and returns a string representation of its type. The example result is NumberStringBoolInvalid.
example36 = urldecode("http%3A%2F%2Fwww.splunk.com%2Fdownload%3Fr%3Dheader")
comment36 = Takes one string argument and returns the url decoded. The example result is "http://www.splunk.com/download?r=header".
example37 = validate(isint(port), "ERROR: Port is not an integer", port >= 1 AND port <= 65535, "ERROR: Port is out of range")
comment37 = Takes an even number of arguments like case(), with odd arguments being boolean expressions. The even arguments are strings, and the function returns the string corresponding to the first expression that evaluates to false, or null if all checks pass. The example runs a simple check for valid ports.
example38 = commands(searchstr_field)
comment38 = takes a splunk search and returns a multivalued field contain a list of commands used in that search
example39 = relative_time(now(), "-1d@d")
comment39 = Takes a UTC time as the first argument and a relative time specifier as the second argument and returns the UTC time of that relative time applied to the first arugment.
example40 = strftime(_time, "%H:%M")
comment40 = Takes a UTC time as the first argument and renders it as a string using the format specified by the second argument
example41 = strptime(timeStr, "%H:%M")
comment41 = Takes a string time representing and parses it using the format specified by the second argument, returning a UTC time
example42 = time()
comment42 = Returns the current wall-clock time with microsecond resolution.  Will be different for each event based on when that event was processed by eval.
example43 = mvjoin(foo, ";")
comment43 = Join together individual values of a multi-valued field foo using a semicolon as the delimiter
example44 = mvappend(foo, "bar", baz)
comment44 = Append the value "bar" and the values of field baz to the values of field foo and return as multi-valued.  foo and baz could either be multi or single valued field.
example45 = split(foo, ";")
comment45 = Split the value(s) of field foo on the delimiter ';' and returns as multi-valued
example46 = mvfind(mymvfield, "err\d+")
comment46 = Try to find a value in the multivalued field "mymvfield" matching the regex "err\d+". The index of the first matching value is returned (zero indexed). If no values match, null is returned.
example47 = sigfig(number)
comment47 = Display number with the correct number of significant figures.
example48 = spath(input, path)
comment48 = Extract the data at path "path" from "input".  May result in a multivalued field.
example49 = mvzip(hosts, ports)
comment49 = Combine 2 multivalue fields by stitching together the first value of one field with the first value of another field, then the second with the second, and so forth.  Similar to python's zip command
example50 = mvrange(1,11,2)
comment50 = create a multivalued field with the values 1,3,5,7,9.  The first argument is the starting number, second is ending number (exclusive) and the third number, which is optional is the step increment.
example51 = mvdedup(mvfield)
comment51 = Takes a multivalued field as input and returns a multivalued field with its duplicate values removed
example52 = mvsort(mvfield)
comment52 = Takes a multivalued field as input and returns a multivalued field with its values sorted lexicographically

#################
# fieldformat
#################
[fieldformat-command]
syntax = fieldformat <field> = <eval-expression>
description = Expresses how to render a field at output time without changing the underlying value
example1 = fieldformat start_time = strftime(start_time, "%H:%M:%S")
comment1 = Specify that the start_time should be rendered by taking the value of start_time (assuming it is an epoch number) and rendering it to display just the hours minutes and seconds corresponding that epoch time.
usage = public
maintainer = steveyz
tags = field format
category = formatting

##################
# findkeywords
##################

[findkeywords-command]
syntax      = findkeywords labelfield=<field> (dedup=<bool>)?
shortdesc = Given some integer labeling of events into groups, finds searches to generate those groups.
description = Typically run after the "cluster" command or similar.  Takes a set of results with a field \
	    ("labelfield") that supplies a partition of the results into a set of groups. The command \
	    then derives a search to generate each of these groups, which may be saved as an event type\
	    if applicable.
example1 =  ... | findkeywords labelfield=foo
example2 =  ... | findkeywords labelfield=foo dedup=true
category = reporting
appears-in = 6.2
maintainer = aneels
usage = public
related = cluster findtypes
tags = findkeywords cluster patterns findtypes


##################
# kvform
##################

[kvform-command]
syntax = kvform (form=<string>)? (field=<field>)?
shortdesc = Extracts values from search results, using a form template.
description = Extracts key/value pairs from events based on a form\
   template that describes how to extract the values.  If FORM is specified,\
   it uses an installed <FORM>.form file found in the splunk configuration form directory.\
   For example, if "form=sales_order", would look for a "sales_order.form"\
   file in the 'forms' subdirectory in all apps, e.g. $SPLUNK_HOME$/etc/apps/*/forms/.  \
   All the events processed would\
   be matched against that form, trying to extract values.\p\\
   If no FORM is specified, then the FIELD value determines the name of the field to\
   extract.  For example, if "field=error_code", then an event that has an error_code=404,\
   would be matched against a "404.form" file.\p\\
   The default value for FIELD is "sourcetype", thus by default kvform will look for \
   <SOURCETYPE>.form files to extract values.\p\\
   A .form file is essentially a text file or all static parts of a form, \
   interspersed with named references to regular expressions, of the type found in\
   transforms.conf.  A .form might might look like this:\i\\
              Students Name: [[string:student_name]] \i\\
              Age: [[int:age]] Zip: [[int:zip]] .
note = Multiple whitespaces, including blanklines are removed from matching, which could be merged into kv to be called automatically, having no cost if the needed .form file is not found.
comment1 = Extract values from "eventtype.form" if the file exists.
example1 = ... | kvform field=eventtype
appears-in = 4.0
usage = public/experimental
maintainer = david
related = extract, multikv, rex, xmlkv
tags = form extract template
category = fields::add

##################
# extract
##################

[extract-command]
syntax = extract <extract-opt>* <extractor-name>*
alias = kv
shortdesc = Extracts field-value pairs from search results.
description = Forces field-value extraction on the result set.
note = Use pairdelims & kvdelim to select how to extract data.
comment1 = Extract field/value pairs that are defined in the transforms.conf stanza 'access-extractions'.
example1 = ... | extract access-extractions
commentcheat1 = Extract field/value pairs and reload field extraction settings from disk.
examplecheat1 = ... | extract reload=true
commentcheat2 = Extract field/value pairs that are delimited by '|' or ';', and values of fields that are delimited by '=' or ':'.
examplecheat2 = ... | extract pairdelim="|;", kvdelim="=:", auto=f
category = fields::add
maintainer = ledion
appears-in = 3.0
usage = public
related = kvform, multikv, rex, xmlkv
tags = extract kv field extract

[extract-opt]
syntax = (segment=<bool>)|(reload=<bool>)|(limit=<int>)|(maxchars=<int>)|(mv_add=<bool>)|(clean_keys=<bool>)
description = Extraction options. \
	      "segment"    specifies whether to note the locations of key/value pairs with the results (internal, false). \
	      "reload"     specifies whether to force reloading of props.conf and transforms.conf (false). \
	      "kvdelim"    string specifying a list of character delimiters that separate the key from the value \
	      "pairdelim"  string specifying a list of character delimiters that separate the key-value pairs from each other \
	      "maxchars"   specifies how many characters to look into the event (10240). \
	      "mv_add"     whether to create multivalued fields. Overrides MV_ADD from transforms.conf \
	      "clean_keys" whether to clean keys. Overrides CLEAN_KEYS from transforms.conf \
	      "keep_empty_vals" whether to keep KV pairs with empty values. Overrides KEEP_EMPTY_VALS from transforms.conf
example1 = reload=true

[extractor-name]
syntax = <string>
description = A stanza that can be found in transforms.conf
note = This is used when props.conf did not explicitly cause an extraction \
       for this source, sourcetype or host.
example1 = access-extractions

##################
# fsdiscover
# TODO: brian complete SPEC
##################


##################
# fields
##################

[fields-command]
syntax = fields ("+"|"-")? <wc-field-list>
shortdesc = Keeps or removes fields from search results.
description = Keeps or removes fields based on the field list criteria. \
	      If "+" is specified, only the fields that match one of the fields in the list are kept. \
	      If "-" is specified, only the fields that match one of the fields in the list are removed.
note = "_*" is the wildcard pattern for Splunk internal fields.\
              This is similar to an SQL SELECT statement.
comment1 = Keep only the fields 'source', 'sourcetype', 'host', and all fields beginning with 'error'.
example1 = ... | fields source, sourcetype, host, error*
commentcheat1 = Keep only the "host" and "ip" fields, and display them in the order: "host", "ip".
examplecheat1 = ... | fields host, ip
commentcheat2 = Remove the "host" and "ip" fields.
examplecheat2 = ... | fields - host, ip
category = fields::filter
maintainer = david
appears-in = 3.0
usage = public
tags = fields select columns
related = rename

##################
# fieldsummary
##################

[fieldsummary-command]
syntax = fieldsummary (maxvals=<num>)? <wc-field-list>?
shortdesc = Generates summary information for all or a subset of the fields.
description = Generates summary information for all or a subset of the fields.  Emits a maximum of maxvals distinct values for each field (default = 100).
maintainer = steveyz
appears-in = 5.0
comment1 = Return summaries for all fields
example1 = ... | fieldsummary
comment2 = Returns summaries for only fields that start with date_ and return only the top 10 values for each field
example2 = ... | fieldsummary maxvals=10 date_*
category = reporting
usage = public
related = af, anomalies, anomalousvalue, stats

##################
# filldown
##################
[filldown-command]
syntax = filldown <wc-field-list>
shortdesc = Replace null values with last non-null value
description = Replace null values with the last non-null value for a field or set of fields. \
	      If no list of fields is given, filldown will be applied to all fields.  \
	      If there were not any previous values for a field, it will be left blank (null).
comment = Filldown null values for all fields
example = ... | filldown
comment1 = Filldown null values for the count field only
example1 = ... | filldown count
comment2 = Filldown null values for the count field and any field that starts with 'score'
example2 = ... | filldown count score*
maintainer = steveyz
appears-in = 4.2
usage = public
tags = empty default
category = fields::modify
related = fillnull

##################
# fillnull
##################

[fillnull-command]
syntax = fillnull (value=<string>)? <field-list>
shortdesc = Replaces null values with a specified value.
description = Replaces null values with a user specified value (default "0"). \
	      Null values are those missing in a particular result, but \
	      present for some other result.  If a field-list is provided, fillnull \
	      is applied to only fields in the given list (including any fields that \
	      does not exist at all).  Otherwise, applies to all existing fields.
comment = Build a time series chart of web events by host and fill all empty fields with NULL.
example = sourcetype="web" | timechart count by host | fillnull value=NULL
comment1 = For the current search results, fill all empty fields with zero.
example1 = ... | fillnull
comment2 = For the current search results, fill all empty fields with NULL.
example2 = ... | fillnull value=NULL
comment3 = For the current search results, fill all empty field values of "foo" and "bar" with NULL.
example3 = ... | fillnull value=NULL foo bar
maintainer = steveyz
appears-in = 3.0
usage = public
tags = empty default
category = fields::modify
related = eval

##################
# folderize
##################
[folderize-command]
syntax = folderize attr=<string> (sep=<string>)? (size=<string>)? (minfolders=<int>)? (maxfolders=<int>)?
shortdesc = Replaces "attr" with higher-level grouping, such as replacing filenames with directories.
description = Replaces the "attr" attribute value with a more generic value, which is the result of grouping it with other values from other results, where grouping happens via tokenizing the attr value on the sep separator value. For example, it can group search results, such as those used on the Splunk homepage to list hierarchical buckets (e.g. directories or categories). Rather than listing 200 sources on the Splunk homepage, folderize breaks the source strings by a separator (e.g. "/"), and determines if looking at just directories results in the number of results requested.  The default "sep" separator is "::"; the default size attribute is "totalCount"; the default "minfolders" is 2; and the default "maxfolders" is 20.
example1 = | metadata type=sources | folderize maxfolders=20 attr=source sep="/"| sort totalCount d
maintainer = david
usage = deprecated
category = results::group
tags = cluster group collect gather
related = bucketdir

##################
# bucketdir
##################
[bucketdir-command]
syntax = bucketdir pathfield=<field> sizefield=<field> (maxcount=<int>)? (countfield=<field>)? (sep=<char>)?
shortdesc = Replaces PATHFIELD with higher-level grouping, such as replacing filenames with directories.
description = Returns at most MAXCOUNT events by taking the incoming events and rolling up multiple sources into directories, by preferring directories that have many files but few events.  The field with the path is PATHFIELD (e.g., source), and strings are broken up by a SEP character.  The default pathfield=source; sizefield=totalCount; maxcount=20; countfield=totalCount; sep="/" or "\\", depending on the os.
maintainer = david
usage = public
appears-in = 4.0
comment1 = get 10 best sources and directories
example1 = ... | top source|bucketdir pathfield=source sizefield=count maxcount=10
category = results::group
tags = cluster group collect gather
related = cluster dedup

##################
# foreach
##################
[foreach-command]
syntax = foreach (<wc-field>)+ (fieldstr=<string>)? (matchstr=<string>)? (matchseg1=<string>)? (matchseg2=<string>)? (matchseg3=<string>)? <subsearch>
shortdesc = Run a templatized streaming subsearch for each field in a wildcarded field list
description = Run a templated streaming subsearch for each field in a wildcarded field list.  For each field that is matched, the templated subsearch will have the following patterns replaced:  \i\\
     option         default          replacement \i\\
     fieldstr       <<FIELD>>        whole field name \i\\
     matchstr       <<MATCHSTR>>     part of field name that matches wildcard(s) in the specifier \i\\
     matchseg1      <<MATCHSEG1>>    part of field name that matches first wildcard \i\\
     matchseg2      <<MATCHSEG2>>    part of field name that matches second wildcard \i\\
     matchseg3      <<MATCHSEG3>>    part of field name that matches third wildcard
example1= ... | eval total=0 | eval test1=1 | eval test2=2 | eval test3=3 | foreach test* [eval total=total + <<FIELD>>]
comment1= add together all fields with a name that starts with "test" into a total field (result should be total=6)
example2= ... | foreach foo* [eval new_<<MATCHSTR>> = <<FIELD>> + bar<<MATCHSTR>>]
comment2= for each field that matches foo*, add it to the corresponding bar* field and write to a new_* field (e.g. new_X = fooX + barX)
example3= ... | foreach foo bar baz [eval <<FIELD>> = "<<FIELD>>"]
comment3= equivalent to:  eval foo="foo" | eval bar="bar" | eval baz="baz"
example4= ... | foreach foo*bar* fieldstr="#field#" matchseg2="#matchseg2#" [eval #field# = "#matchseg2#"]
comment4= for the field fooAbarX, this would be to equivalent to: eval fooAbarX = "X"
usage=public
maintainer = steveyz
appears-in=6.0
related=eval
tags=subsearch eval computation wildcard fields
category=search::subsearch

##################
# format
##################
[format-command]
syntax = format (<string> <string> <string> <string> <string> <string>)?
shortdesc = Takes the results of a subsearch and formats them into a single result.
description = Used implicitly by subsearches, to take the search results of a subsearch and return a single result that is a query built from the input search results. \
          The six optional arguments correspond to row prefix, column prefix, column separator, column end, row separator, and row end.  They default to: "(" "(" "AND" ")" "OR" ")".
comment1 = Get top 2 results and create a search from their host, source and sourcetype, resulting in a single search result with a _query field: _query=( ( "host::mylaptop" AND "source::syslog.log" AND "sourcetype::syslog" ) OR ( "host::bobslaptop" AND "source::bob-syslog.log" AND "sourcetype::syslog" ) )
example1 = ... | head 2 | fields source, sourcetype, host | format
usage = public
maintainer = david
appears-in=3.0
tags = format query subsearch
category = search::subsearch
related = search

##################
# gauge
##################
[gauge-command]
syntax = gauge (<num>|<field>) ((<num>|<field>)+)?
description = Transforms results into a format suitable for display by the Gauge chart types.  Each argument must be a real number or the name of a numeric field.  Numeric field values will be taken from the first input result, the remainder are ignored.  The first argument is the gauge value and is required.  Each argument after that is optional and defines a range for different sections of the gauge.  If there are no range values provided, the gauge will start at 0 and end at 100.  If two or more range values are provided, The guage will begin at the first range value, and end with the final range value.  Intermediate range values will be used to split the total range into subranges which will be visually distinct.  A single range value is meaningless and will be treated identically as no range values.
shortdesc = Transforms results into a format suitable for display by the Gauge chart types.
example1 = ... | gauge count 0 25 50 75 100
comment1 = Use the value of the count field as the gauge value and have 4 regions to the gauge (0-25,25-50,50-75,75-100)
maintainer = steveyz
appears-in = 4.2
usage = public
tags = stats format display chart dial
category = reporting
related = eval stats

##################
# gentimes
##################

[gentimes-command]
syntax = gentimes start=<timestamp> (end=<timestamp>)? (increment=<increment>)?
shortdesc = Generates time range results.
description = Generates time range results. This command is useful in conjunction with the 'map' command.
comment1 = All daily time ranges from oct 25 till today
example1 = | gentimes start=10/25/07
comment2 = All daily time ranges from 30 days ago until 27 days ago
example2 = | gentimes start=-30 end=-27
comment3 = All daily time ranges from oct 1 till oct 5
example3 = | gentimes start=10/1/07 end=10/5/07
comment4 = All HOURLY time ranges from oct 1 till oct 5
example4 = | gentimes start=10/1/07 end=10/5/07 increment=1h
maintainer = david
appears-in = 3.2
usage = public beta
tags = time timestamp subsearch range timerange
generating = true
category = results::generate
related = map

[timestamp]
# the current bnf format would be a royal pain to represent this.  perhaps we should support regex
# perhaps some token to indicate no-ws is implied. -- <allws> ....
# syntax = "(\d{1,2})/(\d{1,2})(?:/(\d{2,4}))?(?::(\d{1,2}):(\d{2}):(\d{2}))?"|<int>
syntax = ( MM/DD/YYYY|MM/DD/YYYY:HH:MM:SS) |<int>
example1 = 10/1/2007:12:34:56
comment2 = 5 days ago
example2 = -5

[increment]
syntax = <int:increment>(s|m|h|d)?
comment1 = 1 hour
example1 = 1h

##################
# geostats
##################
[geostats-command]
syntax = geostats (translatetoxy=<bool>)? (latfield=<string>)? (longfield=<string>)? (outputlatfield=<string>)? (outputlongfield=<string>)? (globallimit=<int>)? (locallimit=<int>)? (binspanlat=<float> binspanlong=<float>)? (maxzoomlevel=<int>)? (<stats-agg-term>)* (<by-clause>)?
shortdesc = Generate statistics which are clustered into geographical bins to be rendered on a world map.
description = Use the geostats command to compute statistical functions suitable for rendering on \
              a world map. First, the events will be clustered based on latitude and longitude \
              fields in the events.  Then, the statistics will be evaluated on the generated \
              clusters, optionally grouped or split by fields using a by-clause.\p\\
              For map rendering and zooming efficiency, geostats generates clustered stats at a \
              variety of zoom levels in one search, the visualization selecting among them. The \
              quantity of zoom levels can be controlled by the options \
              binspanlat/binspanlong/maxzoomlevel. The initial granularity is selected by \
              binspanlat together with binspanlong.  At each level of zoom, the number of bins \
              will be doubled in both dimensions (a total of 4x as many bins for each zoom-in).
example1= ... | geostats latfield=eventlat longfield=eventlong avg(rating) by gender
comment1= compute the average rating for each gender after clustering/grouping the events by "eventlat" and "eventlong" values.
example2= ... | geostats count
comment2=  cluster events by default latitude and longitude fields "lat" and "lon" respectively. Calculate the count of such events
example3= sourcetype = access_combined_wcookie | iplocation clientip | geostats count by date_hour
comment3 = take events from apache logs, use iplocation to geocode the ip addresses of the client, and then cluster the events based on how many are happening in each hour of the day.
usage=public
maintainer = arahut
appears-in=6.0
related=stats, xyseries, chart
tags = stats statistics
category = reporting

[binspanlat]
syntax = binspanlat=<float>
description = The size of the bins in latitude degrees at the lowest zoom level. Defaults to 22.5. \
              With default binspanlong=45.0, leads to a grid size of 8x8.

[binspanlong]
syntax = binspanlong=<float>
description = The size of the bins in longitude degrees at the lowest zoom level. Defaults to \
              45.0. With default binspanlat=22.5, leads to a grid size of 8x8.

[globallimit]
syntax = globallimit=<int>
description = Controls the number of named categories to add to each pie-chart. When used with \
              count and additive statistics, there will be one additional category called "OTHER" \
              which groups all other split-by values. Setting globallimit=0 removes all limits and \
              renders all categories.  Defaults to 10.

[latfield]
syntax = latfield=<field>
description = Specify a field from the pre-search that represents the latitude coordinates to use \
              in your analysis. Defaults to "lat".

[longfield]
syntax = longfield=<field>
description = Specify a field from the pre-search that represents the longitude coordinates to use \
              in your analysis. Defaults to "lon".

[maxzoomlevel]
syntax = maxzoomlevel=<int>
description = The maximum level to be created in the quad tree. Defaults to 9, which specifies \
              that 10 zoom levels will be created: 0-9.

[outlatfield]
syntax = outlatfield=<string>
description = Specify a name for the latitude field in your geostats output data. \
              Defaults to "latitude".

[outlongfield]
syntax = outlongfield=<string>
description = Specify a name for the longitude field in your geostats output data. \
              Defaults to "longitude".

[translatetoxy]
syntax = translatetoxy=<bool>
description = If true, geostats produces one result per each binned location for rendering on a \
              map.  If false, geostats produces one result per category per binned location and \
              cannot be rendered on a map. Defaults to true.

##################
# geom
##################
[geom-command]
syntax = geom (<featureCollection>)? (featureIdField=<string>)?
shortdesc = Geom command is for choropleth map's UI visualization.
description = Geom command can generate polygon geometry in JSON style, for UI visualization; This command depends on lookup having been installed with external_type=geo.
example1 = ...| geom
comment1 = When no arguments are provided, geom command looks for a column named "featureCollection" and a column named "featureId" in the event. These commands are present in the default output from a Lookup on the given geoindex.
example2 = ...| geom "geo_us_states"
comment2 = This case specifies spatial index name to "geo_us_states".
example3 = ...| geom "geo_us_states" featureIdField="state"
comment3 = This case specifies featureId to "state" field in event.
usage = public
maintainer = ghendrey
appears-in = 6.3
related = geomfilter, lookup
tags = choropleth map
category = reporting

[featureCollection]
syntax = <string>
description = This option is used to specify the spatial index; the provided string is the index name.

[featureIdField]
syntax = featureIdField=<string>
description = This option is used to specify the field name, when event contains featureId in a field named something other than "featureId".

##################
# geomfilter
##################
[geomfilter-command]
syntax = geomfilter (min_x=<float>)? (min_y=<float>)? (max_x=<float>)? (max_y=<float>)?
shortdesc = Geomfilter command is for choropleth map's clipping feature.
description = Geomfilter command accepts 2 points that specify a bounding box for clipping choropleth map; points fell out of the bounding box will be filtered out.
default = "min_x=-180 min_y=-90 max_x=180 max_y=90"
example1 = ...| geomfilter
comment1 = This case uses the default bounding box, which will clip the whole map.
example2 = ...| geomfilter min_x=-90 min_y=-90 max_x=90 max_y=90
comment2 = This case clips half of the whole map.
usage = public
maintainer = ghendrey
appears-in = 6.3
related = geom
tags = choropleth map
category = reporting

[min_x]
syntax = min_x=<float>
description = X coordinate of bounding box's bottom-left corner, range [-180, 180].
default = "min_x=-180"

[min_y]
syntax = min_y=<float>
description = Y coordinate of bounding box's bottom-left corner, range [-90, 90].
default = "min_y=-90"

[max_x]
syntax = max_x=<float>
description = X coordinate of bounding box's up-right corner, range [-180, 180].
default = "max_x=180"

[max_y]
syntax = max_y=<float>
description = Y coordinate of bounding box's up-right corner, range [-90, 90].
default = "max_y=90"

##################
# head
##################

[head-command]
syntax = head ((<int:n>)|("("<eval-expression>")"))? (limit=<int>)? (null=<bool>)? (keeplast=<bool>)?
shortdesc = Returns the first n number of specified results.
description = Returns the first n results, or 10 if no integer is specified.\
	      New for 4.0, can provide a boolean eval expression, in which case we return events until that expression evaluates to false.
commentcheat = Return the first 20 results.
examplecheat = ... | head 20
example1 = ... | streamstats range(_time) as timerange | head (timerange<100)
comment1 = Return events until the time span of the data is >= 100 seconds
category = results::order
maintainer = steveyz
appears-in = 3.2
usage = public beta
related = reverse, tail
tags = head first top leading latest

##################
# tail
##################

[tail-command]
syntax = tail (<int:n>)?
shortdesc = Returns the last n number of specified results.
description = Returns the last n results, or 10 if no integer is specified.  The events\
	      are returned in reverse order, starting at the end of the result set.
commentcheat = Return the last 20 results (in reverse order).
examplecheat = ... | tail 20
category = results::order
maintainer = steveyz
appears-in = 3.2
usage = public beta
related = head, reverse
tags = tail last bottom trailing earliest

##################
# reverse
##################

[reverse-command]
syntax = reverse
description = Reverses the order of the results.
commentcheat = Reverse the order of a result set.
examplecheat = ... | reverse
category = results::order
maintainer = steveyz
appears-in = 3.2
usage = public
related = head, sort, tail
tags = reverse flip invert inverse upsidedown

##################
# history
##################

[history-command]
syntax = history (events=<bool>)?
shortdesc = Returns a history of searches, either as events or as non-event results (default)
description = Returns information about searches that the current user has run.  \
              By default, the search strings are presented as a field called "search". \
              If events=true, then the search strings are presented as the text of the \
              events, as the _raw field.
comment = Returns a history of searches as a table
example = | history
maintainer = steveyz
appears-in = 4.2
usage = public
tags = history search
category = results::read
related = search
generating = true

#################
# dbinspect
#################
[dbinspect-command]
syntax      = dbinspect (<index-specifier>)* (<bucket-span>|<timeformat>)?
shortdesc   = Returns information about the Splunk index.
description = Produces results describing the buckets that belong to a Splunk index or indexes. \
              When invoked without span, results describe buckets using the following fields: \
              bucketId, endEpoch, eventCount, hostCount, id, modTime, path, \
              rawSize, sizeOnDiskMB, sourceCount, sourceTypeCount, splunk_server, startEpoch, state.  \p\\
              In this format, the timeformat flag is used to control the representation \
              of the modTime field. \p\\
              When invoked with span, produces results describing a chartable representation of \
              the spans of each bucket.
comment1    = Display a chart with the span size of 1 day.
example1    = | dbinspect index=_internal span=1d
maintainer  = igor
appears-in  = 4.0
usage       = public beta
tags        = inspect index bucket
generating  = true
category = administrative
related  = metadata

##################
# iconify
##################
[iconify-command]
syntax = iconify <field-list>
description = Causes the UI to make a unique icon for each value of the fields listed.
comment1 = Displays an different icon for each eventtype.
example1 = ... | iconify eventtype
comment2 = Displays an different icon for each process id.
example2 = ... | iconify pid
comment3 = Displays an different icon for url and ip combination.
example3 = ... | iconify url ip
category = formatting
appears-in = 4.0
maintainer = david
usage = public
related = highlight, abstract
tags = ui search icon image


##################
# input
##################
[input-command]
syntax      = input (add|remove) (sourcetype=<string>)? (index=<string>)? (<string>=<string>)*
shortdesc = Adds or disables sources from being processed by Splunk.
description = Adds or removes (disables) sources from being processed by splunk, enabling or disabling inputs in inputs.conf, with optional sourcetype and index settings. Any additional attribute=values are set added to inputs.conf.  Changes are logs to $SPLUNK_HOME/var/log/splunk/inputs.log.  Generally to be used in conjunction with the "crawl" command.
comment1 = Add each source found by crawl in the default index with automatic source classification (sourcetyping)
example1 = | crawl | input add
comment2 = Remove all csv files that are currently being processed
example2 = | crawl | search source=*csv | input remove
comment3 = Add all sources found in bob's home directory to the 'preview' index with sourcetype=text, setting custom user fields 'owner' and 'name'
example3 = | crawl root=/home/bob/txt | input add index=preview sourcetype=text owner=bob name="my nightly crawl"
maintainer  = david
appears-in  = 3.2-Britney
usage       = public
tags = input index crawl
category = index::add
related = crawl

[input-option]
syntax = <string>=<string>
description = Override settings from input.conf.
example1 = root=/home/bob


##################
# inputcsv
##################

[inputcsv-command]
syntax = inputcsv (dispatch=<bool>)? (append=<bool>)? (start=<int>)? (max=<int>)? (events=<bool>)? <filename> (WHERE <string:search-query>)?
shortdesc = Loads search results from the specified CSV file.
description = Populates the results data structure using the given csv file, which is not modified. The filename must refer to a relative path in $SPLUNK_HOME/var/run/splunk (if dispatch option is set to true, filename refers to a file in the job directory in $SPLUNK_HOME/var/run/splunk/dispatch/<job id>/) and if the specified file does not exist and the filename did not have an extension, then filename with a ".csv" extension is assumed. \i\\
      The optional argument 'start' controls the 0-based offset of the first event to be read (default=0). The optional argument 'max' controls the maximum number of events to be read from the file (default = 1000000000). 'events' is an option that allows the imported results to be treated as events, i.e., so that a proper timeline and fields picker are displayed. If 'append' is set to true (false by default), the data from the csv file is appended to the current set of results rathering than replacing it.
note = 'keeptempdir' is a debugging option, that if true, retains the temporary directory that the given file is copied into for manipulation by the search pipeline.  This option should not be mentioned in the external documentation or typeahead.
usage = public
maintainer = steveyz
appears-in = 3.2
comment1 = Read in events from the CSV file: "$SPLUNK_HOME/var/run/splunk/foo.csv".
example1 = | inputcsv foo.csv
example2 = | inputcsv start=100 max=500 bar
example3 = | inputcsv foo.csv where foo>2 OR bar=5
comment2 = Read in events 101 to 600 from either file 'bar' (if exists) or 'bar.csv'.
comment3 = Same as example1 except that the events are filtered to where foo is greater than 2 or bar equals 5
commentcheat = Read in results from the CSV file: "$SPLUNK_HOME/var/run/splunk/all.csv", keep any that contain the string "error", and save the results to the file: "$SPLUNK_HOME/var/run/splunk/error.csv"
examplecheat = | inputcsv all.csv | search error | outputcsv errors.csv
category = results::read
related = outputcsv
tags = input csv load read
generating = true

##################
# inputlookup
##################

[inputlookup-command]
syntax = inputlookup (required=<bool>)? (append=<bool>)? (start=<int>)? (max=<int>)? (<filename>|<string:tablename>) (WHERE <string:search-query>)?
shortdesc = Loads search results from a specified static lookup table.
description = Reads in lookup table as specified by a filename (must end with .csv or .csv.gz) or a table name (as specified by a stanza name in transforms.conf).\
   If 'append' is set to true (false by default), the data from the lookup file is appended to the current set of results rathering than replacing it. \
   If 'required' is set to false (true by default), the search will proceed normally if the lookup table is non-existant, generating only a warning message.
usage = public
maintainer = steveyz
appears-in = 4.0
example1 = | inputlookup users.csv
example2 = | inputlookup usertogroup
example3 = | inputlookup append=t usertogroup
example4 = | inputlookup usertogroup where foo>2 OR bar=5
comment1 = Read in "users.csv" lookup file (under $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/*/lookups).
comment2 = Read in "usertogroup" lookup table (as specified in transforms.conf).
comment3 = Same as example2 except that the data from the lookup table is appended to any current results.
comment4 = Same as example2 except that the data from the lookup table is filtered to where foo is greater than 2 or bar equals 5 before returned.
generating = true
related = inputcsv, join, lookup, outputlookup
tags = lookup input table
category = results::read
optout-in = lite, lite_free

##################
# internalinputcsv
##################

[internalinputcsv-command]
syntax = internalinputcsv <filename>
description = Reads in events from <filename> but does not do as much error checking and will not collapse into \
	    multiple files if the filename is too large. Internal debugging operator.
usage = internal
appears-in = 3.2
example1 = | internalinputcsv foo.csv
generating = true
maintainer = steveyz
optout-in = lite, lite_free

##################
# join
##################
[join-command]
syntax = join (<join-options>)* <field-list> [<search-pipeline>]
shortdesc = Use the join command to combine the results of a subsearch with the results of a main search.
description = You can perform an inner or left join.  Use either 'outer' or 'left' to specify a left outer join. One or more of the fields must be common to each result set. If no fields are specified, all of the fields that are common to both result sets are used. Limitations on the join subsearch are specified in the limits.conf.spec file. Note: Another command, such as append or lookup, in combination with either stats or transaction might be a better alternative to the join command for flexibility and performance.
maintainer = steveyz
appears-in = 3.3
usage = public
example1 = ... | join product_id [search vendors]
comment1 = Joins previous result set with results from 'search vendors', on the product_id field.
related = selfjoin, append, set, appendcols
tags = join combine unite append csv lookup inner outer left
category = results::append

[join-options]
syntax = type=(inner|outer|left) | usetime=<bool> | earlier=<bool> | overwrite=<bool> | max=<int>
description = Options to the join command.  In both inner and left joins, events that match are joined. The results of an inner join do not include events from the main search that have no matches in the subsearch. The results of a left (or outer) join include all of the events from the main search, and any of the events from the subsearch that have matching values in the main search. The usetime option specifies whether to limit matches to subresults that are earlier or later than the main result to join with. The earlier option is only valid when usetime=true. The default for usetime is false. The overwrite option indicates if fields from the subresults should overwrite those from the main result if they have the same field name. The default for overwrite is true.  The max option specifies the maximum number of subresults each main result can join with. The default for max is 1. Specify 0 to indicate there is no limit.
default = type=inner | usetime=false | earlier=true | overwrite=true | max=1
example1 = type=outer
example2 = usetime=t
example3 = usetime=t earlier=f
example4 = overwrite=f
example5 = max=3

##################
# selfjoin
##################
[selfjoin-command]
syntax = selfjoin (<selfjoin-options>)* <field-list>
shortdesc = Joins results with itself.
description = Join results with itself, based on a specified field or list of fields to join on.
maintainer = steveyz
appears-in = 3.3
usage = public
example1 = ... | selfjoin id
comment1 = Join results with itself on 'id' field.
related = join
tags = join combine unite
category = results::filter

[selfjoin-options]
syntax = overwrite=<bool> | max=<int> | keepsingle=<int>
description = The selfjoin joins each result with other results that have the same value for the join fields.  'overwrite' controls if fields from these 'other' results should overwrite fields of the result used as the basis for the join (default=true).  max indicates the maximum number of 'other' results each main result can join with.  (default = 1, 0 means no limit).  'keepsingle' controls whether or not results with a unique value for the join fields (and thus no other results to join with) should be retained.  (default = false)
default = overwrite=true | max=1 | keepsingle=false
example1 = overwrite=f
example2 = max=3
example3 = keepsingle=t


##################
# kmeans
##################

[kmeans-command]
syntax = kmeans (<kmeans-options> )* <field-list>
shortdesc = Performs k-means clustering on selected fields.
description = Performs k-means clustering on select fields (or all numerical fields if empty).  Events in the same cluster will be \
              moved next to each other.  Optionally the cluster number for each event is displayed. The centroid of each cluster will also \
              be displayed (with an option to disable it).
maintainer = steveyz
appears-in = 3.0
usage = public
comment1 = Group results into 2 clusters based on the values of all numerical fields.
example1 = ... | kmeans
commentcheat = Group search results into 4 clusters based on the values of the "date_hour" and "date_minute" fields.
examplecheat = ... | kmeans k=4 date_hour date_minute
category = results::group
related = anomalies, anomalousvalue, cluster, outlier
tags = cluster group collect gather

[kmeans-options]
syntax = <kmeans-reps>|<kmeans-iters>|<kmeans-t>|<kmeans-k>|<kmeans-cnumfield>|<kmeans-distype>|<kmeans-centroids>
description = Options for kmeans command

[kmeans-reps]
syntax = reps=<int>
description = Number of times to repeat kmeans using random starting clusters
default = "reps=10"

[kmeans-iters]
syntax = maxiters=<int>
description = Maximum number of iterations allowed before failing to converge
default = "maxiters=10000"

[kmeans-t]
syntax = t=<num>
description = Algorithm convergence tolerance
default = "t=0"

[kmeans-k]
syntax = k=<int>(-<int>)?
description = Number of initial clusters to use.  If specified as a range, clustering will be performed for each \
	      count of clusters in the range, and a summary of the result of each run will be provided expressing \
	      the size of the clusters, and the 'distortion', which describes a kind of non-fittedness of the data. \
	      Distortion is the sum of the squared distances bewteen each item and its cluster center.
default = "k=2"

[kmeans-cnumfield]
syntax = cfield=<field>
description = Controls the field name for the cluster number for each event
default = "cfield=CLUSTERNUM"

[kmeans-distype]
syntax = dt=(l1|l1norm|cityblock|cb|l2|l2norm|sq|sqeuclidean|cos|cosine)
simplesyntax = dt=(l1norm|l2norm|cityblock|sqeuclidean|cosine)
description = Distance metric to use (L1/L1NORM equivalent to CITYBLOCK).  L2NORM equivalent to SQEUCLIDEAN
default = "dt=L2NORM"

[kmeans-centroids]
syntax = showcentroid=(true|false)
description = Expose the centroid centers in the search results if showcentroid is true; don't if false.
default = "showcentroid=true"

##################
# localize
##################
[localize-command]
syntax      = localize <lmaxpause-opt>? <after-opt>? <before-opt>?
shortdesc = Returns a list of time ranges in which the search results were found.
description = Generates a list of time contiguous event regions \
              defined as: a period of time in which consecutive events \
              are separated by at most 'maxpause' time. The found regions \
              can be expanded using the 'timeafter' and 'timebefore' modifiers \
              to expand the range after/before the last/first event in \
              the region respectively. The Regions are return in time descending\
              order, just as search results (time of region is start time).\
              The regions discovered by localize are meant to be feed into \
              the MAP command, which will use a different region for each iteration. \
              Localize also reports: (a) number of events in the range, (b) range \
              duration in seconds and (c) region density defined as (#of events in range) \
              divided by (range duration) - events per second.
comment1 = As an example, searching for "error" and then calling localize finds good regions around \
           where error occurs, and passes each on to the search inside of the the map command, so \
           that each iteration works with a specific timerange to find promising transactions
example1   = error | localize | map search="search starttimeu::$starttime$ endtimeu::$endtime$ |transaction uid,qid maxspan=1h"
commentcheat = Search the time range of each previous result for "failure".
examplecheat = ... | localize maxpause=5m | map search="search failure starttimeu=$starttime$ endtimeu=$endtime$"
category = search::subsearch
maintainer = david, ledion
usage      = public beta
appears-in = 3.2
related = map, transaction
tags = time timestamp subsearch range timerange

[lmaxpause-opt]
syntax      = maxpause=<int>(s|m|h|d)?
description = the maximum (inclusive) time between two consecutive events in a contiguous time region
default     = "maxpause=1m"

[after-opt]
syntax      = timeafter=<int>(s|m|h|d)?
description = the amount of time to add to endtime (ie expand the time region forward in time)
default     = "timeafter=30s"

[before-opt]
syntax      = timebefore=<int>(s|m|h|d)?
description = the amount of time to subtract from starttime (ie expand the time region backwards in time)
default     = "timebefore=30s"


##################
# localop
##################
[localop-command]
syntax = localop
shortdesc = Prevents subsequent commands from being executed on remote peers.
description = Prevents subsequent commands from being executed on remote peers, i.e. forces subsequent commands to be part of the reduce step.
example1 = FOO BAR | localop | iplocation
comment1 = The iplocation command in this case will never be run on remote peers.  All events from remote peers from the initial search for the terms FOO and BAR will be forwarded to the search head where the iplocation command will be run.
maintainer = steveyz
appears-in = 4.0
tags       = debug distributed
usage      = public unsupported/beta
category   = search::search
optout-in = lite, lite_free


##################
# loadjob
##################
[loadjob-command]
syntax      = loadjob (<sid-opt>|<savedsearch-opt>) <result-event-opt>? <delegate-opt>? <artifact-offset-opt>? <ignore-running-opt>?
shortdesc   = Loads events or results of a previously completed search job.
description = The artifacts to load are identified either by the search job id or a scheduled search name and the time range of the current search. If a savedsearch name is provided and multiple artifacts are found within that range the latest artifacts are loaded.
example1    = | loadjob 1233886270.2 events=t
comment1    = Loads the events that were generated by the search job with id=1233886270.2
example2    = | loadjob savedsearch="admin:search:MySavedSearch"
comment2    = Loads the results of the latest scheduled execution of savedsearch MySavedSearch in the 'search' application owned by admin
maintainer = ledion
appears-in = Madonna
related    = inputcsv, file
usage      = public
tags       = artifacts
generating = true
category = results::generate

[sid-opt]
syntax      = <string>
description = The search id of the job whose artifacts need to be loaded.
example     = 1233886270.2

[savedsearch-identifier]
syntax      = savedsearch="<user-string>:<application-string>:<search-name-string>"
description = The unique identifier of a savedsearch whose artifacts need to be loaded. A savedsearch \
              is uniquely identified by the triplet {user, application, savedsearch name}.
example     = savedsearch="admin:search:my saved search"

[result-event-opt]
syntax      = events=<bool>
description = events=true loads events, while events=false loads results.  Defaults to false.
example     = events=true

[delegate-opt]
syntax      = job_delegate=<string>
description = When specifying a savedsearch, this option selectes jobs that were started by the given user. \
              Scheduled jobs will be run by the delegate "scheduler".  Dashboard-embedded searches will be \
              run in accordance with the savedsearche's dispatchAs parameter (typically the owner of the search). \
              Defaults to scheduler.
example     = job_delegate=scheduler

[artifact-offset-opt]
syntax      = artifact_offset=<int>
description = Select a search artifact other than the most recent one, based on search start time.  For example \
              if artifact_offset=1, the second most recent will be loaded; if  artifact_offset=2, the third most recent \
              will be loaded.  Attempting to load an offset past the last available artifact will result in an error.\
              Defaults to 0, or the most recent.
example     = artifact_offset=1

[ignore_running]
syntax      = ignore_running=<bool>
description = Skip over artifacts whose search is still running (default: true)
example     = ignore_running=false

##################
# lookup
##################
[lookup-command]
syntax = lookup (local=<bool>)? (update=<bool>)? <string:lookup-table-name> (<field:lookup> (as <field:local>)? )+ (OUTPUT|OUTPUTNEW (<field:dest> (as <field:local-dest>)? )+ )?
shortdesc = Explicitly invokes field value lookups.
description = Manually invokes field value lookups from an existing lookup table or external \
              script. Lookup tables must be located in the lookups directory of \
              $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/<app-name>/lookups. \
              External scripts must be located in $SPLUNK_HOME/etc/searchscripts or \
              $SPLUNK_HOME/etc/apps/<app_name>/bin.\p\\
              Specify a lookup field to match to a field in the events and, optionally, \
              destination fields to add to the events. If you do not specify destination fields, \
              adds all fields in the lookup table to events that have the match field. You can \
              also overwrite fields in the events with fields in the lookup table, if they have \
              the same field name.
example1 = ... | lookup usertogroup user as local_user OUTPUT group as user_group
comment1 = There is a lookup table specified in a stanza name 'usertogroup' in transform.conf.  This lookup table contains (at least) two fields, 'user' and 'group'. For each event, we look up the value of the field 'local_user' in the table and for any entries that matches, the value of the 'group' field in the lookup table will be written to the field 'user_group' in the event.
usage = public
appears-in = 4.0
maintainer = steveyz
related = appendcols inputlookup outputlookup
tags = join combine append lookup table
category = fields::read


##################
# makecontinuous
##################

[makecontinuous-command]
syntax = makecontinuous (<field>)? (<bucketing-option> )*
description = Makes a field that is supposed to be the x-axis continuous (invoked by chart/timechart).
usage = public beta
comment1 = Make "_time" continuous with a span of 10 minutes.
example1 = ... | makecontinuous _time span=10m
appears-in = 3.2
maintainer = steveyz
category = reporting
tags = continuous
related = chart timechart

##################
# makeresults
##################

[makeresults-command]
syntax = makeresults (<count-option>)? (<annotate-option>)? (<splunk-server-option>)? (<splunk-server-group-option>)*
shortdesc = Create a specified number of empty results.
description = Creates a specified number of empty search results. This command will run only on the local machine \
by default and will generate one unannotated empty result. It maybe used in conjunctiion with the eval command to \
generate an empty result for the eval command to operate on.
note = If the search begins with an eval command it will return no results. makeresults is implicitly injected to the \
beginning of such searches.
example1 = makeresults | eval foo="foo"
example2 = index=_internal _indextime > [makeresults | eval it=now()-60 | return $it]
appears-in = 6.3
usage = public
category = results::generate
maintainer = amathew

[count-option]
syntax      = count=<num>
description = The number of empty results to generate

[annotate-option]
syntax      = annotate=<bool>
description = If set to true the results will conatin field for the splunk_server aplunk_server_group and _time when they were created \
These may be used to compute aggregats etc. Certain order sensitive processors may also fail if the internal _time field is absent. \
False by default.

[splunk-server-opt]
syntax      = splunk_server=<string>
description = Optional, argument specifies whether or not to limit results to one specific server. Use "local" to refer to the search head

[splunk-server-group-opt]
syntax      = splunk_server_group=<string>
description = Optional, argument specifies whether or not to limit results to one speciic server_group.

##################
# map
##################

[map-command]
syntax = map (<searchoption>|<savedsplunkoption>) <maxsearchesoption>?
shortdesc = Looping operator, performs a search over each search result.
description = For each input search result, takes the field-values\
 from that result and substitutes their value for the $variable$ in the\
 search argument.  The value of variables surrounded in quotes (e.g. text="$_raw$") will be quote escaped. \
 The search argument can either be a subsearch to run\
 or just the name of a savedsearch. The following metavariables are \
 also supported: \
 1. $_serial_id$ - 1-based serial number within map of the search being executed.
usage = public beta
maintainer = david
appears-in = 3.2
example1 = error | localize | map mytimebased_savedsearch
tags = map subsearch loop savedsearch
category = results::generate
related = gentimes, search

[searchoption]
syntax = "[" <subsearch> "]"  | search=\"<string>\"
description = Search to run map on.
example1 = [search starttimeu::$start$ endtimeu::$end$ source="$source$"]
default = none

[savedsplunkoption]
syntax = <string>
description = Name of saved search
example1 = mysavedsearch
default = none

[maxsearchesoption]
syntax      = maxsearches=<int>
description = The maximum number of searches to run. Will generate warning if \
              there are more search results.
example1    = maxsearches=42
default     = maxsearches=10

##################
# multikv
##################
[multikv-command]
syntax = multikv (<multikv-option> )*
shortdesc = Extracts field-values from table-formatted events.
description = Extracts fields from events with information in a tabular format (e.g. top, netstat, ps, ... etc). \
              A new event will be created for each table row. Field names will be derived from the title row of the table.
appears-in = 3.0
usage = public
maintainer = ledion, steveyz
comment1 = Extract the "pid" and "command" fields.
example1 = ... | multikv fields pid command
commentcheat = Extract the "COMMAND" field when it occurs in rows that contain "splunkd".
examplecheat = ... | multikv fields COMMAND filter splunkd
category = fields::add
related = extract, kvform, rex, xmlkv
tags = extract table tabular column

[multikv-option]
syntax      = <multikv-copyattrs>|<multikv-fields>|<multikv-filter>|<multikv-forceheader>|<multikv-multitable>|<multikv-noheader>|<multikv-rmorig>
description = Multikv available options

[multikv-copyattrs]
syntax      = copyattrs=<bool>
description = When true, multikv copies all fields from the original event to the events generated from that event.  \
              When false, no fields are copied from the original event.  \
              This means there will be no _time field and you will not be able to see the events in the UI. (default = true)

[multikv-fields]
syntax      = fields <field-list>
description = Limit the fields set by multikv to this list.  \
              Any fields in the table which are not on this list will be ignored.

[multikv-filter]
syntax      = filter <field-list>
description = If specified, multikv will skip  over table rows that do not contain at least one of the strings in the filter list. \
              Quoted  expressions are permitted such as "multiple words" or "trailing_space ".

[multikv-forceheader]
syntax      = forceheader=<int>
description = Forces the use of the given line number (1 based) as the table's header. \
              Empty lines are not included in the count.  \
              By default, multikv attempts to determine the header line automatically.

[multikv-multitable]
syntax      = multitable=<bool>
description = Controls whether or not there can be multiple tables in a single _raw in the original events. (default = true)

[multikv-noheader]
syntax      = noheader=<bool>
description = Handle a table without header row identification.  \
              The size of the table will be inferred from the first row, and fields will be anmed Column_1, Column_2, etc.  \
              noheader=true implies multitable=false (default = false)

[multikv-rmorig]
syntax      = rmorig=<bool>
description = When true, the original events will not be included in the output results.  \
              When false, the original events are retained in the output results, with each original event \
              emitted after the batch of generated results from that original. (default=true)

###########################
# multisearch
###########################

[multisearch-command]
syntax = multisearch <subsearch1> <subsearch2> <subsearch3> ...
shortdesc = Do multiple searches at the same time
description = Executes multiple *streaming* searches at the same time.  Must specify at least 2 subsearches and only purely streaming operations are allowed in each subsearch (e.g. search, eval, where, fields, rex, ...)
example = | multisearch [search index=a | eval type = "foo"] [search index=b | eval mytype = "bar"]
comment = search for both events from index a and b and add different fields using eval in each case
maintainer = steveyz
appears-in = Ace
usage = public
tags = append join combine unite combine
category = results::append
related = append, join


##################
# mvcombine
##################

[mvcombine-command]
syntax = mvcombine (delim=<string>)? <field>
shortdesc = Combines events in the search results that have a single differing field value into one result with a multi-value field of the differing field.
description = For each group of results that are identical except for the given field, combine them into a single result where the given field is a multivalue field.  DELIM controls how values are combined, defaulting to a space character (' ').
maintainer = steveyz
usage = public
appears-in = 3.3
comment = Combine the values of "foo" with ":" delimiter.
example = ... | mvcombine delim=":" foo
related = makemv, mvexpand, nomv
tags = combine merge join unite multivalue
category = results::filter

##################
# mvexpand
##################

[mvexpand-command]
syntax = mvexpand <field> (limit=<int>)?
shortdesc = Expands the values of a multi-value field into separate events for each value of the multi-value field.
description = For each result with the specified field, create a new result for each value of that field in that result if it a multivalue field.
maintainer = steveyz
usage = public
appears-in = 3.3
comment1 = Create new events for each value of multi-value field, "foo".
example1 =  ... | mvexpand foo
comment2 = Create new events for the first 100 values of multi-value field, "foo".
example2 =  ... | mvexpand foo limit=100
related = makemv, mvcombine, nomv
tags = separate divide disconnect multivalue
category = results::generate

##################
# makemv
##################

[makemv-command]
syntax = makemv (delim=<string>|tokenizer=<string>)? (allowempty=<bool>)? (setsv=<bool>?) <field>
shortdesc = Changes a specified field into a multi-value field during a search.
description = Treat specified field as multi-valued, using either a simple string delimiter (can be multicharacter), or a regex tokenizer.  If neither is provided, a default delimiter of " " (single space) is assumed.  \
	      The allowempty=<bool> option controls if consecutive delimiters should be treated as one (default = false).\
	      The setsv boolean option controls if the original value of the field should be kept for the single valued version.  It is kept if setsv = false, and it is false by defult.
maintainer = steveyz
usage = public
appears-in = 3.3
comment1 = Separate the value of "foo" into multiple values.
example1 = ... | makemv delim=":" allowempty=t foo
comment2 = For sendmail search results, separate the values of "senders" into multiple values. Then, display the top values.
example2 = eventtype="sendmail" | makemv delim="," senders | top senders
related = mvcombine, mvexpand, nomv
tags = multivalue convert
category = fields::convert

##################
# nomv
##################

[nomv-command]
syntax = nomv <field>
shortdesc = Changes a specified multi-value field into a single-value field at search time.
description = Converts values of the specified multi-valued field into one single value (overrides multi-value field configurations set in fields.conf).
maintainer = steveyz
usage = public
appears-in = 3.3
comment = For sendmail events, combine the values of the senders field into a single value; then, display the top 10 values.
example = eventtype="sendmail" | nomv senders | top senders
related = makemv, mvcombine, mvexpand, convert
tags = single multivalue
category = fields::convert

##################
# newseriesfilter
##################

[newseriesfilter-command]
syntax = newseriesfilter <string>
description = Used by timechart.
maintainer = steveyz
usage = internal
appears-in = 3.0
example =

##################
# nokv
##################

[nokv-command]
syntax = nokv
description = Tells the search pipeline not to perform any automatic key/value extraction.
usage = internal
maintainer = ssorkin
appears-in = 3.0
example1 = ... | nokv

##################
# outlier
##################

[outlier-command]
syntax = outlier (<outlier-option> )* (<field-list>)?
alias = outlierfilter
shortdesc =  Removes outlying numerical values.
description = Removes or truncates outlying numerical values in selected fields. If no fields are specified, then outlier will attempt to process all fields.
maintainer = steveyz
appears-in = 3.0
comment1 = Remove all outlying numerical values.
example1 = ... | outlier
comment2 = For a timechart of webserver events, transform the outlying average CPU values.
example2 = 404 host="webserver" | timechart avg(cpu_seconds) by host | outlier action=tf
usage = public
related = anomalies, anomalousvalue, cluster, kmeans
tags = outlier anomaly unusual odd irregular dangerous unexpected
category = reporting

[outlier-option]
syntax = <outlier-action-opt>|<outlier-param-opt>|<outlier-uselower-opt>
description = Outlier options

[outlier-action-opt]
syntax = action=(rm|remove|tf|transform)
simplesyntax = action=(remove|transform)
description = What to do with outlying events.  RM | REMOVE removes the field from events containing outlying numerical values.  \
	      TF | TRANSFORM truncates the outlying value to the threshold for outliers and, if mark=true, prefixes the value with "000"
default = "action=transform"

[outlier-param-opt]
syntax = param=<num>
description = Parameter controlling the threshold of outlier detection.  An outlier is defined as \
              a numerical value that is outside of param multiplied the inter-quartile range.
default = "param=2.5"

[outlier-uselower-opt]
syntax = uselower=<bool>
description = Controls whether to look for outliers for values below the median in addition to above it
default = "uselower=false"

##################
# dump
##################

[dump-command]
syntax = dump basefilename=<string> (rollsize=<num>) (maxlocal=<num>)? (compress=<num>)? (format=<string>)? (fields=<comma-delimited-string>)?
shortdesc   = Executes a given search query and export events to a set of chunk files on local disk.
description = Executes a given search query and export events to a set of chunk files on local disk. This command runs a specified search \
              query and oneshot export search result to local disk at "$SPLUNK_HOME/var/run/splunk/dispatch/&lt;sid&gt;/dump". It recognizes  \
              a special field in the input events, _dstpath, which if set will be used as a path to be appended to dst to compute final destination path. \i\\
              "basefilename"       - prefix of the export filename. \i\\
              "rollsize"           - minimum file size at which point no more events are written to the file and \i\\
                                     it becomes a candidate for HDFS transfer, unit is "MB", default "64MB". \i\\
              "maxlocal"           - maximum allowable local disk usage at which point in-memory data and local files are \i\\
                                     flushing and exporting to HDFS, unit is "MB", default "1GB". \i\\
              "compress"           - gzip compression level from 0 to 9, 0 means no compression, higher number \i\\
                                     means more compression and slower writing speed, default 2. \i\\
              "format"             - output data format, supported values are raw | csv | tsv | json | xml \i\\
              "fields"             - list of splunk event fields exported to export data, invalid fields will be ignored \i\\

comment1 = Export all events from index "bigdata" to the location "YYYYmmdd/HH/host" at "$SPLUNK_HOME/var/run/splunk/dispatch/&lt;sid&gt;/dump/" \
           directory on local disk with "MyExport" as the prefix of export filenames. Partitioning of the export data is achieved by eval preceeding the dump command.
example1 = index=bigdata | eval _dstpath=strftime(_time, "%Y%m%d/%H") + "/" + host | dump basefilename=MyExport
comment2 = Export all events from index "bigdata" to the location "/myexport/host/source" on local disk with "MyExport" as the prefix of export filenames
example2 = index=bigdata | dump basefilename=MyExport
category = exporting

##################
# outputcsv
##################

[outputcsv-command]
syntax = outputcsv (append=<bool>)? (create_empty=<bool>)? (dispatch=<bool>)? (usexml=<bool>)? (singlefile=<bool>)? (<filename>)?
shortdesc = Outputs search results to the specified CSV file.
description = If no filename specified, rewrites the contents of each result as a CSV row into the "_xml" field. \
	      Otherwise writes into file (appends ".csv" to filename if filename has no existing extension).  \
	      If singlefile is set to true and output spans multiple files, collapses it into a single file.  \
	      the option usexml=[t|f] specifies whether or not to encode the csv output into xml and has effect \
	      only when no filename is specified.  This option should not specified when invoking outputcsv from \
	      the UI.  If dispatch option is set to true, filename refers to a file in the job directory in \
	      $SPLUNK_HOME/var/run/splunk/dispatch/<job id>/ \
	      If 'create_empty' is true and no results are passed to outputcsv, an 0-length file is created.  \
	      When false (the default) no file is created and the file is deleted if it previously existed.   \
	      If 'append' is true, we will attempt to append to an existing csv file if it exists or create a \
	      file if necessary.  If there is an existing file that has a csv header already, we will only emit \
	      the fields that are referenced by that header.  (Defaults to false)  .gz files cannot be append to.
usage = public
maintainer = steveyz
appears-in = 3.0
comment1 = Output search results to the CSV file 'mysearch.csv'.
example1 = ... | outputcsv mysearch
related = inputcsv
tags = output csv save write
category = results::write

##################
# outputlookup
##################

[outputlookup-command]
syntax = outputlookup (append=<bool>) (create_empty=<bool>)? (max=<int>)? (key_field=<field_name>)? (createinapp=<bool>)? (<filename>|<string:tablename>)
shortdesc = Save search results to specified static lookup table.
description = Saves results to a lookup table as specified by a filename (must end with .csv or .gz) or a table name (as specified by a stanza name in transforms.conf).  If the lookup file does not exist, we will by default create the file in the lookups directory of the current application.  If the 'createinapp' option is set to false or if there is no current application context, then we will create the file in the system lookups directory. \
	      If 'create_empty' is true (the default) and no results are passed to outputlookup, an 0-length file is created.  \
	      When false no file is created and the file is deleted if it previously existed. \
        If 'key_field' is set to a valid field name and this is a key-value store-based lookup, we will \
        attempt to use the specified field as the key to a value and replace that value. \
	      If 'append' is true, we will attempt to append to an existing csv file if it exists or create a \
	      file if necessary.  If there is an existing file that has a csv header already, we will only emit \
	      the fields that are referenced by that header.  (Defaults to false).  .gz files cannot be appended to.
usage = public
maintainer = steveyz
appears-in = 4.0
example1 = | outputlookup users.csv
example2 = | outputlookup usertogroup
comment1 = Write to "users.csv" lookup file (under $SPLUNK_HOME/etc/system/lookups or $SPLUNK_HOME/etc/apps/*/lookups).
comment2 = Write to "usertogroup" lookup table (as specified in transforms.conf).
tags = output csv save write lookup table
category = results::write
related = inputlookup, lookup, outputcsv, outputlookup
optout-in = lite, lite_free

##################
# outputraw
# TODO: need complete spec.
##################
[outputraw-command]
syntax = outputraw
shortdesc = Outputs search results in a simple, raw text-based format.
description = Outputs search results in a simple, raw text-based format, with each attribute value on a separate text line.  Useful for commandline searches.
maintainer = david
example1 = ... | outputraw
usage = deprecated
related = outputcsv, outputtext
tags = output
category = formatting
optout-in = lite, lite_free


##################
# outputrawr
##################
[outputrawr-command]
syntax = outputrawr
description = An easter egg command that replaces incoming search results with a single result with ascii art.
usage = fun
maintainer = brian
appears-in = 3.0
example1 = ... | outputrawr
tags = output
category = formatting

##################
# outputtext
##################
[outputtext-command]
syntax = outputtext (usexml=<bool>)?
shortdesc = Outputs the raw text (_raw) of results into the _xml field.
description = Rewrites the _raw field of the result into the "_xml" field. \
	      If usexml is set to true (the default), the _raw field is \
	      XML escaped.
usage = public beta
maintainer = ssorkin
appears-in = 3.0
comment1 = Output the "_raw" field of your current search into "_xml".
example1 = ... | outputtext
related = outputcsv, outputraw
tags = output
category = formatting

##################
# overlap
##################

[overlap-command]
syntax = overlap
shortdesc = Finds events in a summary index that overlap in time or have missed events.
description = Find events in a summary index that overlap in time, or\
 find gaps in time during which a scheduled saved search may have\
 missed events.  Note: If you find a gap, run the search over the period\
 of the gap and summary index the results (using | collect). If you\
 find overlapping events, manually delete the overlaps from the summary\
 index by using the search language.  Invokes an external python script\
 (in etc/searchscripts/sumindexoverlap.py), which expects input events\
 from the summary index and finds any time overlaps and gaps between\
 events with the same 'info_search_name' but different\
 'info_search_id'.  Input events are expected to have the following\
 fields: 'info_min_time', 'info_max_time' (inclusive and exclusive,\
 respectively) , 'info_search_id' and 'info_search_name' fields.
maintainer = steveyz/ledion
usage = public
appears-in = 3.3
comment = Find overlapping events in "summary".
example = index=summary | overlap
related = collect sistats sitop sirare sichart sitimechart
tags = collect overlap index summary summaryindex
category = index::summary
optout-in = lite, lite_free


##################
# pivot
##################

[pivot-command]
syntax      = pivot <modelName> <objectName> <pivotSearch>
shortdesc   = Allows user to run pivot searches against a particular datamodel object.
description = Must be the first command in a search.  User must specify the model, object, and the pivot search to run. \
	      The command will expand and run the specified pivot search.
#example1 =  | pivot myModel myObject count(myObject)
#example2 =  | pivot myModel myObject avg(myValue) SPLITROW foo AS bar FILTER thing BY top 10 count(otherThing)
category = reporting
appears-in = 6.0
maintainer = aneels
usage = public
related = datamodel
tags = datamodel model pivot
optout-in = lite, lite_free

##################
# predict
##################
[predict-command]
syntax = predict <variable_to_predict> (as <new_field_name>) (<option_name>=<option_value>)
shortdesc = Predict future values of fields.
description = Predict future values of fields. The predicted values are written to the new_field_name field.\
		The current option types and names are: algorithm=<algorithm_name>, future_timespan=<num>,\
		,holdback=<num>, period=<num>, upperXX=<field>, lowerYY=<field>, correlate=<field>.\
		The algorithm names are: LL, LLP, LLT and LLB. The first three deal with univaritate time series while\
		the fourth deals with bivariate time series. Each option has a default value and thus can be left\
		unspecified. The XX and YY in upperXX and lowerYY options specify confidence levels above and below\
		the predicted value. By default they are both set to 95 (meaning 95 percents confidence interval).\
		The period option specifies the period of the time series. It is automatically computed but should be specified\
		if known. Also, the period option is only used if the algorithm is LLP and is ignored otherwise.\
		The holdback option specifies the number of data points (from the end) that are NOT to be used in the\
		predict algorithm. Its typical use is to allow comparing predicted values to the actual data.
example1 = ... | predict foo
comment1 = predict foo using either LL or LLP depending on whether foo is periodic
example2 = ... | predict foo as fubar algorithm=LL upper90=high lower97=low future_timespan=10 holdback=20
comment2 = upper and lower confidence intervals need not be equaled
example3 = ... | predict foo2 as fubar algorithm=LLB correlate=foo1 holdback=100
comment3 = llustrates the LLB algorithm. The foo2 field is predicted by correlating it with the foo1 field.
usage = public
maintainer = nghi
appears-in = Ace
category = reporting
related = trendline, x11
tags = forecast predict univariate bivariate Kalman

##################
# preview
##################

[preview-command]
syntax = preview
shortdesc = See what events from a file will look like when indexed without actually indexing the file.
description = Given a source file and a set of props.conf settings in \
              $SPLUNK_HOME/var/run/splunk/dispatch/<job_id>/indexpreview.csv, \
              generate the events that the file would yield if it were indexed.
usage = internal
maintainer = ewoo
appears-in = 4.3
category = results::generate
tags = index preview

##################
# rare
##################

[rare-command]
syntax = rare <top-opt>* <field-list> (<by-clause>)?
shortdesc = Displays the least common values of a field.
description = Finds the least frequent tuple of values of all fields in the field list. \
	    If optional by-clause is specified, this command will return rare tuples of values for\
	    each distinct tuple of values of the group-by fields.
comment1 = Find the least common "user" value for a "host".
example1 = ... | rare user by host
commentcheat = Return the least common values of the "url" field.
examplecheat = ... | rare url
category = reporting
maintainer = steveyz
appears-in = 3.0
usage = public
supports-multivalue = true
related = top, stats, sirare
tags = rare few occasional scarce sparse uncommon unusual

##################
# regex
##################

[regex-command]
syntax = regex ((<field>(=|!=)<regex-expression>)|<regex-expression>)
shortdesc = Removes results that do not match the specified regular expression.
description = Removes results that do not match the specified regular expression. You can specify for the regex to keep results that match the expression, or to keep those that do not match.  Note: if you want to use the "or" ("|") command in a regex argument, the whole regex expression must be surrounded by quotes (ie. regex "expression"). Matches the value of the field against the unanchored regex and only keeps those events that match in the case of '=' or do not match in the case of '!='. If no field is specified, the match is against "_raw".
example1 = ... | regex _raw="complicated|regex(?=expression)"
example2 = ... | regex _raw="(?=!\d)10.\d{1,3}\.\d{1,3}\.\d{1,3}(?!\d)"
commentcheat = Keep only search results whose "_raw" field contains IP addresses in the non-routable class A (10.0.0.0/8).
examplecheat = ... | regex _raw="(?<!\d)10.\d{1,3}\.\d{1,3}\.\d{1,3}(?!\d)"
category = results::filter
appears-in = 3.0
maintainer = ssorkin
usage = public
related = rex, search
tags = regex regular expression filter where

[regex-expression]
syntax = (\")?<string>(\")?
description = A Perl Compatible Regular Expression supported by the pcre library.
comment1 =  Selects events whose _raw field contains ip addresses in the non-routable class A (10.0.0.0/8).
example1 = ... | regex _raw="(?<!\d)10.\d{1,3}\.\d{1,3}\.\d{1,3}(?!\d)"


##################
# relevancy
##################

[relevancy-command]
syntax = relevancy
shortdesc = Calculates how well the event matches the query.
description = Calculates the 'relevancy' field based on how well the events _raw field matches the keywords of the 'search'.  Useful for retrieving the best matching events/documents, rather than the default time-based ordering. Events score a higher relevancy if they have more rare search keywords, more frequently, in fewer terms.  For example a search for "disk error" will favor a short event/document that has 'disk' (a rare term) several times and 'error' once, than a very large event that has 'disk' once and 'error' several times.
comment1 = Calculate the relevancy of the search and sort the results in descending order.
example1 = disk error | relevancy | sort -relevancy
maintainer = david
appears-in = 4.0
usage = public
tags = search relevance precision text doc ir
related = abstract, highlight, sort
category = fields::add


##################
# rename
##################

[rename-command]
syntax = rename (<wc-field> as <wc-field>)+
shortdesc = Renames a specified field (wildcards can be used to specify multiple fields).
description = Renames a field. If both the source and destination fields are \
	      wildcard expressions with he same number of wildcards, \
	      the renaming will carry over the wildcarded portions to the \
	      destination expression.
comment1 = Rename the "count" field.
example1 = ... | rename count as "Count of Events"
comment2 = Rename fields beginning with "foo".
example2 = ... | rename foo* as bar*
commentcheat = Rename the "_ip" field as "IPAddress".
examplecheat = ... | rename _ip as IPAddress
category = fields::modify
maintainer = steveyz
appears-in = 3.0
usage = public
tags = rename alias name as aka
related = fields

##################
# replace
##################

[replace-command]
syntax = replace (<wc-str> with <wc-str>)+ (in <field-list>)?
shortdesc = Replaces values of specified fields with a specified new value.
description = Replaces a single occurrence of the first string with the second \
	      within the specified fields (or all fields if none were specified). \
	      Non-wildcard replacements specified later take precedence over those specified earlier. \
	      For wildcard replacement, fuller matches take precedence over lesser matches.\
	      To assure precedence relationships, one is advised to split the replace into \
	      two separate invocations. \
	      When using wildcarded replacements, the result must have the same number \
	      of wildcards, or none at all. \
	      Wildcards (*) can be used to specify many values to replace, or replace values with.
example1 = ... | replace 127.0.0.1 with localhost
example2 = ... | replace 127.0.0.1 with localhost in host
example3 = ... | replace 0 with Critical, 1 with Error in msg_level
example4 = ... | replace aug with August in start_month end_month
example5 = ... | replace *localhost with localhost in host
example6 = ... | replace "* localhost" with "localhost *" in host
commentcheat = Change any host value that ends with "localhost" to "localhost".
examplecheat = ... | replace *localhost with localhost in host
category = fields::modify
maintainer = steveyz
appears-in = 3.0
usage = public
tags = replace change set
related = fillnull setfields rename

##################
# rex
##################

[rex-command]
syntax      = rex (field=<field>)? ( ( <regex-expression> (max_match=<int>)? ) | mode=sed <sed-expression>)
shortdesc = Specifies a Perl regular expression named groups to extract fields while you search.
description = Matches the value of the field against the unanchored regex and extracts \
	      the perl regex named groups into fields of the corresponding names. If \
	      mode is set to 'sed' the given sed expression will be applied to the value \
	      of the chosen field (or to _raw if a field is not specified). \
              max_match controls the number of times the regex is matched, if greater than one \
              the resulting fields will be multivalued fields, defaults to 1, use 0 to mean unlimited.
comment1 = Anonymize data matching pattern
example1 =  ... | rex mode=sed "s/(\\d{4}-){3}/XXXX-XXXX-XXXX-/g"
commentcheat = Extract "from" and "to" fields using regular expressions. If a raw event contains "From: Susan To: Bob", then from=Susan and to=Bob.
examplecheat = ... | rex field=_raw "From: (?<from>.*) To: (?<to>.*)"
category = fields::add
appears-in = 3.2
maintainer = ssorkin
usage = public
related = extract, kvform, multikv, xmlkv, regex
tags = regex regular expression extract

##################
# rtorder
##################
[rtorder-command]
syntax = rtorder (discard=<bool>)? (buffer_span=<span-length>)? (max_buffer_size=<int>?)
shortdesc = Buffers events from real-time search to emit them in ascending time order when possible
description = The rtorder command creates a streaming event buffer that takes input events, stores them \
	      in the buffer in ascending time order, and emits them in that order from the buffer \
	      only after the current time reaches at least the span of time given by buffer_span \
	      after the timestamp of the event.  The buffer_span is by default 10 seconds.  \
	      Events will also be emitted from the buffer if the maximum size of the buffer is exceeded.\
	      The default max_buffer_size is 50000, or the max_result_rows setting of the [search] \
	      stanza in limits.conf.  If an event is received as input that is earlier than an event \
	      that has already been emitted previously, that out of order event will be emitted \
	      immediately unless the discard option is set to true (it is false by default).  \
	      When discard is set to true, out of order events will always been discarded, assuring \
	      that the output is always strictly in time ascending order. \
example1 = ... | rtorder discard=t buffer_span=5m
comment1 = Keep a buffer of the last 5 minutes of events, emitting events in ascending time order once \
	   they are more than 5 minutes old.  Newly received events that are older than 5 minutes are \
	   dicarded if an event after that time has already been emitted.
maintainer = steveyz
usage = public
appears-in = 4.1
related = sort
tags = realtime sort order

##################
# select
##################
[select-command]
syntax = select <select-arg>
alias = report
description = Runs a SQL SELECT statement.
shortdesc = The select command is deprecated. If you want to compute aggregate statistics, try using stats, chart, or timechart. If you want to filter search results, try using search or where.
comment1 = Search for LOGINs, pass those results to a temporary database table "results", then perform the select operation, producing search results with two fields _ip and _url. If an event did not contain a value for _ip or _url, its row would be ignored.
example1 = login | select _ip, _url from results
comment2 = Return ip's that contain 7.
example2 = ... | select _ip from results where _ip regexp "7"
comment3= Return ip's that start with "12" and end with "255".
example3 = ... | select _ip from results where _ip regexp "^12.*255$"
maintainer = david
# deprecated since 3.2
usage = deprecated
appears-in = 3.0
optout-in = lite, lite_free

[select-arg]
syntax = <string>
description = Any value sql select arguments, per the syntax found at\
 http://www.sqlite.org/lang_select.html.  If no "from results" is\
 specified in the select-arg it will be inserted it automatically.\
 Runs a SQL Select query against passed in search\
 results. All fields referenced in the select statement must be\
 prefixed with an underscore. Therefore, "ip" should be references as\
 "_ip" and "_raw" should be referenced as "__raw". Before the select\
 command is executed, the previous search results are put into a\
 temporary database table called "results". If a row has no values,\
 "select" ignores it to prevent blank search results.

##################
# script
##################
[script-command]
syntax = script <script-name-arg> (<script-arg> )* (<maxinputs-opt>)?
alias = run
shortdesc = Runs an external Python-imeplemented search command.
description = Calls an external python program that can modify or generate search results.  \
              Scripts must be declared in commands.conf and be located in "$SPLUNK_HOME/etc/apps/app_name/bin". \
              The  scripts are run with "$SPLUNK_HOME/bin/python".
comment1 = Run the Python script "myscript" with arguments, myarg1 and myarg2; then, email the results.
example1 = ... | script python myscript myarg1 myarg2 | sendemail to=david@splunk.com
maintainer = david
appears-in = 3.0
usage = public
tags = script run python perl custom
category = search::external
optout-in = lite, lite_free

[script-name-arg]
syntax = <string>
description = The name of the scripted search command to execute, as defined in commands.conf
example1 = sendemail

[maxinputs-opt]
syntax = maxinputs=<int>
description = Determines the maximum number of input results passed to the script.
example1 = maxinputs=1000
default = maxinputs=100

[script-arg]
syntax = <string>
description = An argument passed to the script.
example1 = to=bob@mycompany.com


##################
# sample
##################

[sample-command]
syntax = sample <int:sample_size> (<by-clause>)? (resample=<int>)?
shortdesc = Returns a random sampling of N search results.
description = returns a random sampling of N search results.\
    The inclusion of a 'by' clause gives each unique combination of\
    field values its own N random searchresults.  Thus, "sample 100 by\
    source" will return 100 random results for each source. If there\
    are 10 sources, therefore, 1000 results would be returned.  If\
    RESAMPLE > 0 (it defaults to 0), those 1000 results would then be\
    randomly sampled to return RESAMPLE results, basically giving each\
    source an approximately equal number of events.  Thus, "sample 100\
    by source resample=200" would return 200 random results, from\
    equal-sized pools of 100 random events for each source.  Simply\
    doing "sample 200" would also return 200 random events, but the\
    sampling would be biased towards more verbose sources.
maintainer = david
appears-in = Madonna
example1 = error | sample 100
comment1 = Return 100 random error events.
example2 = sourcetype=access* | sample 10 by user | cluster
comment2 = Return a sample of 10 web access events for each user, and then cluster those samples.
usage = experimental
tags = sample poll
optout-in = lite, lite_free

##################
# savedsearch
##################
[savedsearch-command]
syntax = savedsearch <string> (<savedsearch-opt> )*
alias = macro, savedsplunk
shortdesc = Runs a saved search by name.
description = Runs a saved search. \
              If the search contains replacement terms, will perform string replacement.  \
              For example, if the search were something like "index=$indexname$", then \
              the indexname term can be provided at invocation time of the savedsearch command.
usage = public
maintainer = brian
appears-in = 3.0
comment1 = Run the searchindex command with an index provided (as per above)
example1 = | savedsearch searchindex index=main
commentcheat = Run the "mysecurityquery" saved search.
examplecheat = | savedsearch mysecurityquery
category = results::generate
tags = search macro saved bookmark
related = search

[savedsearch-opt]
syntax = <savedsearch-macro-opt>|<savedsearch-replacement-opt>

[savedsearch-macro-opt]
syntax = nosubstitution=<bool>
description = If true, no string substitution replacements are made.
default = nosubstitution=false

[savedsearch-replacement-opt]
syntax = <string>=<string>
description = A key value pair to be used in string substitution replacement.

##################
# sendalert
##################

[sendalert-command]
syntax = sendalert <alert_action_name> (results_link=<url>)? (results_path=<path>)? (param.<name>=<value>)*
shortdesc = Triggers the given custom alert action.
description = Triggers the given alert action according to the custom alert actions framework. The command\
              gathers the configuration for the alert action (from alert_actions.conf, the saved search \
              and custom parameters passed via the command arguments) and performs token replacement. Then\
              the command determines the alert action script and arguments to run, creates the alert action\
              payload and executes the script, handing over the payload via STDIN to the script process.\
              When running the custom script the sendalert command also honors the maxtime setting from\
              alert_actions.conf and forcefully terminates the process if it's running longer than the\
              configured threshold (by default this is set to 5 minutes).
appears-in = 6.3
usage = internal
maintainer = spuchbauer
example1 = ... | sendalert hipchat param.room="SecOps" param.message="There is a security problem!"
comment1 = Trigger the hipchat custom alert action and pass in room and message as custom parameters.
tags = custom alert
category = alerting

##################
# sendemail
##################

[sendemail-command]
syntax = sendemail <to-option> <from-option>? <cc-option>? <bcc-option>? <subject-option>? <message-option>? <footer-option>? <sendresults-option>? <inline-option>? <format-option>? <sendcsv-option>? <sendpdf-option>? <pdfview-option>? (<paperorientation-option>)? <papersize-option>? <priority-option>? <server-option>? <graceful-option>? <content_type-option>? <width_sort_columns-option>? <use_ssl-option>? <use_tls-option>? <maxinputs-option>? <maxtime-option>?
shortdesc =  Emails search results to specified email addresses.
description = Emails search results to a specified email addresses.
appears-in = 3.0
usage = public
maintainer = david
comment1 = Send search results to the specified email.
example1 = ... | sendemail to="elvis@splunk.com"
comment2 = Send search results in HTML format with the subject "myresults".
example2 = ... | sendemail to="elvis@splunk.com,john@splunk.com" content_type=html subject=myresults server=mail.splunk.com
tags = email mail alert
category = alerting

[to-option]
syntax = to=<email_list>
description = List of email addresses to send search results to.

[from-option]
syntax = from=<email_list>
description = Email address from line.
default = splunk@<hostname>

[cc-option]
syntax = cc=<email_list>
description = Cc line; comma-separated quoted list of valid email addresses.

[bcc-option]
syntax = bcc=<email_list>
description = Blind cc line; comma-separated quoted list of valid email addresses.

[email_list]
syntax = <email_address> (, <email_address> )*
example1 = "bob@smith.com, elvis@presley.com"

[email_address]
# if we supported regex, perhaps: [A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}
syntax = <string>
example1 = bob@smith.com

[subject-option]
syntax = subject=<string>
description = Specifies the subject line.
default = Splunk Results

[message-option]
syntax = message=<string>
description = Specifies the message sent in the email.
default = If sendresults=true: Search complete. \
          If sendresults=true, inline=true, and either sendpdf=false or sendcsv=false: Search results. \
          If sendpdf=true or sendcsv=true: Search results attached.

[footer-option]
syntax = footer=<string>
description = Specify an alternate email footer.
default = 'If you believe you've received this email in error, \
          please see your Splunk administrator.\r\n\r\nsplunk > the engine for \
          machine data.'

[sendresults-option]
syntax = sendresults=<bool>
description = Determines whether the results should be included with the email.
default = false

[inline-option]
syntax = inline=<bool>
description = Specifies whether to send the results in the message body or as an attachment.
default = true

[format-option]
syntax = format=csv | table | raw
description = Specifies how to format inline results.
default = table

[sendcsv-option]
syntax = sendcsv=<bool>
description = Specify whether to send the results with the email as an attached csv file or not.
default = false

[sendpdf-option]
syntax = sendpdf=<bool>
description = Specify whether to send the results with the email as an attached PDF or not.
default = false

[pdfview-option]
syntax = pdfview=<string>
description = Name of view to send as a PDF.

[paperorientation-option]
syntax = paperorientation= portrait | landscape
description = Paper orientation: portrait or landscape.
default = portrait

[papersize-option]
syntax = papersize=letter | legal | ledger | a2 | a3 | a4 | a5
description = Default paper size for PDFs. Acceptable values: letter, legal, ledger, a2, a3, a4, a5.
default = letter

[priority-option]
syntax = priority=highest | high | normal | low | lowest
description = Set the priority of the email as it appears in the email client. Lowest or 5, low or 4, high or 2, highest or 1.
default = 3

[server-option]
syntax = server=<string>
description = If the smtp server is not local, use this to specify it.
default = localhost

[graceful-option]
syntax = graceful=<bool>
description = If set to true, no error is thrown, if email sending fails and thus the search pipeline continues execution as if sendemail was not there.
default = false

[content_type-option]
syntax = content_type=html | plain
description = The content type of the email. Plain sends email as plain text and html sends email as a multipart email that include both text and html.
default = html

[width_sort_columns-option]
syntax = width_sort_columns=<bool>
description = This is only valid for plain text emails. Specifies whether the columns should be sorted by their width.
default = true

[use_ssl-option]
syntax = use_ssl=<bool>
description = Whether to use ssl when communicating with the smtp server. When true, you must also specify both the server name or ip address and the tcp port in the 'mailserver' attribute.
default = false

[use_tls-option]
syntax = use_tls=<bool>
description = Specify whether to use tls when communicating with the smtp server.
default = false

[maxinputs-option]
syntax = maxinputs=<int>
description = Set the maximum number of search results sent via alerts.
default = 50000

[maxtime-option]
syntax = maxtime=<int>m | s | h | d
description = The maximum amount of time that the execution of an action is allowed to take before the action is aborted.

##################
# setfields
##################
[setfields-command]
syntax = setfields <setfields-arg>(, <setfields-arg)*
shortdesc = Sets the field values for all results to a common value.
description = Sets the value of the given fields to the specified values for each event in the result set. \
              Missing fields are added, present fields are overwritten.
maintainer = ledion
usage = deprecated
note = use 'eval field="value"'
appears-in = 3.2
example1 = ... | setfields ip="10.10.10.10", foo="foo bar"
category = fields::add
related = fillnull setfields rename
tags = annotate set note

[setfields-arg]
syntax = <string>="<string>"
description = a key-value pair with quoted value. Standard key cleaning will be performed, ie all non-alphanumeric \
              characters will be replaced with '_' and leading '_' will be removed.

##################
# spath
##################

[spath-command]
syntax      = spath (output=<field>)? (path=<datapath>|<datapath>)? (input=<field>)?
shortdesc = Extracts values from structured data (XML or JSON) and stores them in a field or fields.
description = When called with no path argument, spath extracts all fields from the \
	    first 5000 (limit is configurable via limits.conf characters, with the produced fields named by their path. \
	    If a path is provided, the value of this path is extracted to a field \
	    named by the path by default, or to a field specified by the output \
	    argument if it is provided.\
	    Paths are of the form 'foo.bar.baz'.  Each level can also have an \
	    optional array index, deliniated by curly brackets ex 'foo{1}.bar'. \
	    All array elements can be represented by empty curly brackets e.g. 'foo{}'. \
	    The final level for XML queries can also include an attribute name, \
	    also enclosed by curly brackets,  e.g. 'foo.bar{@title}'. \
	    By default, spath takes the whole event as its input.  The input \
	    argument can be used to specify a different field for the input source.
example1 =  ... | spath output=myfield path=foo.bar.baz
example2 =  ... | spath input=oldfield output=newfield path=catalog.book{@id}
example3 =  ... | spath server.name
category = fields::add
appears-in = 4.3
maintainer = aneels
usage = public
related = rex, regex
tags = spath xpath json xml extract

##################
# table
##################
[table-command]
syntax = table <wc-field-list>
description = Returns a table formed by only the fields specified in the arguments. Columns are \
              displayed in the same order that fields are specified. Column headers are the field \
              names. Rows are the field values. Each row represents an event.
shortdesc = Returns a table formed by only the fields specified in the arguments.
usage = public
appears-in = 4.1
maintainer = steveyz
example1 = ... | table foo bar baz*
comment1 = Resulting table has field foo then bar then all fields that start with 'baz'
tags = fields
related = fields
category = results::filter

##################
# transpose
##################
[transpose-command]
syntax = transpose (<int>)? (column_name=<string>)? (header_field=<field>)? (include_empty=<bool>)?
description = Turns rows into columns (each row becomes a column).  Takes an optional integer argument that limits the number of rows we transpose (default = 5).  column_name is the name of the field in the output where the names of the fields of the inputs will go (default = "column").  header_field, if provided, will use the value of this field in each input row as the name of the output field for that column (default = no field provided, output fields will be named "row 1", "row 2", ...).  include_empty is an optional boolean option, that if false, will exclude any field/column in the input that had no values for any row (defaults = true).
shortdesc = Turns rows into columns.
usage = public
appears-in = 3.x
maintainer = steveyz
example1 = ... | transpose
comment1 = Turns the first five rows into columns
example2 = ... | transpose 20
comment2 = Turns the first 20 rows into columns
example3 = ... | transpose column_name="Test Name" header_field=sourcetype include_empty=false
comment3 = Turns the first five rows into columns, where the input field names are put into the output field called "Test Name", and the input row values for the sourcetype field will be used as the output field names.
tags = fields, stats
related = fields, stats
category = reporting

##################
# uniq
##################
[uniq-command]
syntax = uniq
maintainer = david
shortdesc = Filters out repeated adjacent results
description = Removes any search result that is an exact duplicate with the adjacent result before it.
usage = public
appears-in = 3.0
comment1 = For the current search, keep only unique results.
example1 = ... | uniq
related = dedup
tags = uniq unique duplicate redundant extra
category = results::filter

##################
# metasearch
##################

[metasearch-command]
simplesyntax = metasearch <logical-expression>?
syntax = metasearch <logical-expression>?
shortdesc = Retrieves event metadata from indexes based on terms in the <logical-expression>
description = Retrieves event metadata from indexes based on terms in the <logical-expression>.  Metadata fields include source, sourcetype, host, _time, index, and splunk_server.
appears-in = 4.2
usage = public
maintainer = steveyz
comment1 = Return metadata for events with "404" and from host "webserver1"
example1 = 404 host="webserver1"
category = search::search
tags = search query find
related = search metadata

##################
# search
##################

[search-command]
simplesyntax = search <logical-expression>?
syntax = search <logical-expression>?
description = If the first search command, retrieve events from the indexes, using keywords, quoted phrases, wildcards, and key/value expressions; if not the first, filter results.
shortdesc = Filters results using keywords, quoted phrases, wildcards, and key/value expressions.
appears-in = 3.x
usage = public
maintainer = ssorkin
comment1 = Search for events with "404" and from host "webserver1"
example1 = 404 host="webserver1"
comment2 = Search for events with either codes 10 or 29, and a host that isn't "localhost" and an xqp that is greater than 5
example2 = (code=10 OR code=29) host!="localhost" xqp>5
commentcheat1 = Keep only search results that have the specified "src" or "dst" values.
examplecheat1 = src="10.9.165.*" OR dst="10.9.165.8"
category = search::search
tags = search query find where filter daysago enddaysago endhoursago endminutesago endmonthsago endtime endtime eventtype eventtypetag host hosttag hoursago minutesago monthsago searchtimespandays searchtimespanhours searchtimespanminutes searchtimespanmonths source sourcetype startdaysago starthoursago startminutesago startmonthsago starttime starttimeu tag

[logical-expression]
simplesyntax = (NOT)? <logical-expression>|<comparison-expression>|(<logical-expression> OR? <logical-expression>)
syntax = ("(" <logical-expression> ")")|<time-opts>|<search-modifier>|((not)? <logical-expression>)|<index-expression>|<comparison-expression>|(<logical-expression> (or)? <logical-expression>)

[index-expression]
syntax = \"<string>\"|<term>|<search-modifier>

[comparison-expression]
syntax = <field><cmp><value>

[cmp]
syntax = =|!=|<|<=|>|>=

[value]
syntax = <lit-value>|<field>

[lit-value]
syntax = <string>|<num>

[index-specifier]
syntax = index=<string>
description = Search the specified index instead of the default index

[time-opts]
syntax = (<timeformat>)? (<time-modifier> )*

[search-modifier]
syntax = <sourcetype-specifier>|<host-specifier>|<source-specifier>|<savedsplunk-specifier>|<eventtype-specifier>|<eventtypetag-specifier>|<hosttag-specifier>|<tag-specifier>

[time-modifier]
syntax = <starttime>|<startdaysago>|<startminutesago>|<starthoursago>|<startmonthsago>|<starttimeu>|<endtime>|<enddaysago>|<endminutesago>|<endhoursago>|<endmonthsago>|<endtimeu>|<searchtimespanhours>|<searchtimespanminutes>|<searchtimespandays>|<searchtimespanmonths>|<daysago>|<minutesago>|<hoursago>|<monthsago>

[timeformat]
syntax = timeformat=<string>
description = Set the time format for starttime and endtime terms.
example1 = timeformat=%m/%d/%Y:%H:%M:%S
default = timeformat=%m/%d/%Y:%H:%M:%S

[sourcetype-specifier]
syntax = sourcetype=<string>
description = Search for events from the specified sourcetype

[host-specifier]
syntax = host=<string>
description = Search for events from the specified host

[source-specifier]
syntax = source=<string>
description = Search for events from the specified source

[savedsplunk-specifier]
syntax = (savedsearch|savedsplunk)=<string>
description = Search for events that would be found by specified search/splunk

[eventtype-specifier]
syntax = eventtype=<string>
description = Search for events that match the specified eventtype

[eventtypetag-specifier]
syntax = eventtypetag=<string>
description = Search for events that would match all eventtypes tagged by the string

[hosttag-specifier]
syntax = hosttag=<string>
description = Search for events that have hosts that are tagged by the string

[tag-specifier]
syntax = tag=<field>::<string>
description = Search for all events that have their specified field tagged by string
usage = internal

[starttime]
syntax = starttime=<string>
description = Events must be later or equal to this time. Must match time format.

[startdaysago]
syntax = startdaysago=<int>
description = A short cut to set the start time. starttime = now - (N days)

[startminutesago]
syntax = startminutesago=<int>
description = A short cut to set the start time. starttime = now - (N minutes)

[starthoursago]
syntax = starthoursago=<int>
description = A short cut to set the start time. starttime = now - (N hours)

[startmonthsago]
syntax = startmonthsago=<int>
description = A short cut to set the start time. starttime = now - (N months)

[starttimeu]
syntax = starttimeu=<int>
description = Set the start time to N seconds since the epoch. ( unix time )

[endtime]
syntax = endtime=<string>
description = All events must be earlier or equal to this time.

[enddaysago]
syntax = enddaysago=<int>
description = A short cut to set the end time. endtime = now - (N days)

[endminutesago]
syntax = endminutesago=<int>
description = A short cut to set the end time. endtime = now - (N minutes)

[endhoursago]
syntax = endhoursago=<int>
description = A short cut to set the end time. endtime = now - (N hours)

[endmonthsago]
syntax = endmonthsago=<int>
description = A short cut to set the start time. starttime = now - (N months)

[endtimeu]
syntax = endtime=<int>
description = Set the end time to N seconds since the epoch. ( unix time )

[searchtimespanhours]
syntax = searchtimespanhours=<int>
description = The time span operators are always applied from the last time boundary set.  Therefore, if an endtime operator is closest to the left of a timespan operator, it will be applied to the starttime. If you had 'enddaysago::1 searchtimespanhours::5', it would be equivalent to 'starthoursago::29 enddaysago::1'.

[searchtimespanminutes]
syntax = searchtimespanminutes=<int>

[searchtimespandays]
syntax = searchtimespandays=<int>

[searchtimespanmonths]
syntax = searchtimespanmonths=<int>

[daysago]
syntax = daysago=<int>
description = Search the last N days. ( equivalent to startdaysago )

[minutesago]
syntax = minutesago=<int>
description = Search the last N minutes. ( equivalent to startminutesago )

[hoursago]
syntax = hoursago=<int>
description = Search the last N hours. ( equivalent to starthoursago )

[monthsago]
syntax = monthsago=<int>
description = Search the last N months. ( equivalent to startmonthsago )



##################
# set
##################
[set-command]
syntax = set (union|diff|intersect) <subsearch> <subsearch>
shortdesc = Performs set operations on subsearches.
description = Performs two subsearches and then executes the specified set operation on the two sets of search results.
maintainer = david
usage = public
comment1 = Return all urls that have 404 errors and 303 errors.
example1 = | set intersect [search 404 | fields url] [search 303 | fields url]
commentcheat = Return values of "URL" that contain the string "404" or "303" but not both.
examplecheat = | set diff [search 404 | fields url] [search 303 | fields url]
category = search::subsearch
generating = true
related = append, appendcols, join, diff
tags = diff union join intersect append

[subsearch]
syntax = [<string>]
description = Specifies a subsearch.
example1 = [search 404 | select url]
tags = set union diff intersect

#################
# cluster
#################

[cluster-command]
syntax = cluster (<slc-option> )*
alias = slc
shortdesc = Clusters similar events together.
description = Fast and simple clustering method designed to operate on event text (_raw field).  With default options, a single representative event is retained for each cluster.
usage = public
maintainer = steveyz
comment = Cluster syslog events together.
example = sourcetype=syslog | cluster
commentcheat = Cluster events together, sort them by their "cluster_count" values, and then return the 20 largest clusters (in data size).
examplecheat = ... | cluster t=0.9 showcount=true | sort - cluster_count | head 20
category = results::group
appears-in = 3.2
related = anomalies, anomalousvalue, cluster, kmeans, outlier
tags = cluster group collect gather

[slc-option]
syntax = (t=<num>|(delims=<string>)|(showcount=<bool>)|(countfield=<field>)|(labelfield=<field>)|(field=<field>)|(labelonly=<bool>)|(match=(termlist|termset|ngramset)))
description = 	Options for configuring the simple log clusters.  \
	       	"T=" sets the threshold which must be > 0.0 and < 1.0.  The closer the threshold is to 1, the more similar events have to be in order to be considered in the same cluster.  Default is 0.8 \
	       	"delims" configures the set of delimiters used to tokenize the raw string.  By default everything except 0-9, A-Z, a-z, and '_' are delimiters. \
	      	"showcount" if yes, this shows the size of each cluster (default = false) \
	        "countfield" name of field to write cluster size to if showcount=true , default = "cluster_count" \
		"labelfield" name of field to write cluster number to, default = "cluster_label" \
		"field" name of field to analyze, default = _raw \
		"labelonly" if true, instead of reducing each cluster to a single event, keeps all original events and merely labels with them their cluster number\
                "match" determines the similarity method used, defaulting to termlist.  termlist requires the exact \
		   same ordering of terms, termset allows for an unordered set of terms, and ngramset compares sets of \
                   trigram (3-character substrings).  ngramset is significantly slower on large field values and is most useful for short non-textual fields, like 'punct'
example1 = t=0.9 delims=" ;:" showcount=true countfield="SLCCNT" labelfield="LABEL" field=_raw labelonly=true


##################
# shape
##################

[shape-command]
syntax = shape <field> (maxvalues=<int>)? (maxresolution=<int>)?
shortdesc = Produces a symbolic 'shape' attribute describing the shape of a numeric multivalued field
description = Given a numeric multivalued FIELD, produce a 'shape' \
 attribute, describing the shape of the values, in a symbolic \
 representation.  The symbolic representation will be at most MAXVALUES \
 long and have at most MAXRESOLUTION different characters.  The \
 defaults are MAXVALUES = 5 and MAXRESOLUTION = 10, normally producing \
 a SHAPE value of 5 characters made up of 10 letters (a-k).
usage = public
maintainer = david
comment1 = group each dest_ip into a separate transaction, keeping a list of all their delay values, and then calculating the shape of that dest_ip's delays. dest_ips can be subsequently be clustered by the shape of their delays.
example1 = ... | fields dest_ip, delay | transaction dest_ip mvlist=true | shape delay
appears-in = 4.0
category = reporting
tags = summary symbolic
related = anomalousvalue, cluster, kmeans, outlier


##################
# showargs
##################

[showargs-command]
syntax = showargs [<string>]
description = Treats the given string as a subsearch, executes that subsearch \
	      and renders the results as an event. This is useful for debugging \
	      subsearches.
usage = internal
maintainer = steveyz
example1 = ... | showargs [search * | top source | fields source | format]
appears-in = 3.0
generating = true

##################
# sort
##################

[sort-command]
syntax = sort (<int:count>)? <sort-by-clause>+ (d|desc)?
simplesyntax = sort (<int:count>)? <sort-by-clause>+ desc?
shortdesc = Sorts search results by the specified fields.
description = Sorts by the given list of fields. If more than one field is specified, \
	      the first denotes the primary sort order, the second denotes the secondary, etc. \
	      If the fieldname is immediately (no space) preceded by "+", the sort is ascending (default). \
	      If the fieldname is immediately (no space) preceded by "-", the sort is descending. \
	      If white space follows "+/-", the sort order is applied to all following fields without a different explicit sort order. \
	      Also a trailing "d" or "desc" causes the results to be reversed. \
	      Results missing a given field are treated as having the smallest or largest \
	      possible value of that field if the order es descending or ascending respectively. \
	      If the field takes on numeric values, the collating sequence is numeric. \
	      If the field takes on IP address values, the collating sequence is for IPs. \
	      Otherwise, the collating sequence is lexicographic ordering. \
	      If the first term is a number, then at most that many results are returned (in order).  \
	      If no number is specified, the default limit of 10000 is used.  If number is 0, all results will be returned.
example1 = ... | sort _time, -host
comment1 = Sort results by the "_time" field in ascending order and then by the "host" value in descending order.
example2 = ... | sort 100 -size, +source
comment2 = Sort first 100 results in descending order of the "size" field and then by the "source" value in ascending order.
commentcheat = Sort results by "ip" value in ascending order and then by "url" value in descending order.
examplecheat = ... | sort ip, -url
category = results::order
maintainer = steveyz
appears-in = 3.0
usage = public
related = reverse
tags = arrange, order, rank, sort

[sort-by-clause]
syntax = ("-"|"+")?( )?<sort-field> (",")?
simplesyntax = ("-"|"+")<sort-field> ","
description = List of fields to sort by and their sort order (ascending or descending)
example1 = _time, -host
example2 = - time, host
example3 = -size, +source

[sort-field]
syntax = <field> | ((auto|str|ip|num) "(" <field> ")")
description = a sort field may be a field or a sort-type and field.  sort-type can be "ip" to interpret \
	      the field's values as ip addresses.  "num" to treat them as numbers, "str" to order lexigraphically, \
	      and "auto" to make the determination automatically.  If no type is specified, it is assumed to be "auto"
example1 = auto(size)
example2 = ip(source_addr)
example3 = str(pid)
example4 = host
example5 = _time


##################
# collect
##################

[collect-command]
syntax      = collect <collect-index> (<collect-arg>)*
alias       = stash, summaryindex, sumindex
shortdesc = Puts search results into a summary index.
description = Adds the results of the search into the specified index. Behind the scenes, the events are written \
              to a file whose name format is: "<random-num>_events.stash", unless overridden, in a directory \
              which is watched for new events by Splunk. If the events contain a _raw field then the raw field \
              is saved, if they don't a _raw field is constructed by concatenating all the fields into a \
              comma-separated list of key="value" pairs.
usage       = public
appears-in  = 3.2
maintainer  = ledion
comment1 = Put "download" events into an index named "downloadcount".
example1    = eventtypetag="download" | collect index=downloadcount
related = overlap, sichart, sirare, sistats, sitop, sitimechart
tags = collect summary overlap summary index summaryindex
category = index::summary
optout-in = lite, lite_free

[collect-arg]
syntax = <collect-addtime> | <collect-index> | <collect-file> | <collect-spool> | <collect-marker> | <collect-testmode> | <collect-run-in-preview>

[collect-addtime]
syntax      = addtime=<bool>
description = whether to prefix a time into each event if the event does not contain a _raw field. \
              The first found field of the following times is used: info_min_time, _time, now() \
              defaults to true

[collect-addinfo]
syntax	    = addinfo=<bool>
description = If true, will write the search time and time bounds into the text \
	      out of each summary index event in the format \
	      info_min_time=<search_earliest_time>, info_max_time=<search_latest_time>, \
	      info_search_time=<search_exec_time>

[collect-index]
syntax      = index=<string>
description = name of the index where splunk should add the events to. Note: the index must exist \
              for events to be added to it, the index is NOT created automatically.

[collect-file]
syntax      = file=<string>
description = name of the file where to write the events to. Optional, default "<random-num>_events.stash" \
              The following placeholders can be used in the file name $timestamp$, $random$ and will be \
              replaced with a timestamp, a random number respectively

[collect-spool]
syntax      = spool=<bool>
description = If set to true (default is true), the summary indexing file will be written to \
	      Splunk's spool directory, where it will be indexed automatically. \
	      If set to false, file will be written to $SPLUNK_HOME/var/run/splunk, where it will remain \
	      until further administrative actions are taken.

[collect-marker]
syntax      = marker=<string>
description = a string, usually of key-value pairs, to append to each event written out. Optional, default ""

[collect-testmode]
syntax      = testmode=<bool>
description = toggle between testing and real mode. In testing mode the results are not written  \
              into the new index  but the search results are modified to appear as they would if \
              sent to the index. (defaults to false)

[collect-run-in-preview]
syntax      = run_in_preview=<bool>
description = controls whether this command is enabled during preview generation. Generally you do not \
              want to insert preview results into the summary index - that is why this defaults to false. \
              However, in some rare cases, such as when a custom search command is used as part of the search \
              to ensure correct summary-indexable previews are generated, this flag can be turned on. \
              (defaults to false)

##################
# streamstats
##################
[streamstats-command]
syntax = streamstats (current=<bool>)? (window=<int>)? (global=<bool>)? (allnum=<bool>)? (<stats-agg-term>)* (<by-clause>)?
shortdesc = Adds summary statistics to all search results in a streaming manner.
description = Similar to the 'eventstats' command except that only events seen before a given event (plus that event itself if current=t, which it is by default) are used to compute the aggregate statistics applied to each event.  The 'window' option specify window size to be used in computing the statistics, and if 0 (by default), means that all previous (plus current) events are used.  If the 'global' option is set to false (it is true by default) and 'window' is set to a non-zero value, a seperate window is used for each group of values of the group by fields.  The 'allnum' option has the same affect as for the stats and eventstats commands.
example1 = ... | streamstats count
comment1 = For each event, add a count field that represent the number of event seen so far (including that event).  i.e., 1 for the first event, 2 for the second, 3, 4 ... and so on
example2 = ... | streamstats avg(foo) window=5
comment2 = For each event, compute the average of field foo over the last 5 events (including the current event).  Similar to doing trendline sma5(foo)
example3 = ... | streamstats count current=f
comment3 = Same as example1, except that the current event is not included in the count
example4 = ... | streamstats avg(foo) by bar window=5 global=f
comment4 = Compute the average value of foo for each value of bar including only the only 5 events with that value of bar.
usage       = public
appears-in  = 4.0
maintainer  = steveyz
related = accum, autoregress, delta, eventstats, stats, streamstats, trendline
tags = stats statistics event
category = reporting


##################
# eventstats
##################
[eventstats-command]
syntax = eventstats (allnum=<bool>)? (<stats-agg-term>)* (<by-clause>)?
shortdesc = Adds summary statistics to all search results.
description = Generate summary statistics of all existing fields in your search results and save them as values in new fields. Specify a new field name for the statistics results by using the as argument. If you don't specify a new field name, the default field name is the statistical operator and the field it operated on (for example: stat-operator(field)). Just like the 'stats' command except that aggregation results are added inline to each event, and only the aggregations that are pertinent to that event.  The 'allnum' option has the same meaning as that option in the stats command.  See stats-command for detailed descriptions of syntax.
example1 = ... | eventstats avg(duration) as avgdur
comment1 = Compute the overall average duration and add 'avgdur' as a new field to each event where the 'duration' field exists
example2 = ... | eventstats avg(duration) as avgdur by date_hour
comment2 = Same as example1 except that averages are calculated for each distinct value of date_hour and the aggregate value that is added to each event is the aggregate that perhaps to the value of date_hour in that event.
usage       = public
appears-in  = 3.2
maintainer  = steveyz
related = stats
tags = stats statistics event
category = reporting

##################
# stats
##################

[stats-command]
simplesyntax = stats (((c|count|dc|distinct_count|estdc|estdc_error|earliest|latest|avg|stdev|stdevp|var|varp|sum|min|max|mode|median|first|last|earliest|latest|percint|list|values|range) "(" <field> ")") (as <field>)? )+ (by <field-list>)?
syntax =  stats (partitions=<num>)? (allnum=<bool>)? (delim=<string>)? (<stats-agg-term> | <sparkline-agg-term>)* (<by-clause>)?
shortdesc = Provides statistics, grouped optionally by field.
description = Calculate aggregate statistics over the dataset. optionally grouped by a list of fields.\
	      Aggregate statistics include: \i\\
	         * count, distinct count \i\\
	         * mean, median, mode \i\\
	         * min, max, range, percentiles \i\\
	         * standard deviation, variance \i\\
	         * sum \i\\
	         * earliest and latest occurrence \i\\
	         * first and last (according to input order into stats command) occurrence \p\\
	      Similar to SQL aggregation. \
	      If called without a by-clause, one row is produced, which represents the \
	      aggregation over the entire incoming result set. If called with a \
	      by-clause, one row is produced for each distinct value of the by-clause. \
	      The 'partitions' option, if specified, allows stats to partition the \
	      input data based on the split-by fields for multithreaded reduce. \
	      The 'allnum' option, if true (default = false), computes numerical statistics on each \
	      field if and only if all of the values of that field are numerical. \
              The 'delim' option is used to specify how the values in the 'list' or 'values' aggregation are delimited.  (default is a single space)\
	      When called with the name "prestats", it will produce intermediate results (internal).
note =  When called without any arguments, stats assumes the argument "default(*)".\
     	This produces a table with the cross-product of aggregator and field as columns,\
	And a single row with the value of that aggregator applied to that field across all data.
example1 = sourcetype=access* | stats avg(kbps) by host
example2 = sourcetype=access* | top limit=100 referer_domain | stats sum(count)
commentcheat1 = Remove duplicates of results with the same "host" value and return the total count of the remaining results.
examplecheat1 = ... | stats distinct_count(host)
commentcheat2 = Return the average for each hour, of any unique field that ends with the string "lay" (for example, delay, xdelay, relay, etc).
examplecheat2 = ... | stats avg(*lay) BY date_hour
commentcheat3 = Search the access logs, and return the number of hits from the top 100 values of "referer_domain".
examplecheat3 = sourcetype=access_combined | top limit=100 referer_domain | stats sum(count)
category = reporting
appears-in = 3.0
usage = public
maintainer = steveyz
supports-multivalue = true
related = eventstats, rare, sistats, streamstats, top
tags = stats statistics event sparkline count dc mean avg stdev var min max mode median

[sparkline-agg-term]
syntax = <sparkline-agg> (as <wc-field>)?
description = A sparkline specifier optionally renamed to a new field name.
example1 = sparkline(count(user))
example2 = sparkline(dc(device)) AS numdevices

[sparkline-agg]
syntax = sparkline "(" count ("," <span-length> )? ")" | sparkline "(" <sparkline-func> "(" <wc-field> ")" ( "," <span-length> )? ")"
description = A sparkline specifier, which takes the first argument of a aggregation function on a field, \
	      optionally followed by a timespan specifier.  If no timespan specifier is used, an appropriate \
	      timespan is chosen based on the time range of the search.  If the sparkline is not scoped to a field, \
	      only the count aggregator is permitted.
example1 = sparkline(count)
example2 = sparkline(count(source))
example3 = sparkline(dc(source)) by sourcetype
example4 = sparkline(dc(source),5m) by sourcetype

[sparkline-func]
syntax = c|count|dc|mean|avg|stdev|stdevp|var|varp|sum|sumsq|min|max|range
description = Aggregation function to use to generate sparkline values.  Each sparkline value is produced by applying \
	      this aggregation to the events that fall into each particular time bucket

[stats-agg-term]
syntax = <stats-agg> (as <wc-field>)?
description = A statistical specifier optionally renamed to a new field name.
example1 = avg(kbps)
example2 = count(device) AS numdevices

[stats-agg]
syntax = <stats-func>( "(" ( <evaled-field> | <wc-field> )? ")" )?
description = A specifier formed by a aggregation function applied to a field or set of fields.  \
	      As of 4.0, it can also be an aggregation function applied to a arbitrary eval expression.  \
	      The eval expression must be wrapped by "eval(" and ")".  \
	      If no field is specified in the parenthesis, \
	      the aggregation is applied independently to all fields, \
	      and is equivalent to calling a field value of *\
	      When a numeric aggregator is applied to a not-completely-numeric field \
	      no column is generated for that aggregation.
example1 = avg(kbps)
example2 = max(size)
comment3 = applies to both delay and xdelay
example3 = stdev(*delay)
example4 = count(eval(sourcetype="splunkd"))
comment4 = count of events where sourcetype has the value "splunkd".  The { must immediately follow the (

[evaled-field]
syntax = "eval("<eval-expression>")"
description = A dynamically evaled field

[stats-func]
syntax = <stats-c>|<stats-dc>|<stats-mean>|<stats-stdev>|<stats-var>|<stats-sum>|<stats-sumsq>|<stats-min>|<stats-max>|<stats-mode>|<stats-median>|<stats-first>|<stats-last>|<stats-perc>|<stats-list>|<stats-values>|<stats-range>|<stats-estdc>|<stats-estdc-error>
description = Statistical aggregators.

[stats-estdc]
syntax = estdc
description = The estimated count of the distinct values of the field.

[stats-estdc-error]
syntax = estdc_error
description = The theoretical error of the estimated count of the distinct values of the field, where the error represents a ratio of abs(estimate_value - real_value)/real_value

[stats-c]
syntax = c|count
simplesyntax = count
description = The count of the occurrences of the field.

[stats-dc]
syntax = dc|distinct-count
simplesyntax = distinct-count
description = The count of distinct values of the field.

[stats-mean]
syntax = mean|avg
simplesyntax = avg
description = The arithmetic mean of the field.

[stats-stdev]
syntax = stdev|stdevp
description = The {sample, population} standard deviation of the field.

[stats-var]
syntax = var|varp
description = The {sample, population} variance of the field.

[stats-sum]
syntax = sum
description = The sum of the values of the field.

[stats-sumsq]
syntax = sumsq
description = The sum of the square of the values of the field.

[stats-min]
syntax = min
description = The minimum value of the field (lexicographic, if non-numeric).

[stats-max]
syntax = max
description = The maximum value of the field (lexicographic, if non-numeric).

[stats-range]
syntax = range
description = The difference between max and min (only if numeric)

[stats-mode]
syntax = mode
description = The most frequent value of the field.

[stats-median]
syntax = median
description = The middle-most value of the field.

[stats-first]
syntax = first
description = The first seen value of the field.
note = In general the first seen value of the field is \
       the chronologically most recent instance of this field.

[stats-last]
syntax = last
description = The last seen value of the field.

[stats-perc]
syntax = (perc|p|exactperc|upperperc)<int>
simplesyntax = perc<int>
description = The n-th percentile value of this field.  perc<int>, p<int>, and upperperc<int> give approximate values for the integer percentile requested.  The approximation algorithm we use provides a strict bound of the actual value at for any percentile.  perc<int> and p<int> return a single number that represents the lower end of that range while upperperc<int> gives the approximate upper bound.  exactperc<int> provides the exact value, but will be very expensive for high cardinality fields.

[stats-list]
syntax = list
description = List of all values of this field as a multi-value entry.  Order of values reflects order of input events.

[stats-values]
syntax = values
description = List of all distinct values of this field as a multi-value entry.  Order of values is lexigraphical.

[by-clause]
syntax = by <field-list>
description = Fields to group by.
example1 = BY host
example2 = BY addr, port

##################
# strcat
##################
[strcat-command]
syntax = strcat (allrequired=<bool>)? <srcfields> <field>
shortdesc = Concatenates string values.
description = Stitch together fields and/or strings to create a new field.  \
 Quoted tokens are assumed to be literals and the rest field names.  \
 The destination field name is always at the end. \
 If allrequired=t, for each event the destination field is only \
 written to if all source fields exist.  If allrequired=f (default) \
 the destination field is always written and any source fields \
 that do not exist are treated as empty string.
comment1 = Add a field, address, which combines the host and port values into the format <host>::<port>.
example1 = ... | strcat host "::" port address
comment2 = Add the field, comboIP, and then create a chart of the number of occurrences of the field values.
example2 = host="mailserver" | strcat sourceIP "/" destIP comboIP | chart count by comboIP
commentcheat = Add the field, comboIP, which combines the source and destination IP addresses and separates them with a front slash character.
examplecheat =  ... | strcat sourceIP "/" destIP comboIP
category = fields::add
usage = public
maintainer = steveyz
appears-in = 3.2
tags = strcat concat string append
related = eval

[srcfields]
syntax = (<field>|<quoted-str>) (<field>|<quoted-str>) (<field>|<quoted-str> )*
description = Fields should either be key names or quoted literals

[quoted-str]
syntax = "" <string> ""

##################
# streamedcsv
##################
[streamedcsv-command]
syntax = streamedcsv (chunk=<int>)? <string>
description = Internal command to test dispatch.
example1 = | streamedcsv
usage = internal
maintainer = ssorkin
appears-in = 3.2

#########################
# summary indexing stats
#########################
[sistats-command]
syntax = sistats <stats_syntax>
shortdesc = Summary indexing friendly versions of stats command.
description = Summary indexing friendly versions of stats command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.
example1 = ... | sistats avg(foo) by bar
comment1 = Compute the necessary information to later do 'stats avg(foo) by bar' on summary indexed results
usage = public
maintainer = steveyz
appears-in = 4.0
tags = stats summary index summaryindex
related = collect, overlap, sichart, sirare, sitop, sitimechart,
category = index::summary
optout-in = lite, lite_free

#########################
# summary indexing top
#########################
[sitop-command]
syntax = sitop <top_syntax>
shortdesc = Summary indexing friendly versions of top command.
description = Summary indexing friendly versions of top command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.
example1 = ... | sitop foo bar
comment1 = Compute the necessary information to later do 'top foo bar' on summary indexed results.
usage = public
maintainer = steveyz
appears-in = 4.0
tags = top summary index summaryindex
related = collect, overlap, sichart, sirare, sistats, sitimechart
category = index::summary
optout-in = lite, lite_free

#########################
# summary indexing rare
#########################
[sirare-command]
syntax = sirare <rare_syntax>
shortdesc = Summary indexing friendly versions of rare command.
description = Summary indexing friendly versions of rare command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.
example1 = ... | sirare foo bar
comment1 = Compute the necessary information to later do 'rare foo bar' on summary indexed results.
usage = public
maintainer = steveyz
appears-in = 4.0
tags = rare summary index summaryindex
related = collect, overlap, sichart, sistats, sitimechart, sitop
category = index::summary
optout-in = lite, lite_free

#########################
# summary indexing chart
#########################
[sichart-command]
syntax = sichart <chart_syntax>
shortdesc = Summary indexing friendly versions of chart command.
description = Summary indexing friendly versions of chart command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.
example1 = ... | sichart avg(foo) by bar
comment1 = Compute the necessary information to later do 'chart avg(foo) by bar' on summary indexed results.
usage = public
maintainer = steveyz
appears-in = 4.0
tags = chart summary index summaryindex
related = collect, overlap, sirare, sistats, sitimechart, sitop
category = index::summary
optout-in = lite, lite_free

#############################
# summary indexing timechart
#############################
[sitimechart-command]
syntax = sitimechart <timechart_syntax>
shortdesc = Summary indexing friendly versions of timechart command.
description = Summary indexing friendly versions of timechart command, using the same syntax.  Does not require explicitly knowing what statistics are necessary to store to the summary index in order to generate a report.
example1 = ... | sitimechart avg(foo) by bar
comment1 = Compute the necessary information to later do 'timechart avg(foo) by bar' on summary indexed results.
usage = public
maintainer = steveyz
appears-in = 4.0
tags = timechart summary index summaryindex
related = collect, overlap, sichart, sirare, sistats, sitop
category = index::summary
optout-in = lite, lite_free

##################
# tags
##################
[tags-command]
syntax = tags (outputfield=<field>)? (inclname=<bool>)? (inclvalue=<bool>)? (<field> )*
shortdesc = Annotates specified fields in your search results with tags.
description = Annotate the search results with tags. If there are fields specified only annotate tags for those fields otherwise look for tags for all fields.  If outputfield is specifid, the tags for all fields will be written to this field.  Otherwise, the tags for each field will be written to a field named tag::<field>.  If outputfield is specified, inclname and inclvalue control whether or not the field name and field values are added to the output field.  By default only the tag itself is written to the outputfield.  E.g.: (<field>::)?(<value>::)?tag
example1 = ... | tags host eventtype
comment1 = write tags for host and eventtype fields into tag::host and tag::eventtype
example2 = ... | tags outputfield=test
comment2 = write new field test that contains tags for all fields
example3 = ... | tags outputfield=test inclname=t host sourcetype
comment3 = write tags for host and sourcetype into field test in the format host::<tag> or sourcetype::<tag>
usage = public
maintainer = steveyz
appears-in = 4.2
tags = tags
related = eval
category = fields::add

##################
# tagcreate
##################
[tagcreate-command]
syntax = tagcreate <tag> <field-and-value-list>
description = Sets the tag on each fielded value.
example1 = ... | tagcreate webserver host::web
usage = internal
maintainer = brian
appears-in = 3.0

##################
# tagdelete
##################
[tagdelete-command]
syntax = tagdelete <tag> <field-and-value-list>
description = Deletes the tag from each fielded value if they were tagged with tag.
example1 = ... | tagdelete webserver host::web
usage = internal
maintainer = brian
appears-in = 3.0

##################
# tagset
##################
[tagset-command]
syntax = tagset <field-and-value> <tag-list>
description = Sets the tags for the fielded value to be the tag list. Other tags are deleted.
example1 = ... | tagset host::web webserver
usage = internal
maintainer = brian
appears-in = 3.0

##################
# trendline
##################
[trendline-command]
syntax = trendline (<trend_type>"("<field>")" (as <field>)?)+
shortdesc = Computes the moving averages of fields.
description = Computes the moving averages of fields.  Current supported trend_types include \
	      simple moving average (sma), exponential moving average(ema), and weighted moving average(wma)\
	      The output is written to a new field where the new field name can be explicitly specified or\
	      by default it is simply the trend_type + field.
example1 = ... | trendline sma5(foo) as smoothed_foo ema10(bar)
comment1 = Computes a 5 event simple moving average for field 'foo' and write to new field 'smoothed_foo'\
	   also computes N=10 exponential moving average for field 'bar' and write to field 'ema10(bar)'.
usage = public
maintainer = steveyz
appears-in = 4.0
category = reporting
related = accum, autoregress, delta, streamstats, trendline
tags = average mean

[trend_type]
syntax = (sma|ema|wma)<num>
description = The type of trend to compute which consist of a trend type and trend period (integer between 2 and 10000)
example1 = sma10


##################
# timechart
##################

[timechart-command]
syntax = timechart (sep=<string>)? (format=<string>)? (partial=<bool>)? (cont=<bool>)? (limit=<int>)? (agg=<stats-agg-term>)? (<bucketing-option> )* ( <single-agg> | ( "(" <eval-expression> ")" ) )+ by <split-by-clause>
shortdesc = Creates a time series chart with corresponding table of statistics.
description = Creates a chart for a statistical aggregation applied to a field against time. When \
              the data is split by a field, each distinct value of this split-by field is a series. \
              If used with an eval-expression, the split-by-clause is required. \p\\
              When a where clause is not provided, you can use limit and agg options to specify \
              series filtering. If limit=0, there is no series filtering. \p\\
              When specifying multiple data series with a split-by-clause, you can use sep and \
              format options to construct output field names.\p\\
              When called without any bucketing-option, timechart defaults to bins=300. This finds \
              the smallest bucket size that results in no more than three hundred distinct buckets.

note = When called without any bucketing-option, TIMECHART assumes bins=300 has been specified. This finds the smallest bucket size that results in no more than 300 distinct buckets.
example1 = ... | timechart span=5m avg(delay) by host
example2 = sourcetype=access_combined | timechart span=1m count(_raw) by product_id usenull=f
example3 = sshd failed OR failure | timechart span=1m count(eventtype) by source_ip usenull=f where count>10
commentcheat1 = Graph the average "thruput" of hosts over time.
examplecheat1 = ... | timechart span=5m avg(thruput) by host
commentcheat2 = Create a timechart of average "cpu_seconds" by "host", and remove data (outlying values) that may distort the timechart's axis.
examplecheat2 = ... | timechart avg(cpu_seconds) by host | outlier action=tf
commentcheat3 = Calculate the average value of "CPU" each minute for each "host".
examplecheat3 = ... | timechart span=1m avg(CPU) by host
commentcheat4 = Create a timechart of the count of from "web" sources by "host"
examplecheat4 = ... | timechart count by host
commentcheat5 = Compute the product of the average "CPU" and average "MEM" each minute for each "host"
examplecheat5 = ... | timechart span=1m eval(avg(CPU) * avg(MEM)) by host
maintainer = sorkin
category = reporting
appears-in = 3.0
usage = public
supports-multivalue = true
related = bucket, chart, sitimechart
tags = chart graph report count dc mean avg stdev var min max mode median per_second per_minute per_hour per_day

[sep]
syntax = sep=<string>
description = Specify the seperater to use for output field names when multiple data series are \
              used along with a split-by field.

[format]
syntax = format=<string>
description = Specify a parameterized expression with $AGG$ and $VAL$ to construct the output \
              field names when multiple data series are used along with a split-by field. Replaces \
              $AGG$ with the stats aggregator and function and $VAL$ with the value of the \
              split-by-field. Takes precedence over sep.

[partial]
syntax = partial=<bool>
description = Controls if partial time buckets should be retained (true) or not (false). \
              Only the first and last bucket can be partial. Defaults to true.

[single-agg]
syntax = count|c|<stats-func>(<field>|<evaled-field>)
simplesyntax = count|<stats-func>(<field>)
description = A single aggregation applied to a single field (can be evaled field).  No wildcards are allowed.  \
	      The field must be specified, except when using the special 'count' aggregator that applies to events as a whole.
note = C and COUNT are explicitly specified because they may be called without a field. \
       In this mode, the result is the count of results overall, or in the by-clause if specified.
example1 = count
example2 = avg(delay)
example3 = sum(eval(date_hour * date_minute))

[timechart-single-agg]
syntax = count|c|<stats-func>|per_second|per_minute|per_hour|per_day(<field>|<evaled-field>)
description = Same as single-agg excep that additional per_* functions are allowed for computing rates over time
example1 = per_second(drop_count)

[split-by-clause]
syntax = <field> (<tc-option> )* (<where-clause>)?
description = Specifies a field to split by.  If field is numerical, default discretization is applied.
note = Discretization is specified by the tc-option.

[tc-option]
syntax = <bucketing-option>|(usenull=<bool>)|(useother=<bool>)|(nullstr=<string>)|(otherstr=<string>)
description = Options for controlling the behavior of splitting by a field.  \
	      In addition to the bucketing-option: \
	      usenull controls whether or not a series is \
	      created for events that do not contain the split-by field. \
	      This series is labeled by the value of the nullstr option, and defaults to NULL. \
    	      useother specifies if a series should be added for data series not included in the graph \
	      because they did not meet the criteria of the <where-clause>. \
	      This series is labeled by the value of the otherstr option, and defaults to OTHER.
example1 = bins=10
example2 = usenull=f
example3 = otherstr=OTHERFIELDS

[where-clause]
syntax = where <single-agg> <where-comp>
description = Specifies the criteria for including particular data series when a field is given in the tc-by-clause. \
	      This optional clause, if omitted, default to "where sum in top10". \
	      The aggregation term is applied to each data series and the result of \
	      these aggregations is compared to the criteria. \
	      The most common use of this option is to select for spikes rather than overall \
	      mass of distribution in series selection. The default value finds the \
	      top ten series by area under the curve. Alternately one could replace sum with \
	      max to find the series with the ten highest spikes.
note = This has no relation to the where-command or SQLite.
example1 = where sum in top5
example2 = where count notin bottom10
example3 = where avg > 100
example4 = where max < 10

[where-comp]
syntax = <wherein-comp>|<wherethresh-comp>
description = A criteria for the where clause.

[wherein-comp]
syntax = (in|notin) (top|bottom)<int>
description = A where-clause criteria that requires the aggregated series value be in or not in some top or bottom grouping.
example1 = in top5
example2 = in bottom10
example3 = notin top2

[wherethresh-comp]
syntax = ((<|>)<num>)|((<|>) <num>)
description = A where-clause criteria that requires the aggregated series value be greater than or less than some numeric threshold.
example1 = > 2.5
example2 = < 100

##################
# top
##################

[top-command]
syntax = top (<top-opt> )* <field-list> (<by-clause>)?
alias = common
shortdesc = Displays the most common values of a field.
description = Finds the most frequent tuple of values of all fields in the field list, along with a count and percentage.\
	    If a the optional by-clause is provided, we will find the most frequent values\
	    for each distinct tuple of values of the group-by fields.
comment1 = Return top URL values.
example1 = ... | top url
comment2 = Return top "user" values for each "host".
example2 = ... | top user by host
commentcheat = Return the 20 most common values of the "url" field.
examplecheat = ... | top limit=20 url
category = reporting
maintainer = steveyz
appears-in = 3.0
usage = public
supports-multivalue = true
related = rare, sitop, stats
tags = top popular common many frequent typical

[top-opt]
syntax = (showcount=<bool>)|(showperc=<bool>)|(limit=<int>)|(countfield=<string>)|(percentfield=<string>)|(useother=<bool>)|(otherstr=<string>)
description = Top arguments:\
	      showcount: Whether to create a field called "count" (see countfield option) with the count of that tuple. (T) \
	      showperc: Whether to create a field called "percent" (see percentfield option) with the relative prevalence of that tuple. (T) \
	      limit: Specifies how many tuples to return, 0 returns all values. (10) \
	      countfield: Name of new field to write count to (default is "count") \
	      percentfield: Name of new field to write percentage to (default is "percent") \
	      useother: If true, adds a row, if necessary, to represent all values not included \
	                due to the limit cutoff. (default is false) \
	      otherstr: If useother is true, the value that is written into the row representing \
	                all other values (default is "OTHER")

##################
# tscollect
##################
[tscollect-command]
syntax = tscollect (namespace=<string>)? (squashcase=<bool>)? (keepresults=<bool>)?
shortdesc = Writes the result table into *.tsidx files using indexed fields format.
description = Writes the result table into *.tsidx files, for later use by tstats command. \
              Only non-internal fields and values are written to the tsidx files. \
              squashcase is false by default; if true, the field *values* are converted \
              to lowercase when writing them to the *.tsidx files. If namespace is provided, \
              the tsidx files are written to a directory of that name under the main tsidx  \
              stats directory. These namespaces can be written to multiple times to add new \
              data. If namespace is not provided, the files are written to a directory within \
              the job directory of that search, and will live as long as the job does. \
              If keepresults is set to true, tscollect will output the same results it received \
              as input. By default this is false, and only emits a count of results processed (this \
              is more efficient as we do not need to store as many results). \
              The 'indexes_edit' capability is required to run this command.
related = tstats
maintainer = dmarquardt
tags = tscollect tsidx projection
usage = internal
category = reporting
example1 = ... | tscollect namespace=foo
comment1 = Write the results table to tsidx files in namespace foo.
example2 = index=main | fields foo | tscollect
comment2 = Write the values of field foo for the events in the main index to tsidx files in the job directory.
appears-in = 5.0
optout-in = lite, lite_free

##################
# tstats
##################
[tstats-command]
syntax = tstats (prestats=<bool>)? (local=<bool>)? (append=<bool>)? (summariesonly=<bool>)? (allow_old_summaries=<bool>)? (chunk_size=<unsigned int>)? (<stats-func>)+ (FROM (<string:namespace>|sid=<string:tscollect-job-id>|datamodel=<string:datamodel-name>))? (WHERE <string:search-query>)? ((by|GROUPBY) <field-list> (span=<string:timespan>)? )?
shortdesc = Performs statistics on indexed fields in tsidx files, which could come from normal index data, tscollect data, or accelerated datamodels.
description = Performs statistical queries on indexed fields in tsidx files. You can select from TSIDX data in several different ways:                                  \p\\
              1. Normal index data: If you do not supply a FROM clause, we will select from index data in the same way as search. You are restricted to selecting       \
                 from your allowed indexes by role, and you can control exactly which indexes you select from in the WHERE clause. If no indexes are mentioned          \
                 in the WHERE clause search, we will use your default set of indexes. By default, role-based search filters are applied, but can be turned off in limits.conf. \p\\
              2. Data manually collected with 'tscollect': Select from your namespace with 'FROM <namespace>'. If you supplied no namespace to tscollect, the data      \
                 was collected into the dispatch directory of that job. In that case, you would select from that data with 'FROM sid=<tscollect-job-id>'                \p\\
              3. An accelerated datamodel: Select from this accelerated datamodel with 'FROM datamodel=<datamodel-name>'                                                \
              You can provide any number of aggregates to perform, and also have the option of providing a filtering query using the WHERE keyword. This query looks    \
              like a normal query you would use in the search processor. You can also provide any number of GROUPBY fields. If you are grouping by _time, you should    \
              supply a timespan with 'span' for grouping the time buckets. This timespan looks like any normal timespan in Splunk, like '1hr' or '3d'. It also supports 'auto'.     \p\\
              Arguments:                                                                                                                                                \i\\
              "prestats": This simply outputs the answer in prestats format, in case you want to pipe the results to a                                                  \i\\
                          different type of processor that takes prestats output, like chart or timechart. This is very useful for                                      \i\\
                          creating graphs                                                                                                                               \i\\
              "local": If you set this to true it forces the processor to only be run on the search head.                                                               \i\\
              "append": Only valid in prestats mode, this allows tstats to be run to add results to an existing set of                                                  \i\\
                        results, instead of generating them.                                                                                                            \i\\
              "summariesonly": Only applies when selecting from an accelerated datamodel.  When false (default),                                                        \i\\
                               Splunk will generate results from both summarized data, as well as for data that is not                                                  \i\\
                               summarized. For data not summarized as TSIDX data, the full search behavior will be used                                                 \i\\
                               against the original index data.  If set to true, 'tstats' will only generate results from the                                           \i\\
                               TSIDX data that has been automatically generated by the acceleration, and nonsummarized data                                             \i\\
                               will not be provided.                                                                                                                    \i\\
              "allow_old_summaries": Only applies when selecting from an accelerated datamodel.  When false                                                             \i\\
                                     (default), Splunk only provides results from summary directories when those directories are up-to-date.                            \i\\
                                     In other words, if the datamodel definition has changed, we do not use those summary directories                                   \i\\
                                     which are older than the new definition when producing output from tstats. This default ensures                                    \i\\
                                     that the output from tstats will always reflect your current configuration. If this is instead                                     \i\\
                                     set to true, then tstats will use both current summary data as well as summary data that was                                        \i\\
                                     generated prior to the definition change. Essentially this is an advanced performance                                              \i\\
                                     feature for cases where you know that the old summaries are "good enough".                                                         \i\\
              "chunk_size": Advanced option. This argument controls how many events are retrieved at a time within                                                      \i\\
                            a single TSIDX file when answering queries. The default is 10000000. Only consider supplying a lower                                        \i\\
                            value for this if you find a particular query is using too much memory. The case that could cause this                                      \i\\
                            would be an excessively high cardinality split-by, such as grouping by several fields that have a very                                      \i\\
                            large amount of distinct values. Setting this value too low, however, can negatively impact the overall                                     \i\\
                            runtime of your query.                                                                                                                      \p\\
              NOTE: Except in 'append=t' mode, this is a generating processor, so it must be the first command in a search.
related = tscollect
maintainer = dmarquardt
tags = tstats tsidx projection
category = reporting
example1 = | tstats count FROM mydata
comment1 = Gets the count of all events in the mydata namespace
example2 = | tstats avg(foo) from mydata where bar=value2 baz>5
comment2 = Returns the average of field foo in mydata where bar is specifically 'value2' and the value of baz is greater than 5.
example3 = | tstats count where host=x by source
comment3 = Gives the count by source for events with host=x
example4 = | tstats prestats=t count by _time span=1d | timechart span=1d count
comment4 = Gives a timechart of all the data in your default indexes with a day granularity
example5 = | tstats median(foo) from mydata
comment5 = Gives the median of field foo from mydata
example6 = | tstats prestats=t median(foo) from mydata | tstats prestats=t append=t median(bar) from otherdata | stats median(foo) median(bar)
comment6 = Uses prestats mode in conjunction with append to compute the median values of foo and bar, which are in different namespaces
appears-in = 5.0
usage = public
optout-in = lite, lite_free

##################
# transaction
##################
[transaction-command]
syntax = transaction (<field-list>)? (name=<transaction-name>)? (<txn_definition-opt>)* (<memcontrol-opt>)* (<rendering-opt>)*
alias = transam
shortdesc = Groups events into transactions.
description = Groups events into transactions based on various constraints, such as the beginning \
              and ending strings or time between events. Transactions are made up of the raw text \
              (the _raw field) of each member, the time and date fields of the earliest member, as \
              well as the union of all other fields of each member.\p\\
              Produces two fields to the raw events, duration and eventcount. The duration value \
              is the difference between the timestamps for the first and last events in the \
              transaction. The eventcount value is the number of events in the transaction.

comment1 = Collapse all events that share the same host and cookie value, that occur within 30 seconds, and do not have a\
           pause of more than 5 seconds between the events.
example1 = ... | transaction host,cookie maxspan=30s maxpause=5s
commentcheat = Group search results that have the same "host" and "cookie", occur within 30 seconds of each other, and do not have a pause greater than 5 seconds between each event into a transaction.
examplecheat = ... | transaction host cookie maxspan=30s maxpause=5s
comment2 = Group search results that share the same value of "from", with a maximum span of 30 seconds, and a pause between events no greater than 5 seconds into a transaction.
example2 = ... | transaction from maxspan=30s maxpause=5s
category = results::group
maintainer = ledion/david
usage = public
supports-multivalue = true
tags = transaction group cluster collect gather
related = searchtxn

[transaction-name]
syntax = <string>
description = The name of a transaction definition from transactions.conf to be used for finding transactions. \
              If other arguments (e.g., maxspan) are provided as arguments to transam, they overrule the value \
              specified in the transaction definition.
default =
example1 = purchase_transaction


[txn_definition-opt]
syntax = <maxspan-opt> | <maxpause-opt> | <maxevents-opt> | <field-list> | <start-opt> | <end-opt> | <connected-opt> | <unify-ends-opt> | <keeporphans-opt>

[maxspan-opt]
syntax = maxspan=<int>(s|m|h|d)?
description = The maxspan constraint requires the transaction's events to span less than maxspan. \
              If value is negative, disable the maxspan constraint.
default = maxspan=-1 (no limit)

[maxpause-opt]
syntax = maxpause=<int>(s|m|h|d)?
description = The maxpause constraint requires there be no pause between a transaction's events of greater than maxpause. \
              If value is negative, disable the maxpause constraint.
default = maxpause=-1 (no limit)

[maxevents-opt]
syntax      = maxevents=<int>
description = The maximum number of events in a transaction. If the value is negative this constraint is disabled.
default     = maxevents=1000

[fields-opt]
syntax = fields=<string>? (,<string>)*
description = DEPRECATED: The preferred usage of transaction is for list of fields to be specified directly as arguments.  E.g. 'transaction foo bar' rather than 'transaction fields="foo,bar"' \
	      The 'fields' constraint takes a list of fields.  For search results to be members of a transaction, for each \
              field specified, if they have a value, it must have the same value as other members in that transaction. \
              For example, a search result that has host=mylaptop can never be in the same transaction as a search result \
              that has host=myserver, if host is one of the constraints.  A search result that does not have a host value, \
              however, can be in a transaction with another search result that has host=mylaptop, because they are not inconsistent.
example1 = fields=host,cookie
default =

[start-opt]
syntax = startswith=<transam-filter-string>
description = A search or eval filtering expression which if satisfied by an event marks the beginning of a new transaction
example1 = startswith="login"
example2 = startswith=(username=foobar)
example3 = startswith=eval(speed_field < max_speed_field)
example4 = startswith=eval(speed_field < max_speed_field/12)
default  =

[end-opt]
syntax      = endswith=<transam-filter-string>
description = A search or eval filtering expression which if satisfied by an event marks the end of a transaction
example1 = endswith="logout"
example2 = endswith=(username=foobar)
example3 = endswith=eval(speed_field > max_speed_field)
example4 = endswith=eval(speed_field > max_speed_field/12)
default  =

[transam-filter-string]
syntax      =  "<search-expression>" | (<quoted-search-expression>) | eval(<eval-expression>)
description = Where: \i\\
              <search-expression>       is a valid search expression that does not contain quotes\i\\
              <quoted-search-expression> is a valid search expression that contains quotes\i\\
              <eval-expression>         is a valid eval expression that evaluates to a boolean
example1 = (name="foo bar")
example2 = "user=mildred"
example3 = ("search literal")
example4 = eval(distance/time < max_speed)


[connected-opt]
syntax      = connected=<bool>
description = Relevant iff fields is not empty. Controls whether an event that is not inconsistent and not consistent\
              with the fields of a transaction, opens a new transaction (connected=t) or is added to the transaction. \
              An event can be not inconsistent and not consistent if it contains fields required by the transaction \
              but none of these fields has been instantiated in the transaction (by a previous event addition).
default     = connected=t

[keeporphans-opt]
syntax      = keeporphans=<bool>
description = Whether the transaction command should output the results that are not part of any transactions. \
              The results that are passed through as "orphans" can be distinguished from transactions by looking \
              at the _txn_orphan field, which is set to 1 for orphan results.
default     = keeporphans=f

[unify-ends-opt]
syntax      = unifyends=<bool>
description = Whether to force events that match startswith/endswith constraint to also match at least one of the \
              fields used to unify events into a transactions.
default     = unifyends=f

### memory constraint options ###
[memcontrol-opt]
syntax = <maxopentxn-opt> | <maxopenevents-opt> | <keepevicted-opt>

[maxopentxn-opt]
syntax       = maxopentxn=<int>
description = Specifies the maximum number of not yet closed transactions to keep in the open pool before starting \
              to evict transactions, using LRU policy.
default     = the default value of this field is read from the transactions stanza in limits.conf

[maxopenevents-opt]
syntax       = maxopenevents=<int>
description = Specifies the maximum number of events (which are) part of open transactions before transaction \
              eviction starts happening, using LRU policy.
default     = the default value of this field is read from the transactions stanza in limits.conf

[keepevicted-opt]
syntax      = keepevicted=<bool>
description = Whether to output evicted transactions. Evicted transactions can be distinguished from non-evicted transactions by checking the value of the 'closed_txn' field, which is set to '0' for evicted transactions and '1' for closed ones. A transaction is evicted from memory when the memory limitations are reached.
default     = false
### multivalue rendering options ###
[rendering-opt]
syntax = <delim-opt> | <mvlist-opt> | <nullstr-opt> | <mvraw-opt>

[mvlist-opt]
syntax      = mvlist=<bool>|<field-list>
description = Flag controlling whether the multivalued fields of the transaction are (1) a list of the original \
              events ordered in arrival order or (2) a set of unique field values ordered lexigraphically. If a \
              comma/space delimited list of fields is provided only those fields are rendered as lists
default     = mvlist=f

[delim-opt]
syntax       = delim=<string>
description = A string used to delimit the original event values in the transaction event fields.
default     = delim=" "

[nullstr-opt]
syntax      = nullstr=<string>
description = A string value to use when rendering missing field values as part of mv fields in a transactions. \
This option applies only to fields that are rendered as lists.
default     = nullstr="NULL"

[mvraw-opt]
syntax      = mvraw=<bool>
description = Whether the _raw field of the transaction search result should be a multivalued field
default     = mvraw=f

##################
# typeahead
#################
[typeahead-command]
syntax = typeahead <prefix-opt> <count-opt> (<max-time-opt>)? <index-specifier>? (<starttimeu>)? (<endtimeu>)? (<collapse-opt>)?
shortdesc = Returns typeahead on a specified prefix.
description = Returns typeahead on a specified prefix. Only returns a max of "count" results, can be targeted to an index and restricted by time. \
              If index specifiers are provided they're used to populate the set of indexes used if no index specifiers are found in the prefix.
maintainer = ledion
comment1 = Return typeahead information for sources in the "_internal" index.
example1 = | typeahead prefix="index=_internal source=" count=10
usage = public
appears-in = 4.0
tags = typeahead help terms
generating = true
category = administrative

[prefix-opt]
syntax = prefix=<string>
description = the full search to do typeahead on
example = prefix=source
example1= prefix="index=_internal war"

[count-opt]
syntax = count=<int>
description = The maximum number of results to return
example = count=10

[max-time-opt]
syntax = max_time=<int>
description = The maximum time in seconds that typeahead can run for ( 0 disables this max_time )
example = max_time=3

[collapse-opt]
syntax      = collapse=<bool>
description = whether to collapse terms that are a prefix of another term and the event count is the same
example     = collapse=f
default     = t

##################
# typelearner
##################
[typelearner-command]
syntax = typelearner (<grouping-field>)? (<grouping-maxlen>)?
shortdesc = Generates suggested eventtypes.  Deprecated: preferred command is 'findtypes'
description = Takes previous search results, and produces a list of promising searches that may be used as event-types.
maintainer = david
commentcheat = Have Splunk automatically discover and apply event types to search results
examplecheat = ... | typelearner
category = results::group
usage = deprecated
appears-in=3.2
related = findtypes, typer
tags = eventtype typer discover search classify

[grouping-field]
syntax = <field>
description = By default, the typelearner initially groups events by the value of the grouping-field, and then further unifies and merges those groups, based on the keywords they contain.  The default grouping field is "punct" (the punctuation seen in _raw).
default = punct
example1 = host

[grouping-maxlen]
syntax = maxlen=<int>
description = determines how many characters in the grouping-field value to look at.  If set to negative, the entire value of the grouping-field value is used to initially group events
default = 15
example1 = maxlen=30


##################
# typer
##################
[typer-command]
syntax = typer
shortdesc = Calculates the eventtypes for the search results
description = Calculates the 'eventtype' field for search results that match a known event-type.
commentcheat = Force Splunk to apply event types that you have configured (Splunk Web automatically does this when you view the "eventtype" field).
examplecheat = ... | typer
category = results::group
maintainer = ssorkin
usage = public
related = typelearner
tags = eventtype typer discover search classify

#################
# where
#################

[where-command]
syntax = where <eval-expression>
shortdesc = Runs an eval expression to filter the results. The result of the expression must be Boolean.
description = Keeps only the results for which the evaluation was successful and the boolean result was true.
comment1 = Return "physicjobs" events with a speed is greater than 100.
example1 = sourcetype=physicsobjs | where distance/time > 100
comment2 = Return "CheckPoint" events that match the IP or is in the specified subnet.
example2 = host="CheckPoint" | where (src LIKE "10.9.165.%") OR cidrmatch("10.9.165.0/25", dst)
appears-in = 4.0
usage = public
tags = where filter search
maintainer = marquardt
category = results::filter
related = eval search regex

#################
# highlight
#################

[highlight-command]
syntax = (highlight|hilite) (<string>)+
simplesyntax = highlight (<string>)+
shortdesc = Causes UI to highlight selected strings.
description = Causes each of the space separated or comma-separated strings provided to be highlighted by the splunk web UI. \
	      These strings are matched case insensitively.
commentcheat = Highlight the terms "login" and "logout".
examplecheat = ... | highlight login,logout
comment2 = Highlight the text sequence "access denied".
example2 = ... | highlight "access denied"
category = formatting
maintainer = sorkin
appears-in = 3.2
usage = public
tags = ui search
related = iconify, abstract

##################
# xyseries
##################

[xyseries-command]
syntax = xyseries (grouped=<bool>)? <x-field> <y-name-field> (<y-data-field>)+ (sep=<string>)? (format=<string>)?
shortdesc = Converts results into a format suitable for graphing.
description = Converts results into a format suitable for graphing.  If multiple \
	      y-data-fields are specified, each column name is the \
	      the y-data-field name followed by the sep string (default is ": ") \
	      and then the value of the y-name-field it applies to. \
	      If the grouped option is set to true (false by default), \
	      then the input is assumed to be sorted by the value of the \
	      <x-field> and multi-file input is allowed.
comment1 = Reformat the search results.
example1 =  ... | xyseries delay host_type host
usage = public
maintainer = steveyz
appears-in = 3.0
alias = maketable
related = untable
category = reporting
tags = convert graph

[x-field]
syntax = <field>
description = Field to be used as the x-axis

[y-name-field]
syntax = <field>
description = Field that contains the values to be used as data series labels

[y-data-field]
syntax = <field>
description = Field that contains the data to be charted

################
# untable
################
[untable-command]
syntax = untable <x-field> <y-name-field> <y-data-field>
description = Converts results from a tabular format to a format similar to stats output.  Inverse of xyseries.
comment1 = Reformat the search results.
example1 = ... | timechart avg(delay) by host | untable _time host avg_delay
usage = public
maintainer = steveyz
appears-in = 4.0
related = xyseries
category = reporting
tags = convert table

################
# rest
################
[rest-command]
syntax = rest <rest-uri> (<splunk-server-opt>)? (<splunk-server-group-opt>)*  (<timeout-opt>)? (<get-arg-name>=<get-arg-value>)*
description = Access a REST endpoint and display the returned entities as search results.
comment1 = Access saved search jobs.
example1 = | rest /services/search/jobs count=0 splunk_server=local | search isSaved=1
usage = public
maintainer = ledion
appears-in = 4.3
category = rest
tags = rest endpoint access

[splunk-server-opt]
syntax      = splunk_server=<string>
description = Optional, argument specifies whether or not to limit results to one specific server. Use "local" to refer to the search head

[splunk-server-group-opt]
syntax      = splunk_server_group=<string>
description = Optional, argument specifies whether or not to limit results to one speciic server_group.

[timeout-opt]
syntax      = timeout=<int>
description = Optional, argument specifies the timeout, in seconds, when waiting for the REST endpoint to respond. Defaults to 60 seconds.

################
# surrounding
################

[surrounding-command]
syntax = surrounding id=<event-id> timebefore=<int> timeafter=<int> searchkeys=<key-list> <int:maxresults> readlevel=<readlevel-int> <index-specifier>
description = Finds events surrounding the event specified by event-id filtered by the search keys.
example1 = | surrounding id=0:0 timeBefore=3600 timeAfter=3600 searchKeys=source::foo host::bar maxresults::50 readlevel::2 index::defaultg
usage = internal
maintainer = brian
appears-in = 3.0
generating = true

[event-id]
syntax = <int>:<int>
description = a splunk internal event id

[key-list]
syntax = (<string> )*
description = a list of keys that are ANDed to provide a filter for surrounding command

[readlevel-int]
syntax = 0|1|2|3
description = How deep to read the events, 0 : just source/host/sourcetype, 1 : 0 with _raw, 2 : 1 with kv, 3 2 with types ( deprecated in 3.2 )

###############
# xmlkv
##############

[xmlkv-command]
syntax = xmlkv <maxinputs-opt>
shortdesc = Extracts XML key-value pairs.
description = Finds key value pairs of the form <foo>bar</foo> where foo is the key and bar is the value from the _raw key.
example1 = ... | xmlkv maxinputs=10000
commentcheat = Extract field/value pairs from XML formatted data. "xmlkv" automatically extracts values between XML tags.
examplecheat = ... | xmlkv
category = fields::add
usage = public
maintainer = brian
appears-in = 3.x
related = extract, kvform, multikv, rex, xpath
tags = extract xml

###############
# xmlunescape
###############

[xmlunescape-command]
syntax = xmlunescape <maxinputs-opt>
shortdesc = Un-escapes XML characters.
description = Un-escapes XML entity references (for: &, <, and >) back to their corresponding characters (e.g., "&amp;" -> "&").
commentcheat = Un-escape all XML characters.
examplecheat = ... | xmlunescape
category = formatting
usage = public
maintainer = brian
appears-in = 3.x
tags = unescape xml escape


###############
# xpath
###############

[xpath-command]
syntax = xpath <string:xpath> (field=<field>)? (outfield=<field>)? (default=<string>)?
shortdesc = Extracts the xpath value from FIELD and sets the OUTFIELD attribute.
description = Sets the value of OUTFIELD to the value of the XPATH applied to FIELD.  If no value could be set, the DEFAULT value is set.  FIELD defaults to "_raw"; OUTFIELD, to "xpath"; and DEFAULT, to not setting a default value.  The field value is wrapped in a "<data>...</data>" tags so that the field value is a valid xml, even if it contains some none xml.
comment1 = pull out the name of a book from xml, using the relative path of //book
example1 = sourcetype="books.xml" | xpath "//book/@name"  outfield=name
comment2 = pull out the name of a book from xml, using the full path of /data/book
example2 = sourcetype="books.xml" | xpath "/data/book/@name"  outfield=name
usage = public
maintainer = david
appears-in = Madonna
tags = xml extract
category = fields::add
related = extract, kvform, multikv, rex, xmlkv

###############
# iplocation
###############

[iplocation-command]
syntax = iplocation (prefix=<string>)? (allfields=<bool>)? (lang=<string>)? ip-address-fieldname
shortdesc = Extracts location information from IP addresses using 3rd-party databases.
description = The ip-address field in ip-address-fieldname is looked up in a database and location fields \
	      information is added to the event. The fields are City, Continent, Country, MetroCode, \
	      Region, Timezone, lat(latitude) and lon(longitude). \
	      Not all of the information is available for all ip address ranges, and hence it is \
	      normal to have some of the fields empty. \
	      The Continent, MetroCode, and Timezone are only added if allfields=true (default is false). \
	      prefix=string will add a certain prefix to all fieldnames if you desire to uniquely qualify \
	      added field names and avoid name collisions with existing fields (default is NULL/empty string). \
	      The lang setting can be used to render strings in alternate languages (for example "lang=es" \
	      for spanish)  The set of languages depends on the geoip database in use.  The special language \
	      "lang=code" will return fields as abbreviations where possible.
example1 = sourcetype = access_combined_* | iplocation clientip
example2 = sourcetype = access_combined_* | iplocation allfields=true clientip
example3 = sourcetype = access_combined_* | iplocation prefix=iploc_ allfields=true clientip
usage = public
maintainer = arahut
appears-in = 6.x
tags = ip location city geocode
commentcheat = Add location information (based on IP address).
examplecheat = ... | iplocation
category = fields::add

################
# file
################

[file-command]
syntax = file <filename>
alias = test
shortdesc = Processes the given file as if it were indexed.
description = If filename is a file, the file command will read the file as if it was indexed in Splunk. If filename is a directory, file will display the list of files in that directory with the option of adding those to the inputs.
commentcheat = Display events from the file "messages.1" as if the events were indexed in Splunk.
examplecheat = | file /var/log/messages.1
category = results::read
usage = public
maintainer = brian
appears-in = 3.x
tags = file index read open preview test input
related = inputcsv


################
# rangemap
################

[rangemap-command]
syntax = rangemap field=<field> (attrn=<num>-<num>)+ (default=<string>)?
shortdesc = Sets RANGE field to the name of the ranges that match.
description = Sets RANGE field to the names of any ATTRN that the value of FIELD is within.  If no range is matched, the RANGE is set to the DEFAULT values.
example1 = ... | rangemap field=date_second green=1-30 blue=31-39 red=40-59 default=gray
comment1 = Set RANGE to "green" if the date_second is between 1-30; "blue", if between 31-39; "red", if between 40-59; and "gray", if no range matches (e.g. "0").
example2 = ... | rangemap field=count low=0-0 elevated=1-100 default=severe
comment2 = Sets the value of each event's RANGE field to "low" if COUNT is 0, "elevated" if between 1-100, and "severe" otherwise.
usage = public
maintainer = david
appears-in = 4.0
tags = colors stoplight range
category = fields::add

################
# rawstats
################

[rawstats-command]
syntax = rawstats
shortdesc = Returns statistics about the raw field.
description = Returns statistics about the raw field that might be useful for filtering/classifying events.
example1 = ... | rawstats | search rawstat_width_avg<30 linecount>30
comment1 = get long skinny events
usage = internal
maintainer = david
appears-in = 3.2 Britney


################
# keywordsuggestor
################

[keywordsuggestor-command]
syntax = rawstats (maxsample=<int>)?
description = Adds an info message to output for the user to see, with suggested terms found in _raw, which would be useful in differentiating the top results from each other.
example1 = ... | keywordsuggestor maxsample=50
usage = internal
maintainer = david
appears-in = 3.2 Britney


################
# reltime
################

[reltime-command]
syntax = reltime
shortdesc = Sets the 'reltime' field to a human readable value of the difference between 'now' and '_time'.
description = Sets the 'reltime' field to a human readable value of the difference between 'now' and '_time'.  Human-readable values look like "5 days ago", "1 minute ago", "2 years ago", etc.
comment1 = add a reltime field
example1 = ... | reltime
usage = public beta
maintainer = david
appears-in = 4.1.5
tags = time ago
category = formatting
related = convert


################
# scrub
################

[scrub-command]
syntax = scrub (public-terms=<filename>)? (private-terms=<filename>)? (name-terms=<filename>)? (dictionary=<filename>)? (timeconfig=<filename>)? (namespace=<string>)?
shortdesc = Anonymizes the search results.
description = Anonymizes the search results by replacing identifying data - usernames, IP addresses, domain names, etc. - with fictional values that maintain the same word length. For example, it may turn the string user=carol@adalberto.com into user=aname@mycompany.com. This lets Splunk users share log data without revealing confidential or personal information. By default the dictionary and configuration files found in $SPLUNK_HOME/etc/anonymizer are used.  These can be overridden by specifying arguments to the scrub command.  The arguments exactly correspond to the settings in the stand-alone "splunk anonymize" command, and are documented there.  Anonymizes all attributes, exception those that start with "_" (except "_raw") or "date_", or the following attributes: "eventtype", "linecount", "punct", "sourcetype", "timeendpos", "timestartpos". \
When using alternative filenames, they must not contain paths and refer to files located in $SPLUNK_HOME/etc/anonymizer, or the optional namespace="appname" must be used to specify an app supplying the files, and they will be read from $SPLUNK_HOME/etc/app/<appname>/anonymizer.
comment1 = Anonymize the current search results.
example1 = ... | scrub
usage = public beta
maintainer = david
appears-in = 3.X Madonna
tags = anonymize scrub secure private obfuscate
category = formatting

##############
# metadata
##############

[metadata-command]
syntax = metadata (type=<metadata-type>)? (<index-specifier>)? (splunk_server=<string>)?
shortdesc = Returns a list of source, sourcetypes, or hosts.
description = This search command generates a list of source, sourcetypes, or hosts from the index. Optional splunk_server argument specifies whether or not to limit results to one specific server.
comment1 = Return the values of "host" for events in the "_internal" index.
example1 = | metadata type=hosts index=_internal
comment2 = Return values of "sourcetype" for events in the "_audit" index on server foo
example2 = | metadata type=sourcetypes index=_audit splunk_server=foo
usage = public
appears-in = 3.X Madonna
tags = metadata host source sourcetype
maintainer = brian
category = administrative
related = dbinspect

[metadata-type]
syntax = hosts|sources|sourcetypes
description = controls which metadata type that will be returned

#############
# debug
#############

[debug-command]
syntax = debug cmd=<debug-method> param1=<string> param2=<string> <index-specifier>
shortdesc = Performs a debug command.
description = This search command can be used to issue debug commands to the system.
example1 = | debug cmd=roll index=_internal
usage = debug
maintainer = brian
appears-in = Madonna
tags = debug roll

[debug-method]
syntax = optimize|roll|logchange|validate|delete|sync|sleep|rescan
description = The available commands for debug command

#############
# delete
#############

[delete-command]
syntax = delete
shortdesc = Deletes (makes irretrievable) events from Splunk indexes.
description = Piping a search to the delete operator marks all the events returned by that search so that they are never returned by any later search. No user (even with admin permissions) will be able to see this data using Splunk. \
              The delete operator can only be accessed by a user with the "delete_by_keyword" capability. By default, Splunk ships with a special role, "can_delete" that has this capability (and no others). The admin role does not have this capability by default. Splunk recommends you create a special user that you log into when you intend to delete index data. \
              To use the delete operator, run a search that returns the events you want deleted. Make sure that this search ONLY returns events you want to delete, and no other events. Once you've confirmed that this is the data you want to delete, pipe that search to delete. \
              Note: The delete operator will trigger a roll of hot buckets to warm in the affected index(es).
usage = public
maintainer = brian
appears-in = 4.0
tags = delete hide
# example1 = index=imap invalid | delete
# comment1 = Delete events from the "imap" index that contain the word "invalid".
# example2 = index=insecure | regex _raw = "\d{3}-\d{2}-\d{4}" | delete
# comment2 = Delete events from the "insecure" index that contain strings that look like Social Security numbers.
category = index::delete


[metadata-delete-restrict]
syntax = (host::|source::|sourcetype::)<string>
description = restrict the deletion to the specified host, source or sourcetype.

############
# eventcount
############

[eventcount-command]
syntax = eventcount (<index-specifier>)* (summarize=<bool>)? (report_size=<bool>)? (list_vix=<bool>)?
shortdesc = Returns the number of events in an index.
description = Returns the number of events in an index.  By default, it summarizes the events across all peers and indexes (summarize is True by default).  If summarize is False, it splits the event count by index and search peer.  If report_size is True (it defaults to False), then it will also report the index size in bytes. If list_vix is False (it defaults to True) then virtual indexes will not be listed.
usage = public
maintainer = igor
appears-in = 4.0
tags = count eventcount
category = reporting
example1 = | eventcount index=_internal
comment1 = Return the number of events in the '_internal' index.
example2 = | eventcount summarize=false index=*
comment2 = Gives event count by each index/server pair.
example3 = | eventcount
comment3 = Displays event count over all search peers.


##################
# findtypes
##################
[findtypes-command]
syntax = findtypes max=<int> (notcovered)? (useraw)?
shortdesc = Generates suggested event types.
description = Takes previous search results, and produces a list of\
  promising searches that may be used as event types.  Returns up to MAX\
  event types, defaulting to 10.  If the "notcovered" keyword is\
  specified, then event types that are already covered by other\
  eventtypes are not returned.  At most 5000 events are analyzed for\
  discovering event types.  If the "useraw" keyword is specified, then\
  phrases in the _raw text of the events is used for generating event\
  types.
maintainer = david
commentcheat = Discover 50 common event types and add support for looking at text phrases
examplecheat = ... | findtypes max=50 useraw
category = results::group
usage = public
note = replacement for typelearner
appears-in=4.0.5
related = typer,typelearner
tags = eventtype typer discover search classify




###############
# return
###############

[return-command]
syntax = return (<int:count>)? (<field:alias>=<field>)* (<field>)* ($<field>)*
shortdesc = convenient way to return values up from a subsearch
description = Useful for passing values up from a subsearch.  Replaces the incoming events with one event, with one attribute: "search". Automatically limits the incoming results with "head" and "fields", to improve performance.  Allows convenient outputting of attr=value (e.g., "return source"), alias_attr=value (e.g. "return ip=srcip"), and value (e.g., "return $srcip").  Defaults to using just the first row of results handed to it.  Multiple rows can be specified with COUNT (e.g. "return 2 ip"), and each row is ORd (e.g., output might be "(ip=10.1.11.2) OR (ip=10.2.12.3)").  Multiple values can be specified and are placed within OR clauses.  So "return 2 user ip" might output "(user=bob ip=10.1.11.2) OR (user=fred ip=10.2.12.3)".  Using "return" at the end of a subsearch removes the need, in the vast majority of cases, for "head", "fields", "rename", "format", and "dedup".
usage = public
maintainer = david
appears-in = 4.2
comment1 = search for "error ip=<someip>", where someip is the most recent ip used by Amrit
example1 = error [ search user=amrit | return ip]
comment2 = search for "error (user=user1 ip=ip1) OR (user=user2 ip=ip2)", where users and IPs come from the two most-recent logins
example2 = error [ search login | return 2 user, ip]
#comment3 = return to eval the userid of the last user, and increment it by 1.
#example3 = ... | eval nextid = 1 + [ search user=* | return $id] | ...
tags = format query subsearch search
category = search::subsearch
related = search format




###############
# runshellscript
###############

[runshellscript-command]
syntax = runshellscript <script-filename> <result-count> <search-terms> <search-string> <savedsearch-name> <description> <results-url> <deprecated-arg> <search-id>
shortdesc   = Internal command used to execute scripted alerts
description = Internal command used to execute scripted alerts. The script file needs to be located \
 in either $SPLUNK_HOME/etc/system/bin/scripts OR $SPLUNK_HOME/etc/apps/<app-name>/bin/scripts. \
 The search id is used to create a path to the search's results. All other args are passed to the \
 script (unvalidated) as follows: \i\\
 $0 = scriptname \i\\
 $1 = number of events returned \i\\
 $2 = search terms \i\\
 $3 = fully qualified query string \i\\
 $4 = name of saved splunk \i\\
 $5 = trigger reason (i.e. "The number of events was greater than 1") \i\\
 $6 = link to saved search \i\\
 $7 = DEPRECATED - empty string argument \i\\
 $8 = file where the results for this search are stored(contains raw results)
usage       = internal
maintainer  = ledion
category    = search::external
related     = script



##################
# searchtxn
##################
[searchtxn-command]
syntax = searchtxn <transaction-name> (max_terms=<int>)? (use_disjunct=<bool>)? (eventsonly=<bool>)? <search-string>
shortdesc = finds transaction events given search constraints
description = Retrieves events matching the transactiontype\
TRANSACTION-NAME with events transitively discovered by the initial\
event constraint of the SEARCH-STRING.  \p\\
For example, given an 'email'\
transactiontype with fields="qid pid" and with a search attribute of\
'sourcetype="sendmail_syslog"', and a SEARCH-STRING of "to=root", searchtxn will\
find all the events that match 'sourcetype="sendmail_syslog" to=root'.\p\\
From those results, all the qid's and pid's are transitively used to\
find further search for relevant events. When no more qid or pid\
values are found, the resulting search is run\i\\
'sourcetype="sendmail_syslog" ((qid=val1 pid=val1) OR ...\
....(qid=valn pid=valm) | transaction name=email | search to=root'.\p\\
Options:\p\\
max_terms -- integer between 1-1000 which determines how many unique field values all fields can use (default=1000).  Using smaller values will speed up search, favoring more recent values\p\\
use_disjunct -- determines if each term in SEARCH-STRING should be OR'd on the initial search (default=true)\p\\
eventsonly -- if true, only the relevant events are retrieved, but the "|transaction" command is not run (default=false)
comment1 = find all email transactions to root from david smith
example15 = | searchtxn email to=root from="david smith"
usage = public
maintainer = david
appears-in = 4.2
category = results::group
tags = transaction group cluster collect gather needle winnow
related = transaction



##################
# x11
##################
[x11-command]
syntax = x11 [<type>][<period>](<fieldname>) [as <newname>]
shortdesc = Remove seasonal fluctuations in fields.
description = Remove seasonal fluctuations in fields. This command has a similar purpose to the\
		trendline command, but is more sophisticated as it uses the industry popular X11 method.\
		The type option can be either 'mult' (for multiplicative) or 'add' (for additive). By default,\
		it's 'mult'. The period option should be specified if known; otherwise it is automatically computed.
example1 = ... | x11 foo as fubar
example2 = ... | x11 24(foo) as fubar
example3 = ... | x11 add12(foo) as fubar
usage = public
maintainer = nghi
appears-in = Ace
category = reporting
related = trendline
tags = x11 deseasonal seasonal
